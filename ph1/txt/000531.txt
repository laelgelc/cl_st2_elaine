2004. I was a young masters student about to start my first NLP research project, and my task was to train a language model. Now that language model was a little bit smaller than the ones we have today. It was trained on millions rather than trillions of words. I used a hidden Markov model as opposed to a transformer, but that little language model I trained did something I thought was amazing. It took all this raw text and somehow it organized it into concepts. A concept for months, male first names, words related to the law, countries and continents and so on. But no one taught these concepts to this model. It discovered them all by itself, just by analyzing the raw text. But how? I was intrigued, I wanted to understand it, I wanted to see how far we could go with this. So I became an AI researcher. In the last 19 years, we have come a long way as a research community. Language models and more generally, foundation models, have taken off and entered the mainstream. But, it is important to realize that all of these achievements are based on decades of research. Research on model architectures, research on optimization algorithms, training objectives, data sets. For a while, we had an incredible free culture, a culture of open innovation, a culture where researchers published, researchers released data sets, code, so that others can go further. It was like a jazz ensemble where everyone was riffing off of each other, developing the technology that we have today. But then in 2020, things started changing. Innovation became less open. And then today, the most advanced foundation models in the world are not released openly. They are instead guarded closely behind black box APIs with little to no information about how they're built. So it's like we have these castles which house the world's most advanced AIs and the secret recipes for creating them. Meanwhile, the open community still continues to innovate, but the resource and information asymmetry is stark. This opacity and centralization of power is concerning. Let me give you three reasons why. First, transparency. With closed foundation models, we lose the ability to see, to evaluate, to audit these models which are going to impact billions of people. Say we evaluate a model through an API on medical question answering and it gets 95 percent accuracy. What does that 95 percent mean? The most basic tenet of machine learning is that the training data and the test data have to be independent for evaluation to be meaningful. So if we don't know what's in the training data, then that 95 percent number is meaningless. And with all the enthusiasm to deploying these models in the real world without meaningful evaluation, we are flying blind. And transparency isn't just about the training data or evaluation. It's also about environmental impact, labor practices, release processes, risk mitigation strategies. Without transparency, we lose accountability. It's like not having nutrition labels on the food you eat, or not having safety ratings on the cars you drive. Fortunately, the food and auto industries have matured over time, but AI still has a long way to go. Second, values. So model developers like to talk about aligning foundation models to human values, which sounds wonderful. But whose values are we talking about here? If we were just building a model to answer math questions, maybe we wouldn't care, because as long as the model produces the right answer, we would be happy, just as we're happy with calculators. But these models are not calculators. These models will attempt to answer any question you throw it. Who is the best basketball player of all time? Should we build nuclear reactors? What do you think of affirmative action? These are highly subjective, controversial, contested question, and any decision on how to answer them is necessarily value laden. And currently, these values are unilaterally decided by the rulers of the castles. So can we imagine a more democratic process for determining these values based on the input from everybody? So foundation models will be the primary way that we interact with information. And so determining these values and how we set them will have a sweeping impact on how we see the world and how we think. Third, attribution. So why are these foundation models so powerful? It's because they're trained on massive amounts of data. See what machine-learning researchers call data is what artists call art or writers call books or programers call software. The data here is a result of human labor, and currently this data is being scraped, often without attribution or consent. So understandably, some people are upset, filing lawsuits, going on strike. But this is just an indication that the incentive system is broken. And in order to fix it, we need to center the creators. We need to figure out how to compensate them for the value of the content they produced, and how to incentivize them to continue innovating. Figuring this out will be critical to sustaining the long term development of AI. So here we are. We don't have transparency about how the models are being built. We have to live with a fixed values set by the rulers of the castles, and we have no means of attributing the creators who make foundation models possible. So how can we change the status quo? With these castles, the situation might seem pretty bleak. But let me try to give you some hope. In 2001, Encyclopedia Britannica was a castle. Wikipedia was an open experiment. It was a website where anyone could edit it, and all the resulting knowledge would be made freely available to everyone on the planet. It was a radical idea. In fact, it was a ridiculous idea. But against all odds, Wikipedia prevailed. In the '90s, Microsoft Windows was a castle. Linux was an open experiment. Anyone could read its source code, anyone could contribute. And over the last two decades, Linux went from being a hobbyist toy to the dominant operating system on mobile and in the data center. So let us not underestimate the power of open source and peer production. These examples show us a different way that the world could work. A world in which everyone can participate and development is transparent. So how can we do the same for AI? Let me end with a picture. The world is filled with incredible people: artists, musicians, writers, scientists. Each person has unique skills, knowledge and values. Collectively, this defines the culture of our civilization. And the purpose of AI, as I see it, should be to organize and augment this culture. So we need to enable people to create, to invent, to discover. And we want everyone to have a voice. The research community has focused so much on the technical progress that is necessary to build these models, because for so long, that was the bottleneck. But now we need to consider the social context in which these models are built. Instead of castles, let us imagine a more transparent and participatory process for building AI. I feel the same excitement about this vision as I did 19 years ago as that masters student, embarking on his first NLP research project. But realizing this vision will be hard. It will require innovation. It will require participation of researchers, companies, policymakers, and all of you to not accept the status quo as inevitable and demand a more participatory and transparent future for AI. Thank you. (Applause)