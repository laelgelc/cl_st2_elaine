"You are a disgusting liar." "Someone, somewhere will hunt you down." "I hope someone puts a bullet between your eyes." These are messages received by climate scientists. According to a recent survey, 39 percent of climate scientists have faced online abuse. 18 percent of those are threats of physical violence. "At the end of the day, we're going to see just how much you believe in your global warming and whether you're willing to die for your so-called 'research.'" No scientist should have to fear for their lives. But this is just another day in the life of a climate scientist. I'm not a climate scientist. I'm not a climate change activist. I'm a counterterrorism expert. I started my journey meeting with white supremacists in basements in Sweden and went on to lead a global policy effort after Europe's first major terrorist attack perpetrated by a white supremacist. I went on to found Moonshot, an organization that works to end violence online. I care about climate change denial because it's so often weaponized to serve as a justification for violence. It would be easy to think that if only we could get people to understand climate change is real, we could put an end to this. Unfortunately, it's not that simple. In 2019, a gunman walked into a Walmart in El Paso, Texas. He killed 23 people, many of immigrant background. He called himself an “ecofascist.” He believed in climate change, but he had bought into mis- and disinformation that immigrants were the root cause of it, that sustainability would only be possible with the elimination of people of color. Mis- and disinformation are so often weaponized to serve as a justification for violence. Although they're often used interchangeably, misinformation is information that's false or misleading. Disinformation is spread intentionally to cause harm. It’s so powerful because it taps into your grievances, what makes you really angry, and it offers simplistic solutions. There's typically a villain and a hero. Over the last two years, my team and I have been researching different kinds of manipulation tactics used all over the world to spread disinformation. Two of the most common were decontextualization and fearmongering. Decontextualization is the practice of taking information out of its original context to deliberately mislead people. For example, earlier this year, Europe experienced a series of protests by farmers against a range of proposed environmental regulations. There were street blockades and protests, demonstrations, occupations. Adding to an already tense moment, several inauthentic images circulated. This one purported to show the Ukrainian embassy in Paris getting pummeled with manure. This was actually footage taken months earlier from an entirely different protest about an entirely different issue in Dijon, not even in Paris. And this effort to mislead the public, it wouldn't be complete without the use of new technology. Here's an image showing the streets of Paris lined with bales of hay. It's a really striking image, isn't it? This never happened. It was entirely generated by AI. And this isn't just happening in Europe. Last year, after wildfires raged in Hawaii, a disinformation network linked to the Chinese Communist Party spread inauthentic images purporting that the US government had intentionally spread the wildfires using a so-called “weather weapon.” Can you imagine? Over a hundred people died in those wildfires, and the idea that those fires were deliberately set by their own government against their own people? It's terrifying. These kinds of conspiratorial narratives can spread widespread fear, which takes us to the next powerful tactic of disinformation: fearmongering: deliberately exaggerating an issue so that you can provoke fear and alarm. We know that emotion-driven information processing can overtake evidence-based decision making, which is what makes this form of disinformation so effective. It's for these reasons that a recent MIT study found a false story will travel six times quicker to reach 1,500 people than a true story will. And we know Facebook fact-checkers take up to 72 hours on average to identify and remove this content. By that time, most impressions have already been made. Now I know we have all seen this online, and when you see it happen, it can be really tempting to respond with the facts. I get it. We pride ourselves on logic and science. The truth matters. So when someone is so obviously spreading false information, just correct them, right? Unfortunately, this doesn't always work. Believe me, I spent the last two decades learning how to have conversations with people buying into white supremacy. That is disinformation at its worst. Disinformation wins because of the emotions it inspires, because of the way it makes people feel. So if someone is so bought into disinformation, getting into debates on the facts with them can just risk pushing them even further into a corner, backing them into a corner so that they get really defensive. OK, so if we can't debate the facts endlessly, what can we do? Last year, Moonshot partnered with Google to test an approach known as “prebunking.” Prebunking is a proven communication technique designed to help people spot and reject efforts to manipulate them in the future by giving them forewarning and giving them tools to be able to reject a manipulative message, you lessen the likelihood that they will be misled. This is not about telling people what is true or false or right or wrong. It's about empowering people to protect themselves. We've tapped into the universal human desire not to be manipulated, and this method has been tried and tested for decades, since the 1960s. All prebunking messages contain three essential ingredients. One: an emotional warning. You alert people that there are others out there who may be trying to mislead or manipulate them. Be aware, you may be targeted. Two: stimulus. You show people examples of manipulative messaging so that they will be more likely to be able to identify those in the future. And three: refutation. You give people the tools to be able to refute a message in real time. For example, if you see a headline that’s really sensational, and it either seems too good to be true or it makes you really angry, always Google around for other sources. Always Google around. OK, so we knew the steps to take, but we also knew if we were going to really get at this problem around the world, a one-size-fits-all approach wouldn't work. We knew we needed to get local. So we partnered with civil society organizations in countries around the world, from Germany to Indonesia to Ukraine. And we started first with the evidence. We met with dozens of experts, we surveyed the online space, and we identified the most common manipulation tactics being used in each country. We then partnered with local filmmakers to create educational videos that would teach people about those manipulation tactics that were being used in their home country. In some contexts, we found that people trust close peers and relatives the most. So in Germany, we filmed close friends chatting in a park. In Ukraine, we filmed family dialogues around a kitchen table, a setting that's so familiar to so many of us, where so many of us have had those difficult conversations. We wanted to encourage people to have these kinds of conversations within their own trusted circles, whether they're in El Salvador or Indonesia. And to do so before pivotal moments where online manipulation efforts intensify, like elections. So as we prepared to head into the EU elections, we knew that distrust in climate science had already emerged as a critical misinformation theme. Now one study had found that adults over the age of 45 are less likely to investigate false information when they stumble across it online. Now we also know that adults over the age of 45 have higher voter turnout, which means if it wins, disinformation can have a disproportionate impact on the outcomes of elections. So as we prepare to head into the EU elections, we created content for every EU country, in 27 languages, aiming to empower Europeans to spot and reject efforts to manipulate them before the elections. Over the last year, we have reached millions of people around the globe with these videos. In Germany alone, we reached 42 million people. That's half the German population. And we found on average, viewers of these videos were up to 10 percent more likely to be able to identify manipulation efforts than those who hadn't seen those videos. This is a winning formula. The evidence shows us that prebunking is effective at building resistance to disinformation. It begs the question, how do we make that resistance last? How do we build long-term societal resilience to disinformation efforts? There is an ongoing effort to use disinformation to undermine our democracies. Just last month, the US Justice Department seized 32 internet domains secretly deployed by the Russian government to spread disinformation across the US and Europe. This included deliberate efforts to exploit anxieties and fear across the public about the energy transition, specifically to encourage violence. Now it’s not just the Russian government that we need to be worried about. Easy access to generative AI tools means that anyone, not just those with resources, money and power, can create high-quality, effective, powerful disinformation content. And the sources of disinformation are varied. They can come from our elected officials all the way through to our neighbors down the road. Many of us don't need to look further than our own families. But so many of the tools we tested online are even more powerful when they come directly from the people that you trust and love the most in real life, IRL. So instead of endlessly debating the facts, give your loved ones the tools that they need to protect themselves online. Information manipulation is unfortunately the new norm. But that doesn't mean we need to accept our loved ones being misled, and we shouldn't accept our climate scientists living in fear. So if we can't fact-check our way out of this problem, we need to beat disinformation at its own game by reaching people before disinformation does and giving them all the tools that they need to protect themselves online. Thank you so much. (Applause)