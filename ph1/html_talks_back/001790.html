<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1.0, viewport-fit=cover" data-next-head=""/><script type="text/javascript" src="//b-code.liadm.com/a-091g.min.js" async=""></script><meta property="og:url" content="https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021/transcript" data-next-head=""/><link rel="canonical" href="https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021/transcript" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@tedtalks" data-next-head=""/><meta name="twitter:app:id:iphone" content="376183339" data-next-head=""/><meta name="twitter:app:id:ipad" content="376183339" data-next-head=""/><meta name="twitter:app:id:googleplay" content="com.ted.android" data-next-head=""/><meta property="al:android:app_name" content="TED" data-next-head=""/><meta property="al:android:package" content="com.ted.android" data-next-head=""/><meta property="al:ios:app_name" content="TED" data-next-head=""/><meta property="al:ios:app_store_id" content="376183339" data-next-head=""/><meta name="twitter:app:name:ipad" content="TED" data-next-head=""/><meta name="twitter:app:name:iphone" content="TED" data-next-head=""/><meta name="twitter:app:name:googleplay" content="TED" data-next-head=""/><meta name="HandheldFriendly" content="True" data-next-head=""/><meta name="MobileOptimized" content="320" data-next-head=""/><meta name="apple-mobile-web-app-title" content="TED Talks" data-next-head=""/><meta name="mobile-web-app-capable" content="yes" data-next-head=""/><meta name="apple-mobile-web-app-status-bar-style" content="black" data-next-head=""/><meta name="application-name" content="TED Talks" data-next-head=""/><meta name="msapplication-config" content="https://www.ted.com/browserconfig.xml" data-next-head=""/><meta name="msapplication-TileColor" content="#000000" data-next-head=""/><link rel="apple-touch-icon" href="https://pa.tedcdn.com/apple-touch-icon.png" data-next-head=""/><link rel="apple-touch-icon-precomposed" href="https://pa.tedcdn.com/apple-touch-icon-precomposed.png" data-next-head=""/><meta http-equiv="cleartype" content="on" data-next-head=""/><link rel="icon" href="/favicon.ico" sizes="32x32" data-next-head=""/><link rel="icon" href="/icon.svg" type="image/svg+xml" data-next-head=""/><meta property="fb:app_id" content="201021956610141" data-next-head=""/><meta name="rss-feed" content="https://www.ted.com/feeds/talks.rss" data-next-head=""/><link rel="mask-icon" href="https://pa.tedcdn.com/mask-icon.svg" color="#EB0028" sizes="any" data-next-head=""/><meta name="theme-color" content="#EB0028" data-next-head=""/><title data-next-head="">The TED Interview: The race to build AI that benefits humanity with Sam Altman (from April 2021) | TED Talk</title><meta name="title" content="The TED Interview: The race to build AI that benefits humanity with Sam Altman (from April 2021)" data-next-head=""/><meta name="description" content="In this season of The TED Interview, conversations with people who make a case for ... optimism. Not some blind, hopeful feeling, but the conviction that somewhere out there are solutions that, given the right attention and resources, can guide us out of the dark place we&#x27;re in. For the first episode: artificial intelligence. Will innovation in AI drastically improve our lives, or destroy humanity as we know it? Head of TED Chris Anderson sits down with OpenAI CEO Sam Altman, who makes a case for AI&#x27;s potential to make the future better for all of us -- and explains how his company is leading that charge with an unusual new business model. Listen and subscribe to The TED Interview and more podcasts from the TED Audio Collective wherever you&#x27;re listening to this." data-next-head=""/><meta property="og:title" content="The race to build AI that benefits humanity with Sam Altman (from April 2021)"/><meta property="og:description" content="In this season of The TED Interview, conversations with people who make a case for ... optimism. Not some blind, hopeful feeling, but the conviction that somewhere out there are solutions that, given the right attention and resources, can guide us out of the dark place we&#x27;re in. For the first episode: artificial intelligence. Will innovation in AI drastically improve our lives, or destroy humanity as we know it? Head of TED Chris Anderson sits down with OpenAI CEO Sam Altman, who makes a case for AI&#x27;s potential to make the future better for all of us -- and explains how his company is leading that charge with an unusual new business model. Listen and subscribe to The TED Interview and more podcasts from the TED Audio Collective wherever you&#x27;re listening to this."/><meta property="og:image:width" content="1050"/><meta property="og:image:height" content="550"/><meta property="og:video:duration" content="4154"/><meta property="og:type" content="video.other"/><meta name="twitter:app:url:ipad" content="ted://talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021?source=twitter"/><meta name="twitter:app:url:iphone" content="ted://talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021?source=twitter"/><meta property="twitter:app:url:googleplay" content="https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021/transcript"/><meta property="twitter:image" content="https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=82w=1200"/><meta property="og:image" content="https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=82c=1050%2C550&amp;w=1050"/><meta property="og:image:secure_url" content="https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=82c=1050%2C550&amp;amp;w=1050"/><meta property="og:video:release_date" content="1619188231"/><meta property="og:video:tag" content="science"/><meta property="og:video:tag" content="technology"/><meta property="og:video:tag" content="future"/><meta property="og:video:tag" content="AI"/><meta name="keywords" content="TED, talks, science, technology, future, AI" data-next-head=""/><meta name="medium" content="video" data-next-head=""/><meta name="author" content="The TED Interview" data-next-head=""/><link rel="alternate" type="application/json+oembed" title="oEmbed Profile" href="https://www.ted.com/services/v1/oembed.json?url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fthe_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021%2Ftranscript" data-next-head=""/><link rel="alternate" type="application/xml+oembed" title="oEmbed Profile" href="https://www.ted.com/services/v1/oembed.xml?url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fthe_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021%2Ftranscript" data-next-head=""/><meta name="apple-itunes-app" content="app-id=376183339,app-argument=https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021/transcript?utm_source=ted.com&amp;amp;utm_medium=referral&amp;amp;utm_campaign=smart_app_banner" data-next-head=""/><meta property="al:android:url" content="ted://talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021?source=facebook" data-next-head=""/><meta property="al:ios:url" content="ted://talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021?source=facebook" data-next-head=""/><link rel="alternate" hrefLang="x-default" href="https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021/transcript" data-next-head=""/><link rel="alternate" hrefLang="en" href="https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021?language=en" data-next-head=""/><link rel="preload" href="https://talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg" as="image" type="image/webp" data-next-head=""/><link rel="preconnect" href="https://graphql.ted.com" crossorigin="anonymous"/><link rel="preconnect" href="https://metrics.ted.com" crossorigin="anonymous"/><link rel="preconnect" href="https://js-agent.newrelic.com" crossorigin="anonymous"/><link rel="preconnect" href="https://pi.tedcdn.com" crossorigin="anonymous"/><link rel="preconnect" href="https://pu.tedcdn.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://graphql.ted.com"/><link rel="preconnect" href="https://dev.visualwebsiteoptimizer.com"/><link rel="preconnect" href="https://sli.ted.com" crossorigin="anonymous"/><link rel="preconnect" href="https://rp.liadm.com" crossorigin="anonymous"/><link rel="preconnect" href="https://api.sail-personalize.com" crossorigin="anonymous"/><link rel="preconnect" href="https://geolocation.onetrust.com" crossorigin="anonymous"/><link rel="preconnect" href="https://imasdk.googleapis.com" crossorigin="anonymous"/><link rel="preconnect" href="https://js.stripe.com" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/a7a3d62da9223916.css" as="style"/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"VideoObject","name":"The race to build AI that benefits humanity with Sam Altman (from April 2021)","description":"In this season of The TED Interview, conversations with people who make a case for ... optimism. Not some blind, hopeful feeling, but the conviction that somewhere out there are solutions that, given the right attention and resources, can guide us out of the dark place we&apos;re in. For the first episode: artificial intelligence. Will innovation in AI drastically improve our lives, or destroy humanity as we know it? Head of TED Chris Anderson sits down with OpenAI CEO Sam Altman, who makes a case for AI&apos;s potential to make the future better for all of us -- and explains how his company is leading that charge with an unusual new business model. Listen and subscribe to The TED Interview and more podcasts from the TED Audio Collective wherever you&apos;re listening to this.","thumbnailUrl":"https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=82w=640","uploadDate":"2021-04-23T14:30:31Z","duration":"PT1H09M14S","contentUrl":"https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021/transcript","transcript":"Hello there, this is Chris Anderson, and I am hugely, hugely, tremendously excited to welcome you to a new series of the TED interview. Now, then, this season, we&apos;re trying something new. We&apos;re organising the whole season around a single theme, albeit a theme that some of you may consider inappropriate. But hear me out. The theme is the case for optimism. And yes, I know the world has been hit with extraordinarily ugly things in the last few years. Political division, a racial reckoning, technology run amuck, not to mention a global pandemic and impending climate catastrophe. What on earth are we thinking in this context? Optimism just seems so naive and unwanted, almost annoying. So here&apos;s my position. Don&apos;t think of optimism as a feeling. It&apos;s not just this sort of shallow feeling of hope. Optimism is a search. It&apos;s a determination to look for a pathway forward somewhere out there. I believe I truly believe there are amazing people whose minds contain the ideas, the visions, the solutions that can actually create that pathway forward. If given the support and resources they need, they may very well light the path out of this dark place we&apos;re in. So these are the people who can present not optimism, but a case for optimism. They&apos;re the people I&apos;m talking to this season. So let&apos;s see if they can persuade us now. Then the place I want to start is with A.I. artificial intelligence. This, of course, is the next innovative technology that is going to change everything as we know it, for better or for worse. Today was painted not with the usual dystopian brush, but by someone who truly believes in its potential. Sam Altman is the former president of Y Combinator, the legendary startup accelerator. And in 2015, he and a team launched a company called Open Eye, dedicated to one noble purpose to develop A.I. so that it benefits humanity as a whole. You may have heard, by the way, recently a lot of buzz around in A.I. technology called T3 that was developed by open eye improve the quality of the amazing team of researchers and developers they have work in. There will be hearing a lot about three in the conversation ahead. But sticking to this lofty mission of developing A.I. for humanity and finding the resources to realize it haven&apos;t been simple. Open A.I. is certainly not without its critics, but their goal couldn&apos;t be more important. And honestly, I found it really quite exciting to hear Sam&apos;s vision for where all this could lead. OK, let&apos;s do this. So, Sam Altman, welcome. Thank you for having me. So, Sam, here we are in 2021. A lot of people are fearful of the future at this moment in world history. How would you describe your attitude to the future? I think that the combination of scientific and technological progress and better societal decision making, better societal governance is going to solve in the next couple of decades all of our current most pressing problems, there will be new ones. But I think we are going to get very safe, very inexpensive, carbon free nuclear energy to work. And I think we&apos;re going to talk about that time that the climate disaster looks so bad and how lucky we are. We got saved by science and technology, I think. And we&apos;ve already now seen this with the rapidity that we were able to get vaccines deployed. We are going to find that we are able to cure or at least treat a significant percentage of human disease, including I think we&apos;ll just actually make progress in helping people have much longer decades, longer health spans. And I think in the next couple of decades, that will look pretty clear. I think we will build systems with AI and otherwise that make access to an incredibly high quality education more possible than ever before. I think the lives we look forward like one hundred years, fifty years, even the quality of life available to anyone then will be much better than the quality of life available in the very best case to anyone today, to any single person today. So, yeah, I&apos;m super optimistic. I think, like, it&apos;s always easy to do scroll and think about how bad are the bad things are, but the good things are really good and getting much better. Is it your sincere belief that artificial intelligence can actually make that future better? Certainly. How look, with any technology. I don&apos;t think it will all be better. I think there are always positive and negative use cases of anything new, and it&apos;s our job to maximize the positive ones, minimize the negative ones. But I truly, genuinely believe that the positive impacts will be orders of magnitude bigger than the negative ones. I think we&apos;re seeing a glimpse of that now. Now that we have the first general purpose built out in the world and available via things like RPI, I think we are seeing evidence of just the breadth of services that we will be able to offer as the sort of technological revolution really takes hold. And we will have people interact with services that are smart, really smart, and it will feel like as strange as the world before mobile phones feels now to us. Hmm, yeah, you mentioned your API, I guess that stands for what, application programming interface? It&apos;s the technology that allows complex technology to be accessible to others. So give me a sense of a couple of things that have got you most excited that are already out there and then how that gives you visibility to a pathway forward that is even more exciting. So I think that the things that we&apos;re seeing now are very much glimpse of the future. We released three, which is a general-purpose natural language text model in the summer of twenty twenty. You know, there&apos;s hundreds of applications that are now using it in production that&apos;s ramping up all of the time. But there are things where people use three to really understand the intent behind the search query and deliver results and sort of understand not only intent, but all of the data and deliver the thing of what you want. So you can sort of describe a fuzzy thing and it&apos;ll understand documents. It can understand, you know, short documents, not full books yet, but bring you back to the context of what you want. There&apos;s been a lot of excitement about using the generative capabilities to create sort of games or sort of interactive stories or letting people develop characters or chat with a sort of virtual friend. There are applications that, for example, help a job seeker polish a tailored application for each individual company. There&apos;s the beginning of tutors that can sort of teach people about different concepts and take on different personas. And we can go on for a long time. But I think anything that you can imagine that you do today via computer that you would like to really understand and get to know you. And not only that, but understand all of the data and knowledge in the world and help you have the best experience that is is possible that that will happen. So what gets opened up? What new adjacent possible state is that as a result of these powers from this question, from the point of view of someone who&apos;s starting out on a career, for example, they&apos;re trying to figure out what would be a really interesting thing to do in the future that has only recently become possible. What are some new things that this opens up in a world where you can talk to a computer? And get. The output that would normally require you hiring the world experts back immediately for almost no money, I would say think about what&apos;s possible there. So that could be like, as you said, what can normally only the best programmer in the world or a really great programmer do for me. And can I now instead just ask in English and have that program written? So all these people that, you know, want to develop an app and they have an idea, but they don&apos;t know how to program. Now they can have it. You know, what is the service look like when anyone on Earth who wants really great medical advice? Can get better medical advice than any single doctor could ever get, because this has the total medical knowledge and reasoning ability that the some humanity has ever produced. When you want to learn something, you have sort of a tutor that understands your exact style, how you best learn everything you know, and custom teaches you whatever concept you want to learn someday. You can imagine that like. You have an eye that reads your email and your task list and your calendar and the documents you&apos;ve been sent and in any meeting maximally perfectly prepares you and has all of the information that you need in all the context of your entire career right there for you to go on for a long time. But I think this will just be powerful systems. So it&apos;s really fun playing around with Chapatti three, one compelling example of someone who&apos;s more tax base is try Googling The Guardian essay that was written entirely by different GP2 three queries and stitched together. It&apos;s an essay on why artificial intelligence isn&apos;t a threat to humanity. And that&apos;s impressive. It&apos;s very compelling. I actually tried inputting one of the three online uses. I asked the question what is interesting about some ollman? Oh no. Here&apos;s what it came back with. It was it was rather philosophical, actually. Came back with. I don&apos;t understand what you mean by interesting. It seems to me that this word has no unambiguous meaning in the context of human society beyond its literal definition as somewhat pleasing or entertaining. I do not believe there to be any such thing as INTERESTINGNESS except in the mind of a human or other sentient being that to my knowledge, this is an entirely subjective state that varies from person to person. However, I will grant that this is an interesting question to ask. This does not mean it has been answered. There is no answer to be found. Well, so you can agree that somewhere between profound and gibberish is that almost well, with the state of play is I mean, that&apos;s where we are today. I think somewhere between profound and jibberish is the right way to think about the current capabilities of CGP three. I think they would definitely had a bubble of hype about three last summer. But the thing about bubbles is the reason that smart people fall for them is there&apos;s a kernel of something really real and really interesting that people get overexcited about. And I think people definitely got and still are overexcited about 3:00 today, but still probably underestimated the potential of where these models will go in the future. And so maybe there&apos;s this like short term overhyped and long term under hype for the entire field, for tax models, for whatever you&apos;d like. It&apos;s going on. And as you said, there&apos;s clearly some gibberish in there. But on the other hand, those were like well-formed sentences. And there were a couple of ideas and there that I was like, oh, like they actually maybe that&apos;s right. And I think if artificial intelligence, even in its current very larval state, can make us confront new things and sort of inspire new ideas, that&apos;s already pretty impressive. Give us a sense of what&apos;s actually happening in the background there. I think it&apos;s hard to understand because you read these words seem like someone is trying to mean something. Obviously, I think you believe that there&apos;s whatever you&apos;ve built there, that there&apos;s a sort of thinking, sentient thing that&apos;s going, oh, I must answer this question. So so what how would you describe what&apos;s going on? You&apos;ve got something that has read the entire Internet, essentially all of Wikipedia, etc. We&apos;ve read something that&apos;s read like a small fraction of a random sampling of the Internet. We will eventually train something that has read as much of the Internet or more of the Internet than we&apos;ve done right now. But we have a very long way to go. I mean, we&apos;re still, I think, relative to what we will have operated at quite small scale with quite small eyes. But what is happening is there is a model that is ingesting lots of text and it is trying to predict the next word. So we use Transformer&apos;s they take in a context, which is a particular architecture of an A.I. model, they take in a context of a lot of words, let&apos;s say like a thousand or something like that. And they try to predict the word that comes next in the sequence. And there&apos;s like a lot of other things that happen, but fundamentally that&apos;s it, and I think this is interesting because in the process of playing that little game of trying to predict the next word, these models have to develop a representation and understanding of what is likely to come next and. I think it is maybe not perfectly accurate, but certainly worth considering to say that intelligence is very near the ability to make accurate predictions. What&apos;s confusing about this is that there are so many words on the Internet which are foolish as well as the words that are wise. And and how do you build a model that can distinguish between those two? And this is prompted actually by another example that I typed in. Like I asked, you know, what is a powerful idea, very interested in ideas. That was my question as a powerful idea. And it came back with several things, some of which seemed moderately pronouncements, which seemed moderately gibberish. But then he was he was one that it came back with the idea that the human race has, quote, evolved, unquote, is false evolution or adaptation within a species was abandoned by biology and genetics long ago. Wait a sec. That&apos;s news to me. What have you been reading? And I presume this has been pulled out of some recesses of the Internet, but how is it possible, even in theory, to imagine how a model can gravitate towards truth, wisdom, as opposed to just like majority views? Or how how how do you avoid something taking us further into the sort of the maze of errors and bad thinking and so forth that has already been a worrying feature for the last few years ? It&apos;s a fantastic question, and I think it is the most interesting area of research that we need to pursue. Now, I think at this point, the questions of whether we can build really powerful general-purpose AI system, I won&apos;t say there in the rearview mirror. We still have a lot of hard engineering work to do, but I&apos;m pretty confident we&apos;re going to be able to. And now the questions are like, what should we build? And how and why and what data should we train on and how do we build systems not just that can do these like phenomenally impressive things, but that we can ensure do the things that we want and that understand the concepts of truth and falsehood and, you know, alignment with human values and misalignment with human values. One of the pieces of research that we put out last year that I was most proud of and most excited about is what we call reinforcement learning from human feedback. And we showed that we can take these giant models that are trained on a bunch of stuff, some of it good, some of the bad, and then with a really quite small amount of feedback from human judgment about, hey, this is good, this is bad, this is wrong, this is the behavior I want I don&apos;t want this behavior. We can feed that information from the human judges back into the model and we can teach the model, behave more like this and less like that. And it works better than I ever imagined it would. And that gives me a lot of hope that we can build an aligned system. We&apos;ll do other things, too, like I think curating data sets where there&apos;s just less sort of bad data to train on. It will go a very long way. And as these models get smarter, I think they inherently develop the ability to sort out bad data from good data. And as they get really smart, they&apos;ll even start to do something we call active learning, which is where they ask us for exactly the data they need when they&apos;re missing something, when they&apos;re unsure, when they don&apos;t understand. But I think as a result of simply scaling these models up, building better, I hate to use the word cognition because it sounds so anthropomorphic, but let&apos;s say building a better ability to reason into the models, to think, to challenge, to try to understand and combining that with this idea of online into human values via this technique we developed, that&apos;s going to go a very long way. Now, there&apos;s another question, which you sort of just kicked the ball down the field, too, which is how do we as a society decide to which set of human values do we align these powerful systems? Yeah, indeed. So if I if I understand rightly what you&apos;re saying, that you&apos;re saying that it&apos;s possible to look at the output at any one time of three. And if we don&apos;t like what it&apos;s coming up with, some ways human can say, no, that was off, don&apos;t do that. Whatever algorithm or process led you to that, undo it. Yeah. And that the system is that incredibly powerful at avoiding that same kind of mistake in future because it sort of replicates the instructions , correct? Yeah. And eventually and not much longer, I believe that we&apos;ll be able to not only say that was good, that was bad, but say that was bad for this reason. And also tell me how you got to that answer so I can make sure I understand. But at the end of the day, someone needs to decide who is the wise human or short humans who are looking at the results. So it&apos;s a big difference. Someone who who grew up with intelligent design world view could look at that and go, that&apos;s a brilliant outcome. Well, Goldstar done. And someone else would say something is done awfully wrong here. So how do you avoid and this is a version of the problem that a lot of the, I guess, Silicon Valley companies are facing right now in terms of the pushback they&apos;re getting on the output of social media and so forth. How do you assemble that pool of experts who stand for human values that we actually want? I mean, we talk about this all the time, I don&apos;t think this is like solely or even not even close to majorly up to opening night to decide, I think we need to begin a societal conversation now about how we&apos;re going to make those decisions, how we&apos;re going to make sure we have representational input in that, and how we sort of make these very difficult global governance systems. My personal belief is that we should have pretty broad rules about what these systems will never do and will always do. But then the individual user should get a system that kind of behaves like they want. And there will be people do have very different value systems. Some of them are just fundamentally incompatible. No one gets to use eye to, like, exploit other people, for example, and hopefully we can all agree on. But do you want the AI to like. You know, support you and your belief of intelligent design, like, do I think openly, I should say it can&apos;t, even though I disagree with that is like a scientific conclusion. No, I wouldn&apos;t take that stance. I think the thing to remember about all of this is that history is still quite extraordinarily weak. It&apos;s still has such big problems and it&apos;s still so unreliable that for most use cases it&apos;s still unsuitable. But when we think about a system that is like a thousand times more powerful and let&apos;s say a million times more reliable, it just doesn&apos;t it doesn&apos;t say gibberish very often. It doesn&apos;t totally lose the plot and get distracted or system like that is going to be one that a lot of the economic activity in the world comes to rely on. And I think it&apos;s very important that we don&apos;t have a small group of people sort of saying you can never use it for this thing that, like most of the world wants to use it for because it doesn&apos;t match our personal beliefs. Talk a bit more about some of the other uses of it, because one of the things that&apos;s most surprising is it&apos;s not just about sort of text responses. It&apos;s it can take generalized human instructions and build things up. For example, you can say to it, write a Python program that is designed to put a flashing cursor in one corner of the screen, in the Google logo in the other corner. And and it can go your way and do something like that. Shockingly, quite well, effectively. Yeah, I it can. That&apos;s amazing. I mean, this is amazing to me. That opens the door to. An entirely way to think about programers for the future, that you could you could have people who can program just in human natural language potentially and gain rapid efficiency. I do the engineering. We&apos;re not that far away from that world. We&apos;re not that far away from the world where you will write a spec in English. And for a simple enough program, I will just write the code for you. As you said, you can see glimpses of that even in this very week three which was not trained to code like. I think this is important to remember. We trained it on the language on the Internet very rarely, you know, Internet let language on the Internet also includes some code snippets. And that was enough, so if we really try to go train a model on code itself and that&apos;s where we decide to put the horsepower of the model into, just imagine what will be possible will be quite impressive. But I think what you&apos;re pointing to there is that because models like three to some degree or other, and it&apos;s like very hard to know exactly how much understand the underlying concepts of what&apos;s going on. And they&apos;re not just regurgitating things they found in a website, but they can really apply them and say, oh, yeah, I kind of like know about this word and this idea and code. And this is probably what you&apos;re trying to do. And I won&apos;t get it right always. But sometimes I will just generate this like a brand new program for nothing that anyone has ever asked before. And it will work. That&apos;s pretty cool. And data is data. So it can do that from English to code. It can do that from English to French. Again, we never told it to learn about translation. We never told it about the concepts of English and French, but it learned them, even though we never said this is what English is and this is what French is and this is what it means to translate, it can still do it. Wow, I mean, for creative people, is there a world coming where the sort of the palette of possibility that they can be exposed to is just explodes? I mean, if you&apos;re a musician, is there a near future where you can say to your eye, OK, I&apos;m going to bed now, but in the morning I&apos;d love you to present me with a thousand tuba jingles with words attached that you have of a sort of mean factor to the and you come down in the morning and the computer shows you the stuff. And one of them, you go, wow, that is it. That is a top 10 hit and you build a song from it. Or is that going to be released? Actually be the value add. We released something last year called Jukebox, which is very near what you described, where you can say I want music generated for me in this style or this kind of stuff, and it can come up with the words as well. And it&apos;s like pretty cool. And I really enjoy listening to music that it creates. And I can sort of do four songs, two bars of a jingle, whatever you&apos;d like. And one of my very favorite artists reached out, called to open it after we release this and said that he wanted to talk. And I was like, well, I like total fanboy here. I&apos;d love to join that call. And I was so nervous that he was going to say, this is terrible. This is like a really sad thing for human creativity. Like, you know, why are you doing this? This is like whatever. And he was so excited. And he&apos;s like, this has been so inspiring. I want to do a new album with this. You know, it&apos;s like, give me all these new ideas. It&apos;s making me much better at my job. I&apos;m going to make better music because of this tool. And that was awesome. And I hope that&apos;s how it all continues to go. And I think it is going to lead to this. We see a similar thing now with Dolly, where graphic designers sometimes tell us that they just they see this new set of possibilities because there&apos;s new creative inspiration and they&apos;re cycle time, like the amount of time it takes to just come up with an idea and be able to look at it and then decide whether to go down that path or head in a different direction goes down so much. And so I think it&apos;s going to just be this like incredible creative explosion for humans. And how far away are we some before? And I it comes up with a genuinely powerful new idea, an idea that solves the problem that humans have been wrestling with. It doesn&apos;t have to be as quite on the scale as of, OK, we&apos;ve got a virus coming. Please describe to us what a what a national rational response should look like, but some kind of genuinely innovative idea or solution like one one internal question we&apos;ve asked ourselves is, when will the first genuinely interesting, purely AI written TED talk show up? I think that&apos;s a great milestone. I will say it&apos;s always hard to guess timeline&apos;s I&apos;m sure I&apos;ll be wrong on this, but I would guess the first genuinely interesting. Ted talk, thought of written delivered by an AIDS within the kind of the seven ish year time frame. Maybe a little bit less. And it feels like I mean, just reading that Guardian essay that was kind of it was a composite of several different GPG three responses to questions about, you know, the threats of robotics or whatever. If you throw in a human editor into the mix, you could probably imagine something much sooner. Indeed. Like tomorrow. Yeah. So the hybrid the hybrid version where it&apos;s basically a tool assisted TED talk, but that it is better than any TED talk a human could generate in one hundred hours or whatever, if you can sort of combine human discretion with A.I. horsepower. I suspect that&apos;s like our next year or two years from now kind of thing where it&apos;s just really quite good. That&apos;s that&apos;s really interesting. How do you view the impact of A.I. on jobs? There&apos;s obviously been the familiar story is that every White-Collar job is now up for destruction. What&apos;s what&apos;s your view there? You know, it&apos;s I think it&apos;s always hard to make these predictions. That is definitely the familiar story now. Five years ago, it was every blue collar job is up for destruction, maybe like last year it was. Every creative job is up for destruction because of things like Jukebox I. I think there will be an enormous impact on. The job market, and I really hate it, I think it&apos;s kind of gross when people like working on I pretend like there&apos;s not going to be or sort of say, oh, don&apos;t worry about it. It&apos;ll just all obviously better. It doesn&apos;t always obviously get better. I think what is true is. Every technological revolution produces a change in jobs, we always find new ones, at least so far. It&apos;s difficult to predict from where we&apos;re sitting now what the new ones will be and this technological revolution is likely to be. Again, it&apos;s always tempting to say this time it&apos;s different. Maybe I&apos;ll be totally wrong. But from what I see now, this technological revolution is likely to be more. Dramatic. More of a staccato note than most, and I think we as a society need to figure out how we&apos;re going to cushion everybody through that. I&apos;ve got my own ideas about how to do that. I, I wouldn&apos;t say that I have any reason to believe they&apos;re the right ones, but doing nothing and not really engaging with the magnitude of what&apos;s about to happen, I think it&apos;s like not an acceptable answer. So there&apos;s going to be huge impact. It&apos;s difficult to predict where it shows up the most. I think previous predictions have mostly been wrong, but I I&apos;d like to see us all as a society, certainly as a field, engage in what what the shifts we want to make to the social contract are to kind of get through that in a way that is maximally beneficial to everybody. I mean, in every past revolution, there&apos;s always been a space for humans to move to. That is, if you like, moving up the food chain, it&apos;s sort of we&apos;ve retreated to the things that humans could uniquely do, think better, be more creative and so forth. I guess the worry about A.I. is that in principle, I believe this, that there is no human cognitive feat that won&apos;t ultimately be doable, probably better by artificial general touch, simply because of the extra firepower that ultimately they can have, the vast knowledge they bring to the table and so forth. Is that basically right, that there is ultimately no safe sort of space where we can say, oh, but that would never be able to do that on a very long time horizon? I agree with you, but that&apos;s such a long time horizon. I think that, you know, like maybe we&apos;ve merged by that point, like maybe we&apos;re all plugged in and then, like, we&apos;re this sort of symbiotic thing. Like, I think there&apos;s an interesting example, as we were talking about a few minutes ago, where right now we have these systems that have sort of enormous horsepower but no steering wheel. It&apos;s like, you know , incredible capabilities, but no judgment. And there&apos;s like these obvious ways in which today even a human plus three is far better than either on their own. Many people speak about a world where it&apos;s sort of A.I. as this external threat you speak about. At some point, we actually merge with eyes in some way. What do you mean by that? There&apos;s a lot of different versions of what I think is possible there, you know, in some sense, I&apos;d argue the merge has already like begun the human technology merge like we have this thing in our hands that sort of dictates a lot of what we think, but it gives us real superpowers and that can go much, much further. Maybe it goes all the way to like the Elon Musk vision of neuro link and having our brains plugged into computers and sort of like literally we have a computer on the back of our head or goes the other direction and we get uploaded into one. Or maybe it&apos;s just that we all have a chat bot that kind of constantly steers us and helps us make better decisions than we could. But in any case, I think the fundamental thing is it&apos;s not like the humans versus the eyes competing to be the. Smartest sentient thing on earth or beyond. But it&apos;s that this idea of being on the same team. Hmm. I certainly get very excited by the sort of the medium term potential for creative people of all sorts if they&apos;re willing to expand their palette of possibilities. But with the use of A.I. to be willing to. I mean, the one thing that the history of technology has shown again and again is that something this powerful and with this much benefit is unstoppable and you will get rewarded for embracing it the most and the earliest. So talk about what can go wrong with that, so let&apos;s move away from just the sort of economic displacement factor. You were a co-founder of Open Eye because you saw existential risks to humanity from high today. What would you put as the sort of the most worrying of those risks? And how is open eye working to minimize? I still think all of the really horrifying risks exist. I am more confident, much more confident than I was five years ago when we started that there are technical things we can do about. How we build these systems and the research and the alignment that make us much more likely to end up in the kind of really wonderful camp, but, you know, like maybe open I fall behind and maybe somebody else feels ajai that thinks about it in a very different way or doesn&apos;t care as much as we&apos;d like about safety and the risks or how to strike a different trade off of how fast we should go with this and where we should sort of just say, like, you know, like let&apos;s push on for the economic benefits. But I think all of this sort of like, you know, traditionally what&apos;s been in the realm of sci fi risks are real and we should not ignore them. And I still lose sleep over them. And just to update people is artificial general intelligence. Right now, we have incredible examples of powerful AI operating on specific areas. Ajai is the ability of a computer mind to connect the dots and to make decisions at the same level of breadth that that humans have had. What&apos;s your sort of elevator pitch on Ajai about how to identify and how to think of it? Yeah, I mean, the way that I would say it is that for a while we were in this world of like very narrow A.I. , you know, that could like classify images of cats or whatever, more advanced stuff in that. But that kind of thing. We are now in the era of general purpose, AI, where you have these systems that are still very much imperfect tools, but that can generalize. And one thing like GPP three can write essays and translate between languages and write computer code and do very complicated search. It&apos;s like a single model that understands enough of what&apos;s really going on to do a broad array of tasks and learn new things quickly, sort of like people can. And then eventually we&apos;ll get to this other realm. Some people call it ajai, some people call ostler things. But I think it implies that the systems are like to some degree self directed, have some intentionality of their own is a simple summary to say that, like the fundamental risk is that there&apos;s the potential with general artificial intelligence of a sort of runaway effect of self-improvement that can happen far faster than any kind of humans can even keep up with, so that the day after you get to ajai, suddenly computers are thousands of times more advanced than us and we have no way of controlling what they do with that power. Yeah, and that is certainly in the risk space, which is that we build this thing and at some point somewhat suddenly, it&apos;s much more powerful than we are, we haven&apos;t really done the full merge yet. There&apos;s an event horizon there and it&apos;s sort of hard to see to the other side of it. Again, lots of reasons to think it will go OK. Lots of reasons to think we won&apos;t even get to that scenario. But that is something that. I don&apos;t think people should brush under the rug as much as they do, it&apos;s in the possibility space for sure, and in the possibility subspace of that is one where, like, we didn&apos;t actually do as good of a job on the alignment work as we thought. And this sort of child of humanity kind of acts in a very different way than we think. A framework that I find useful is to sort of think about like a two by two matrix, which is short timelines to ajai and long timelines to ajai and a slow take off and a fast take off on the other axis. And in the short timelines, fast take off quadrant, which is not where I think we&apos;re going to be. But if we get there, I think there&apos;s a lot of scenarios in the direction that you are describing that are worrisome. And we would want to spend a lot of effort planning for. I mean, the fact that a computer could start editing its own code and improving itself while we&apos;re asleep and you wake up in the morning and it&apos;s got smarter, that is the start of something super powerful and potentially scary. I have tremendous misgivings about letting my system, not one we have today, but one that we might not have and too many more years start editing its own code while we&apos;re not paying attention. I think that&apos;s the kind of thing that is worth a great deal of societal discussion about, you know, just because we can do that. Should we? Yes, because one of the things that&apos;s that&apos;s been most shocking to you about the last few years has been just the power of unintended consequences. It&apos;s like you don&apos;t have to have a belief that there&apos;s some sort of waking up of of an alien intelligence that suddenly decided it wants to wreak havoc on humans. That may never happen. What you can have is just incredible power that goes amok. So a lot of people would argue that what&apos;s happened in technology in the last few years is actually an example of that. You know, social media companies created these intelligences that were programmed to maximally harvest attention, for example, for sure. And they understand this from that turned out to be in some ways horrifying and extraordinarily damaging. Is that a meaningful sort of canary in the coal mine saying, look out, humanity, this could be really dangerous? And how how on earth do you protect against those kinds of unintended consequences? I think you raise a great point in general, which is these systems don&apos;t have to wish ill to humanity to cause ill just when you have, like, very powerful systems. I mean, unintended consequences for sure. But another version of that is and I think this applies at the technical level, at the company level, at the societal level, incentives are superpower&apos;s. Charlie Munger had this thing on, which is incentives are so powerful that if you can spend any time whatsoever working on the incentive system, that&apos;s what you should do before you work on anything else. And I really believe that. And I think that applies to the individual models we build and what their reward functions look like. I think it applies to society in a big way, and I think it applies to our corporate structure at open. I you know, we sort of observe that if you have very well-meaning people, but they have this incentive to sort of maximize attention harvesting and profit forever through no one&apos;s ill intentions, that leads to a quite undesirable outcome. And so we set up opening is this thing called a capped profit model specifically so that we don&apos;t have the system incentive to just generate maximum value forever with an AGI that seems like obviously quite broken. But even though we knew that was bad and even though we all like to think of ourselves as good people, it took us a long time to figure out the right structure, to figure out a charter that&apos;s going to govern us and a set of incentives that we believe will let us do our work. And kind of these we have these like three elements that we talk about a lot research sort of engineering, development and deployment policy and safety. Put those all together under a system where you don&apos;t have to rely on. Anything but the natural incentives to push in a direction that we hope will minimize the sort of negative unintended consequences. So help me understand this, because this is I think this is confusing to some people. So you started opening. I initially I think Elon Musk, the co-founder, and there was a group of you and the argument was this technology is too powerful to be left, developed in secret and to be left developed purely by corporations who have whatever incentive they may have. We need a nonprofit that will develop and share knowledge openly. First of all, just even at that early stage, some people were confused about this. It was saying if this thing is so dangerous, why on earth would you want to make it secrets even more available? Well, maybe giving the tools to that sort of AI terrorist in his bedroom somewhere, I think I think we got misunderstood in the way we were talking about that. We certainly don&apos;t think that the right thing to do is to, like, build this a super weapon and hand it to a terrorist. That&apos;s obviously awful. One of the reasons that we like our API model is it lets us make the most powerful AI technology anyone in the world has, as far as we know, available to ever would like to use it, but to put some controls on its usage. And also, if we make a mistake, to be able to pull it back or change it or tweak it or improve it or whatever. But we do want to put and this is continued will continue to be true with appropriate restrictions and guardrails, very powerful technology in the hands of people. I think that is fair. I think that will lead to the best results for the society as a whole. And I think it will sort of maximize benefit. But that&apos;s very different than sort of shipping the whole model and saying, here, do whatever you want with it. We&apos;re able to enforce rules on it. We also think and this is part of the mission that like something the field was doing a lot of that we didn&apos;t feel good about was sort of saying like, oh, we&apos;re going to keep the pace of progress and capabilities secret. That doesn&apos;t feel right, because I think we do need a societal conversation about what&apos;s what&apos;s going on here, what the impacts are going to be. And so we although we don&apos;t always say, like, you know, here&apos;s the super weapon, hopefully we do try to say, like, this is really serious. This is a big deal. This is going to affect all of us. We need to have a big conversation about what to do with it. Help me understand the structure a bit better, because you definitely surprised much people when you announced that Microsoft were putting a billion dollars into the organization and in return, I guess they get certain exclusive licensing rights. And so, for example, they are the exclusive licensee of CP3. So talk about that structure of how you win. Microsoft presumably have invested not purely for altruistic purposes. They think that they will make money on that billion dollars. I sure hope they do. I love capitalism, but I think that I really loved even more about Microsoft as a partner. And I&apos;ll talk about the structure and the exclusive license in a minute is that we like went around to people that might find us. And we said one of the things here is that we&apos;re going to try to make you some money. But like Adjei going well is more important. And we need you to sign this document that says if things don&apos;t go the way we think and we can&apos;t make you money like you just cheerfully walk away from it and we do the right thing for humanity. And they were like, yes, we are enthusiastic about that. We get that the mission comes first here. So again, I hope a phenomenal investment for them. But they were like they really pleasantly surprised us on the upside of how aligned they were with us, about how strange the world may get here and the need for us to have flexibility and put our mission first, even if that means they lose all their money, which I hope they don&apos;t and don&apos;t think they will. So the way it&apos;s set up is that if at some point in the coming year or two, two years, Microsoft decide that there&apos;s some incredible commercial opportunity that they could realize out of the eye that you&apos;ve built and you feel actually, no, that&apos;s that&apos;s damaging. You can block it. You can veto it. Correct. So the four most powerful version of three and its successors are available via the API, and we intend for that to continue. What Microsoft has is the ability to sort of put that model directly into their own technology. If they want to do that. We don&apos;t plan to do that with other people because we can&apos;t have all these controls that we talked about earlier. But they&apos;re like a close trusted partner and they really care about safety, too. But our goal is that anybody who wants to use the API can have the most powerful versions of what we&apos;ve trained. And the structure of the API lets us continue to increase the safety and fix problems when we find them. But but the structure. So we start out as a non-profit, as you said, we realized pretty quickly that although we went into this thinking that the way to get to ajai would be about smarter and smarter algorithms, that we just needed bigger and bigger computers as well. And that was going to require a scale of capital that no one will, at least certainly not me, could figure out how to raise is a nonprofit. We also needed to sort of be able to compensate very highly compensated, talented individuals that do this, but are full for profit company had runaway incentives problem, among other things. Also just one about sort of fairness in society and wealth concentration that didn&apos;t feel right to us either. And so we came up with this kind of hybrid where we have a nonprofit that governs what we do, and it has a subsidiary, LLC, that we structure in a way to make a fixed amount of profit so that all of our investors and employees, hopefully if things go how we like, if not no one gets any money, but hopefully they get to make this one time great return on their investment or the time that they spent it open their equity here. And then beyond that, all the value flows back to the nonprofit and we figure out how to share it as fairly as we can with the world. And I think that this structure and this nonprofit with this very strong charter in place and everybody who joins signing up for the mission come in first and the fact the world may get strange, I think that. That was at least the best idea we could come up with, and I think it feels so far like the incentive system is working, just as I sort of watch the way that we and our partners make decisions. But if I read it right, the cap on the gain that investors can make is 100 Axum. It&apos;s a massive call that was for our very first round investors. It&apos;s way, way lower. Like as we now take a bit of capital, it&apos;s way, way lower. So your deal with Microsoft isn&apos;t you can only make the first hundred billion dollars. I don&apos;t know. It&apos;s way lower than after that. We&apos;re giving it to the world. It&apos;s way lower than that. Have you disclosed what I don&apos;t know if we have, so I won&apos;t accidentally do it now. All right. OK, so explain a bit more about the charter and how it is that you. Hope to avoid or I guess help contribute to an eye that is safe for humanity. What do you see as the keys to us avoiding the worst mistakes and really holding on to something that&apos;s that&apos;s beneficial for humanity? My answer there is actually more about, like technical and societal issues than the charter. So if it&apos;s OK for me to answer it from that perspective, sure. OK, I&apos;m happy to talk about the charter to. I think this question of alignment that we talked about a little earlier is paramount, and then I think to understand that it&apos;s useful to differentiate between accidental misuse of a system and intentional misuse of a system. So like intentional would be a bad actor saying, I&apos;ve got this powerful system, I&apos;m going to use it to like hack into all the computers in the world and wreak havoc on the power grids. And accidental would be kind of the Nick Bostrom make a lot of paper clips and view humans as collateral damage in both cases. But to varying degrees, if we can really, truly, technically solve the alignment problem and the societal problem of deciding to which set of human values do we align, then the systems understand right and wrong, and they understand probably better than we ever can, unintended consequences from complex actions and very complex systems. And, you know, if we can train a system which is like. Don&apos;t harm humanity and the system can really understand what we mean when we say that, again, who is we and what does that have some asterisks on them? Sorry, go ahead. Well, that&apos;s if they could understand what it means to not harm humanity, that there&apos;s a lot wrapped up in that sentence. Because what&apos;s been so striking to me about efforts so far is that they seem to have been based on a very naive view of human nature. Go back to the sort of Facebook and Twitter examples of, well, the engineers building some of the systems would say we&apos;ve just designed them around what humans want to do. You said, well, if someone wants to click on something, we will give them more of that thing. And what could possibly be wrong with that? We&apos;re just supporting human choice, ignoring the fact that humans are complicated, farshid animals for sure, who are constantly making choices, that a more effective version of themselves would agree is not in their long term interests. So that&apos;s one part of it. And then you&apos;ve got layered on top of that or the complications of systemic complexity where, you know, multiple choices by thousands of people end up creating a reality that possibly have designed for how how to cut through that. Like an AI has to make a decision based on a moment, on a specific data set. As those decisions get more powerful, how can we be confident that they don&apos;t lead to this sort of system crashing basically in some way? I think that I&apos;ve heard a lot of behavioral psychologists and other people that have studied this say in different ways, are that I hate to keep picking on Facebook, but we can do it one more time since we&apos;re on the topic. Maybe you can&apos;t in any given moment in night where you&apos;re tired and you have a stressful day, stop yourself from the dopamine hit of scrolling and Instagram, even though you know that&apos;s bad for you and it&apos;s not leading to your best life. But if you were asked in a reflective moment where you were sort of fully alert and thoughtful, do you want to spend as much time as you do scrolling through Instagram? Does it make you happier or not? You would actually be able to give like the right long term answer? It&apos;s sort of the spirit is willing, but the flesh is weak kind of moment. And one thing that I am hopeful is that humans do know what we want and what. On the whole, and presented with research or sort of an objective view about what makes us happy and doesn&apos;t we&apos;re pretty, what&apos;s so great about it, they&apos;re pretty good. But in any particular moment, we are subjected to our animal instincts and it is easy for the lower brain to take over the eye. Well , I think be an even higher brain. And as we can teach it, you know, here is what we really do value. Here&apos;s what we really do want. It will help us make better decisions than we are capable of, even in our best moments. So is that being proposed and talked about as an actual rule? Because it strikes me that there is something potentially super profound here to introduce some kind of rule for development of AIDS that they have to tap into not. What humans one, which is an ill defined question, but as to what humans in reflective mode want. Yeah, we talk about this a lot. I mean, do you see a real chance where something like that could be incorporated as a sort of an absolute golden rule and and if you like, spread around the community so that it seeps into corporations and elsewhere? Because that I&apos;ve seen no evidence that, well, a little corporation that was potentially a game changer. Corporations have this weird incentive problem. Right. What I was trying to speak about was something that I think should be technologically possible , and that&apos;s something that we as a society should demand. And I think it is technically possible for this to be sort of like a layer above the neocortex that makes even better decisions for us and our welfare and our long term happiness and fulfillment than we could make on our own. And I think it is possible for us as a society to demand that. And if we can do like a pincer move between what the technology is capable of and what we what we as society demand, maybe we can make everybody in the middle that way. I mean, there are instances of even though companies have their incentives to make money and so forth, they also in the knowledge age. Can&apos;t make money if they have pissed off too many of their employees and customers and investors by analogy of the climate space right now, you can see more and more companies, even those that are emitting huge amounts of carbon dioxide, saying, wait a sec, we&apos;re struggling to recruit talented people because they don&apos;t want to work for someone who&apos;s evil. And their customers are saying, we don&apos;t want to buy something that is evil. And so, you know, ultimately you can picture processes where they do better. And I I believe that most engineers, for example, work in Silicon Valley. Companies are actually good people who want to design great products for humanity. I think that the people who run these companies want to be a net contribution to humanity. It&apos;s we&apos;ve we&apos;ve rushed really quickly and design stuff without thinking it through properly. And it&apos;s led to a mess up. So it&apos;s like, OK, don&apos;t move fast, break things, slow down and build beautiful things that are built on a real version of human nature and on a real version of system complexity and the risks associated with systemic complexity. Is that the agenda that fundamentally you think that you can push somehow? Yes, but I think the way we can push it is by getting the incentive system right. I think most people are fundamentally extremely good. Very few people wake up in the morning thinking about how can I make the world a worse place? But the incentive systems that we&apos;re in are so powerful. And even those engineers who join with the absolute best of intentions get sucked into this world where they&apos;re like trying to go up from it all for and five or whatever Facebook calls those things and you like, it&apos;s pretty exciting. You get caught up playing the game, you&apos;re rewarded for kind of doing things that move the company&apos;s key metrics. It&apos;s like fun to get promoted. It feels good to make more money and the incentive systems of the company. And that&apos;s what it rewards. An individual performance are maybe like not what we all want. And here I don&apos;t want to pick on Facebook at all because I think there&apos;s versions of this at play it like every big tech company, including in some ways I&apos;m sure it open I but to the degree that we can better align the incentives of companies with the welfare of society and then the incentives of an individual at those companies within the now realign incentives for those companies, the more likely we are to be able to have things like ajai that. Follow an incentive system of. What we want in our most reflective best moments and are even better than what we could think of ourselves is is it still the vision for open eye that you will get to? Artificial general intelligence ahead of. The corporations, so that you can somehow put a stake in the ground and build it the right way. Is that really a realistic thing to to dream for? And if not, how do you live up to the mission and help ensure that this thing doesn&apos;t go off the rails? I think it is. Look, I certainly don&apos;t think we will be the only group to build an AGI, but I think we could be the first. And I think if you are the first, you have a lot of norms that empower. And I think you&apos;ve already seen that. You know, we have released some of the most powerful systems to date. And I think the way that we have done that kind of in controlled release where we&apos;ve released a bigger model than a bigger one than a bigger one, and we sort of try and talk about the potential misuse cases and we try to like talk about the importance of releasing this behind an API so that you can make changes. Other groups have followed suit in some of those directions, and I think that&apos;s good. So, yes, I don&apos;t think we can be the only I do think we can be ahead. And if we are ahead, I think we can use that leverage to hopefully push people in a better direction or maybe we&apos;re wrong and somebody else has a better direction. We&apos;re doing something about do you have a structural advantage in that your mission is to do this for everyone as opposed to for some corporate objective. And that that that allows you that. Why is it that we came out of open eye and not someone else? It&apos;s like it&apos;s surprising in some ways when you&apos;re up against so much money and so much talent in these other companies that you came up with this platform ahead of. You know, in some sense it&apos;s surprising and in some sense, like the startup wins most of the time, like I&apos;m a huge believer in startups as the best force for innovation we have in the world today. I talked a little bit about how we combine these three. Different clans of research, engineering and sort of safety and policy that don&apos;t normally combine well and I think we have an unusual strength, there were clearly like well funded. We have super talented people. But what we really have is like intense focus and self belief that what we&apos;re doing is possible and good. And I appreciate the implied compliment. But, you know, we, like, work really hard. And if we stop doing that, I&apos;m sure someone would run by us fast. Tell us a bit more about some of your prior life sentences. Yeah, for several years, you were running Y Combinator, which had incredible impact on some 70 companies. There are so many startup stories that began at Y Combinator. What were key drivers in your own life that took you on the path you&apos;re on? And how did that path end up at Y Combinator? No exaggeration. I think I have back to back had the two jobs that are at least the most interesting to me in all of Silicon Valley. I, I was I went to college to study computer science. I was a major computer nerd growing up. I knew like a little bit about startups, but not very much. I started working on this project the same year I started working on that. This thing called Y Combinator started and funded me and my co-founders. And we dropped out of school and did this company, which I ran for like seven years. And then after that I got acquired. I had stayed close to my comment the whole time. I thought it was just this incredible group of people and spirit and set of incentives and just badly misunderstood by most of the world, but obvious to everyone within it that it was going to create huge amounts of value and do a lot of new things. My company had acquired PJI, who is the founder of ICI, and like truly one of the most incredible humans and business people. And Paul Burrell, Paul Graham asked me if I wanted to run it. And kind of like the central learning of my career, why I individual startups has been that if you really scale them up, remarkable things can happen. And I. I did it and I was like, one of the things that would make this exciting for me personally motivating would be if I could sort of push it in the direction of doing these hard tech companies, one of which became open. I describe actually what Y Combinator is, you know, how many people come through it to give us a couple of stories of its impact. Yeah. So you basically apply as a handful of people and an idea, maybe a prototype and say, I would like to start a company and will you please fund me? And we review those applications and we I shouldn&apos;t say we anymore. I guess they fund four hundred companies a year. You get about one hundred and fifty thousand dollars while she takes about seven percent ownership and then gives you lots of advice and then networking and sort of this like fast track program for starting a startup. I haven&apos;t looked at this in a while, but at one point a significant fraction of the billion dollar plus companies in the US that got started. It all came through the Wiki program, some recently in the news ones have been like Airbnb, Jordache, Coinbase, insta card stripe. And I think it&apos;s just it has become an incredible way to help. People who understand technology get a three month course in business, but instead of like herding you with an MBA, we actually teach you the things that matter and kind of go on to do incredible, incredible work. What is it about entrepreneurs? Why do they matter? Some people just find them kind of annoying. But I think you would argue I think I would argue that they have done as much as anyone to shape the future. Why ? What is it about them? I think it is the ability to take. And idea and by force of will to make it happen in the world and in an incentive system that rewards you for making the most impact on the most people like in our system. That&apos;s how we get most of the things that that we use. That&apos;s how we got the computer that I&apos;m using, the software I&apos;m using to talk to you on it. Like all of this, you know, everyone in life, everything has a balance sheet. There&apos;s plenty of very annoying things about them. And there&apos;s plenty of very annoying things about the system that sort of idolizes them. But we do get something really important in return. And I think that as a force for making things that make all of our lives better happen, it&apos;s very cool. Otherwise, you know, like if you have, like, a great idea, but you don&apos;t actually do anything useful with it for people, that&apos;s still cool. It&apos;s still intellectually interesting. But like, there&apos;s got to be something about the reward function in society that is like, did you actually do something useful? Did you create value? And I think entrepreneurship and startups are a wonderful way to do that. You know, we get all these great software companies. But I also think it&apos;s like how we&apos;re going to get ajai, how we&apos;re going to get nuclear fusion, how we&apos;re going to get life extension. And like on any of those topics are a long list of other things I could point to. There&apos;s like a number of startups that I think are doing incredible work, some of which will actually deliver. It is a truly amazing thing when you put the camera back and to believe that a human being could be lying awake at night and something pops inside their mind as a patterning of the neurons in their brain that is effectively them saying, aha, I can see a way where the future could be better and and they can actually picture it. And then they wake up and then they talk to other people and they persuade them and they persuade investors and so forth. And the fact that this this system can happen and that they can then actually change the history changes in some sense. It is mind boggling that that happens that way and it happens again and again. So you&apos;ve seen so many of these stories happen. What would you say? Is the is there a key thing that differentiates good entrepreneurs from others? If you could double down on one trait, what would it be? If I could pick only one, I would pick determination. I think that is the biggest predictor of success, the biggest differentiator predictor. And if you would allow a second, I would pick like communication skills or evangelism or something in that direction as well. There are all of the obvious ones that matter, like intelligence, but there&apos;s like a lot of smart people in the world. And when I look back at kind of the thousands of entrepreneurs I&apos;ve worked with, all of many of whom were like quite capable, I would say that&apos;s like one and two of the surprisingly differentiated characteristics. What it&apos;s it&apos;s what I look at, the different things that you&apos;ve built and you&apos;re working on. I mean, it could not be more foundational for the future. I mean, entrepreneurship. I know this is I agree that this is really what has driven the future. Do you see some people get really now they look at Silicon Valley and they look at this story and they worry about the culture. Right. That it&apos;s this is a bro culture. Do you see prospects of that changing anytime soon? And would you welcome it? Can we get better companies by really working to expand a group of people who can be entrepreneurs and who can contribute to aid, for example? For sure. And in fact, I think I&apos;m hopeful, since these are the two things I&apos;ve thought the most about. I&apos;m excited for the day when someone combines them and uses A.I. to better select who did more fairly, maybe even select who to fund and how to advise them and really kind of make entrepreneurship super widely available that will lead to like better outcomes and sort of more societal wealth for all of us. So are. So, yeah, I think. Broadening the set of people able to start companies and that sort of get the resources that you need, that is like an unequivocally good thing and it&apos;s something that I think Silicon Valley is making some progress in. But I hope we see a lot more. And I do really, truly think that the technology industry entrepreneurship is one of the greatest forces for self betterment. If we can just figure out how to be a little bit more inclusive in how we do things. My last question today is about ideas were spreading. If you could inject one idea into the mind of everyone listening, what what would the idea be? We&apos;ve touched on it a bunch, but the one idea would be the ajai really is going to happen. You have to engage with it seriously, and you shouldn&apos;t just listen to this and then brush aside and go about life as if it&apos;s not going to happen because it is going to affect everything. And we will all we all, I think, have an obligation, but also an opportunity to figure out what not means and how we want the world and this sort of one time shift to go on. I&apos;m kind of awed by the breadth of things are engaged with. Thank you so much for spending so much time sharing your vision. Thanks so much for having me. OK, that&apos;s it for today. You can read more about open eyes, vision and progress at open eye dotcom. If you want to try playing with yourself, it&apos;s a little tricky. You have to find a website that has licensed the API. The one I went to was philosopher ehi dot com, where you just you pay a few dollars to get access to a very strange mind. That&apos;s actually quite a lot of fun. The interview is part of the TED Audio Collective, a collection of podcasts dedicated to sparking curiosity and sharing ideas that matter. This show is produced by Kim Net2Phone Pittas and edited by Grace Rubenstein and Sheila Boffano, Sambor Islamic Sir. Fact Check is by Paul Durbin and special thanks to Michele Quent, Colin Helmes and Anna Felin. If you like the show, please write and review it. It helps other people find us. We read every review, so thanks so much for listening. See you next time.","embedUrl":"https://embed.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021","interactionStatistic":{"@type":"InteractionCounter","interactionType":{"@type":"WatchAction"},"userInteractionCount":1051995}}</script><link rel="preload" as="image" imageSrcSet="https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 640w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 750w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 828w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1080w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1200w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1920w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 2048w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 3840w" imageSizes="100vw" data-next-head=""/><script type="text/javascript" src="https://cdn.cookielaw.org/consent/eb3a3101-85ef-45e5-a75f-dbd35e8d0b4d/OtAutoBlock.js"></script><script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-domain-script="eb3a3101-85ef-45e5-a75f-dbd35e8d0b4d"></script><script>function r(){window.__userHasConsentedToTargetingCookies=window.OnetrustActiveGroups.includes("C0004");let t=new Event("OnetrustActiveGroupsLoaded");window.dispatchEvent(t)}</script><script type="text/javascript" id="vwoCode">
            window._vwo_code || (function() {
            var account_id=613676,
            version=2.1,
            settings_tolerance=5000,
            hide_element='body',
            hide_element_style = 'opacity:0 !important;filter:alpha(opacity=0) !important;background:none !important',
            /* DO NOT EDIT BELOW THIS LINE */
            f=false,w=window,d=document,v=d.querySelector('#vwoCode'),cK='_vwo_'+account_id+'_settings',cc={};try{var c=JSON.parse(localStorage.getItem('_vwo_'+account_id+'_config'));cc=c&&typeof c==='object'?c:{}}catch(e){}var stT=cc.stT==='session'?w.sessionStorage:w.localStorage;code={use_existing_jquery:function(){return typeof use_existing_jquery!=='undefined'?use_existing_jquery:undefined},library_tolerance:function(){return typeof library_tolerance!=='undefined'?library_tolerance:undefined},settings_tolerance:function(){return cc.sT||settings_tolerance},hide_element_style:function(){return'{'+(cc.hES||hide_element_style)+'}'},hide_element:function(){if(performance.getEntriesByName('first-contentful-paint')[0]){return''}return typeof cc.hE==='string'?cc.hE:hide_element},getVersion:function(){return version},finish:function(e){if(!f){f=true;var t=d.getElementById('_vis_opt_path_hides');if(t)t.parentNode.removeChild(t);if(e)(new Image).src='https://dev.visualwebsiteoptimizer.com/ee.gif?a='+account_id+e}},finished:function(){return f},addScript:function(e){var t=d.createElement('script');t.type='text/javascript';if(e.src){t.src=e.src}else{t.text=e.text}d.getElementsByTagName('head')[0].appendChild(t)},load:function(e,t){var i=this.getSettings(),n=d.createElement('script'),r=this;t=t||{};if(i){n.textContent=i;d.getElementsByTagName('head')[0].appendChild(n);if(!w.VWO||VWO.caE){stT.removeItem(cK);r.load(e)}}else{var o=new XMLHttpRequest;o.open('GET',e,true);o.withCredentials=!t.dSC;o.responseType=t.responseType||'text';o.onload=function(){if(t.onloadCb){return t.onloadCb(o,e)}if(o.status===200){w._vwo_code.addScript({text:o.responseText})}else{w._vwo_code.finish('&e=loading_failure:'+e)}};o.onerror=function(){if(t.onerrorCb){return t.onerrorCb(e)}w._vwo_code.finish('&e=loading_failure:'+e)};o.send()}},getSettings:function(){try{var e=stT.getItem(cK);if(!e){return}e=JSON.parse(e);if(Date.now()>e.e){stT.removeItem(cK);return}return e.s}catch(e){return}},init:function(){if(d.URL.indexOf('__vwo_disable__')>-1)return;var e=this.settings_tolerance();w._vwo_settings_timer=setTimeout(function(){w._vwo_code.finish();stT.removeItem(cK)},e);var t;if(this.hide_element()!=='body'){t=d.createElement('style');var i=this.hide_element(),n=i?i+this.hide_element_style():'',r=d.getElementsByTagName('head')[0];t.setAttribute('id','_vis_opt_path_hides');v&&t.setAttribute('nonce',v.nonce);t.setAttribute('type','text/css');if(t.styleSheet)t.styleSheet.cssText=n;else t.appendChild(d.createTextNode(n));r.appendChild(t)}else{t=d.getElementsByTagName('head')[0];var n=d.createElement('div');n.style.cssText='z-index: 2147483647 !important;position: fixed !important;left: 0 !important;top: 0 !important;width: 100% !important;height: 100% !important;background: white !important;';n.setAttribute('id','_vis_opt_path_hides');n.classList.add('_vis_hide_layer');t.parentNode.insertBefore(n,t.nextSibling)}var o='https://dev.visualwebsiteoptimizer.com/j.php?a='+account_id+'&u='+encodeURIComponent(d.URL)+'&vn='+version;if(w.location.search.indexOf('_vwo_xhr')!==-1){this.addScript({src:o})}else{this.load(o+'&x=true')}}};w._vwo_code=code;code.init();})();(function(){var i=window;function t(){if(i._vwo_code){var e=t.hidingStyle=document.getElementById('_vis_opt_path_hides')||t.hidingStyle;if(!i._vwo_code.finished()&&!_vwo_code.libExecuted&&(!i.VWO||!VWO.dNR)){if(!document.getElementById('_vis_opt_path_hides')){document.getElementsByTagName('head')[0].appendChild(e)}requestAnimationFrame(t)}}}t()})();
          </script><script type="text/javascript">window.NREUM||(NREUM={});NREUM.info = {"agent":"","beacon":"bam.nr-data.net","errorBeacon":"bam.nr-data.net","licenseKey":"c745aa170b","applicationID":"1145147364","agentToken":null,"applicationTime":4061.753703,"transactionName":"blcDNxEEDEVSWhdbDFcdNgYBIxBXXlwUXRFSZxMKTCsHTkdTEB0kfGZOTBcEDl1AFjgcTRdBDRYEOA==","queueTime":0,"ttGuid":"1718d13d46c78678"}; (window.NREUM||(NREUM={})).init={privacy:{cookies_enabled:true},ajax:{deny_list:["bam.nr-data.net"]},session_trace:{sampling_rate:0.0,mode:"FIXED_RATE",enabled:true,error_sampling_rate:0.0},distributed_tracing:{enabled:true}};(window.NREUM||(NREUM={})).loader_config={agentID:"1386091612",accountID:"1877911",trustKey:"1877911",xpid:"VQ4AVl9SCRABVVVXBgUAVVIF",licenseKey:"c745aa170b",applicationID:"1145147364"};;/*! For license information please see nr-loader-spa-1.284.1.min.js.LICENSE.txt */
(()=>{var e,t,r={8122:(e,t,r)=>{"use strict";r.d(t,{a:()=>i});var n=r(944);function i(e,t){try{if(!e||"object"!=typeof e)return(0,n.R)(3);if(!t||"object"!=typeof t)return(0,n.R)(4);const r=Object.create(Object.getPrototypeOf(t),Object.getOwnPropertyDescriptors(t)),o=0===Object.keys(r).length?e:r;for(let a in o)if(void 0!==e[a])try{if(null===e[a]){r[a]=null;continue}Array.isArray(e[a])&&Array.isArray(t[a])?r[a]=Array.from(new Set([...e[a],...t[a]])):"object"==typeof e[a]&&"object"==typeof t[a]?r[a]=i(e[a],t[a]):r[a]=e[a]}catch(e){(0,n.R)(1,e)}return r}catch(e){(0,n.R)(2,e)}}},2555:(e,t,r)=>{"use strict";r.d(t,{Vp:()=>c,fn:()=>s,x1:()=>u});var n=r(384),i=r(8122);const o={beacon:n.NT.beacon,errorBeacon:n.NT.errorBeacon,licenseKey:void 0,applicationID:void 0,sa:void 0,queueTime:void 0,applicationTime:void 0,ttGuid:void 0,user:void 0,account:void 0,product:void 0,extra:void 0,jsAttributes:{},userAttributes:void 0,atts:void 0,transactionName:void 0,tNamePlain:void 0},a={};function s(e){try{const t=c(e);return!!t.licenseKey&&!!t.errorBeacon&&!!t.applicationID}catch(e){return!1}}function c(e){if(!e)throw new Error("All info objects require an agent identifier!");if(!a[e])throw new Error("Info for ".concat(e," was never set"));return a[e]}function u(e,t){if(!e)throw new Error("All info objects require an agent identifier!");a[e]=(0,i.a)(t,o);const r=(0,n.nY)(e);r&&(r.info=a[e])}},9417:(e,t,r)=>{"use strict";r.d(t,{D0:()=>h,gD:()=>g,xN:()=>p});var n=r(3333);const i=e=>{if(!e||"string"!=typeof e)return!1;try{document.createDocumentFragment().querySelector(e)}catch{return!1}return!0};var o=r(2614),a=r(944),s=r(384),c=r(8122);const u="[data-nr-mask]",d=()=>{const e={feature_flags:[],experimental:{marks:!1,measures:!1,resources:!1},mask_selector:"*",block_selector:"[data-nr-block]",mask_input_options:{color:!1,date:!1,"datetime-local":!1,email:!1,month:!1,number:!1,range:!1,search:!1,tel:!1,text:!1,time:!1,url:!1,week:!1,textarea:!1,select:!1,password:!0}};return{ajax:{deny_list:void 0,block_internal:!0,enabled:!0,autoStart:!0},distributed_tracing:{enabled:void 0,exclude_newrelic_header:void 0,cors_use_newrelic_header:void 0,cors_use_tracecontext_headers:void 0,allowed_origins:void 0},get feature_flags(){return e.feature_flags},set feature_flags(t){e.feature_flags=t},generic_events:{enabled:!0,autoStart:!0},harvest:{interval:30},jserrors:{enabled:!0,autoStart:!0},logging:{enabled:!0,autoStart:!0},metrics:{enabled:!0,autoStart:!0},obfuscate:void 0,page_action:{enabled:!0},page_view_event:{enabled:!0,autoStart:!0},page_view_timing:{enabled:!0,autoStart:!0},performance:{get capture_marks(){return e.feature_flags.includes(n.$v.MARKS)||e.experimental.marks},set capture_marks(t){e.experimental.marks=t},get capture_measures(){return e.feature_flags.includes(n.$v.MEASURES)||e.experimental.measures},set capture_measures(t){e.experimental.measures=t},capture_detail:!0,resources:{get enabled(){return e.feature_flags.includes(n.$v.RESOURCES)||e.experimental.resources},set enabled(t){e.experimental.resources=t},asset_types:[],first_party_domains:[],ignore_newrelic:!0}},privacy:{cookies_enabled:!0},proxy:{assets:void 0,beacon:void 0},session:{expiresMs:o.wk,inactiveMs:o.BB},session_replay:{autoStart:!0,enabled:!1,preload:!1,sampling_rate:10,error_sampling_rate:100,collect_fonts:!1,inline_images:!1,fix_stylesheets:!0,mask_all_inputs:!0,get mask_text_selector(){return e.mask_selector},set mask_text_selector(t){i(t)?e.mask_selector="".concat(t,",").concat(u):""===t||null===t?e.mask_selector=u:(0,a.R)(5,t)},get block_class(){return"nr-block"},get ignore_class(){return"nr-ignore"},get mask_text_class(){return"nr-mask"},get block_selector(){return e.block_selector},set block_selector(t){i(t)?e.block_selector+=",".concat(t):""!==t&&(0,a.R)(6,t)},get mask_input_options(){return e.mask_input_options},set mask_input_options(t){t&&"object"==typeof t?e.mask_input_options={...t,password:!0}:(0,a.R)(7,t)}},session_trace:{enabled:!0,autoStart:!0},soft_navigations:{enabled:!0,autoStart:!0},spa:{enabled:!0,autoStart:!0},ssl:void 0,user_actions:{enabled:!0,elementAttributes:["id","className","tagName","type"]}}},l={},f="All configuration objects require an agent identifier!";function h(e){if(!e)throw new Error(f);if(!l[e])throw new Error("Configuration for ".concat(e," was never set"));return l[e]}function p(e,t){if(!e)throw new Error(f);l[e]=(0,c.a)(t,d());const r=(0,s.nY)(e);r&&(r.init=l[e])}function g(e,t){if(!e)throw new Error(f);var r=h(e);if(r){for(var n=t.split("."),i=0;i<n.length-1;i++)if("object"!=typeof(r=r[n[i]]))return;r=r[n[n.length-1]]}return r}},5603:(e,t,r)=>{"use strict";r.d(t,{a:()=>c,o:()=>s});var n=r(384),i=r(8122);const o={accountID:void 0,trustKey:void 0,agentID:void 0,licenseKey:void 0,applicationID:void 0,xpid:void 0},a={};function s(e){if(!e)throw new Error("All loader-config objects require an agent identifier!");if(!a[e])throw new Error("LoaderConfig for ".concat(e," was never set"));return a[e]}function c(e,t){if(!e)throw new Error("All loader-config objects require an agent identifier!");a[e]=(0,i.a)(t,o);const r=(0,n.nY)(e);r&&(r.loader_config=a[e])}},3371:(e,t,r)=>{"use strict";r.d(t,{V:()=>f,f:()=>l});var n=r(8122),i=r(384),o=r(6154),a=r(9324);let s=0;const c={buildEnv:a.F3,distMethod:a.Xs,version:a.xv,originTime:o.WN},u={customTransaction:void 0,disabled:!1,isolatedBacklog:!1,loaderType:void 0,maxBytes:3e4,onerror:void 0,ptid:void 0,releaseIds:{},appMetadata:{},session:void 0,denyList:void 0,timeKeeper:void 0,obfuscator:void 0,harvester:void 0},d={};function l(e){if(!e)throw new Error("All runtime objects require an agent identifier!");if(!d[e])throw new Error("Runtime for ".concat(e," was never set"));return d[e]}function f(e,t){if(!e)throw new Error("All runtime objects require an agent identifier!");d[e]={...(0,n.a)(t,u),...c},Object.hasOwnProperty.call(d[e],"harvestCount")||Object.defineProperty(d[e],"harvestCount",{get:()=>++s});const r=(0,i.nY)(e);r&&(r.runtime=d[e])}},9324:(e,t,r)=>{"use strict";r.d(t,{F3:()=>i,Xs:()=>o,Yq:()=>a,xv:()=>n});const n="1.284.1",i="PROD",o="CDN",a="^2.0.0-alpha.18"},6154:(e,t,r)=>{"use strict";r.d(t,{A4:()=>s,OF:()=>d,RI:()=>i,WN:()=>h,bv:()=>o,gm:()=>a,lR:()=>f,m:()=>u,mw:()=>c,sb:()=>l});var n=r(1863);const i="undefined"!=typeof window&&!!window.document,o="undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self.navigator instanceof WorkerNavigator||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis.navigator instanceof WorkerNavigator),a=i?window:"undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis),s="complete"===a?.document?.readyState,c=Boolean("hidden"===a?.document?.visibilityState),u=""+a?.location,d=/iPad|iPhone|iPod/.test(a.navigator?.userAgent),l=d&&"undefined"==typeof SharedWorker,f=(()=>{const e=a.navigator?.userAgent?.match(/Firefox[/\s](\d+\.\d+)/);return Array.isArray(e)&&e.length>=2?+e[1]:0})(),h=Date.now()-(0,n.t)()},7295:(e,t,r)=>{"use strict";r.d(t,{Xv:()=>a,gX:()=>i,iW:()=>o});var n=[];function i(e){if(!e||o(e))return!1;if(0===n.length)return!0;for(var t=0;t<n.length;t++){var r=n[t];if("*"===r.hostname)return!1;if(s(r.hostname,e.hostname)&&c(r.pathname,e.pathname))return!1}return!0}function o(e){return void 0===e.hostname}function a(e){if(n=[],e&&e.length)for(var t=0;t<e.length;t++){let r=e[t];if(!r)continue;0===r.indexOf("http://")?r=r.substring(7):0===r.indexOf("https://")&&(r=r.substring(8));const i=r.indexOf("/");let o,a;i>0?(o=r.substring(0,i),a=r.substring(i)):(o=r,a="");let[s]=o.split(":");n.push({hostname:s,pathname:a})}}function s(e,t){return!(e.length>t.length)&&t.indexOf(e)===t.length-e.length}function c(e,t){return 0===e.indexOf("/")&&(e=e.substring(1)),0===t.indexOf("/")&&(t=t.substring(1)),""===e||e===t}},1687:(e,t,r)=>{"use strict";r.d(t,{Ak:()=>c,Ze:()=>l,x3:()=>u});var n=r(7836),i=r(3606),o=r(860),a=r(2646);const s={};function c(e,t){const r={staged:!1,priority:o.P3[t]||0};d(e),s[e].get(t)||s[e].set(t,r)}function u(e,t){e&&s[e]&&(s[e].get(t)&&s[e].delete(t),h(e,t,!1),s[e].size&&f(e))}function d(e){if(!e)throw new Error("agentIdentifier required");s[e]||(s[e]=new Map)}function l(e="",t="feature",r=!1){if(d(e),!e||!s[e].get(t)||r)return h(e,t);s[e].get(t).staged=!0,f(e)}function f(e){const t=Array.from(s[e]);t.every((([e,t])=>t.staged))&&(t.sort(((e,t)=>e[1].priority-t[1].priority)),t.forEach((([t])=>{s[e].delete(t),h(e,t)})))}function h(e,t,r=!0){const o=e?n.ee.get(e):n.ee,s=i.i.handlers;if(!o.aborted&&o.backlog&&s){if(r){const e=o.backlog[t],r=s[t];if(r){for(let t=0;e&&t<e.length;++t)p(e[t],r);Object.entries(r).forEach((([e,t])=>{Object.values(t||{}).forEach((t=>{t[0]?.on&&t[0]?.context()instanceof a.y&&t[0].on(e,t[1])}))}))}}o.isolatedBacklog||delete s[t],o.backlog[t]=null,o.emit("drain-"+t,[])}}function p(e,t){var r=e[1];Object.values(t[r]||{}).forEach((t=>{var r=e[0];if(t[0]===r){var n=t[1],i=e[3],o=e[2];n.apply(i,o)}}))}},7836:(e,t,r)=>{"use strict";r.d(t,{P:()=>c,ee:()=>u});var n=r(384),i=r(8990),o=r(3371),a=r(2646),s=r(5607);const c="nr@context:".concat(s.W),u=function e(t,r){var n={},s={},d={},l=!1;try{l=16===r.length&&(0,o.f)(r).isolatedBacklog}catch(e){}var f={on:p,addEventListener:p,removeEventListener:function(e,t){var r=n[e];if(!r)return;for(var i=0;i<r.length;i++)r[i]===t&&r.splice(i,1)},emit:function(e,r,n,i,o){!1!==o&&(o=!0);if(u.aborted&&!i)return;t&&o&&t.emit(e,r,n);for(var a=h(n),c=g(e),d=c.length,l=0;l<d;l++)c[l].apply(a,r);var p=v()[s[e]];p&&p.push([f,e,r,a]);return a},get:m,listeners:g,context:h,buffer:function(e,t){const r=v();if(t=t||"feature",f.aborted)return;Object.entries(e||{}).forEach((([e,n])=>{s[n]=t,t in r||(r[t]=[])}))},abort:function(){f._aborted=!0,Object.keys(f.backlog).forEach((e=>{delete f.backlog[e]}))},isBuffering:function(e){return!!v()[s[e]]},debugId:r,backlog:l?{}:t&&"object"==typeof t.backlog?t.backlog:{},isolatedBacklog:l};return Object.defineProperty(f,"aborted",{get:()=>{let e=f._aborted||!1;return e||(t&&(e=t.aborted),e)}}),f;function h(e){return e&&e instanceof a.y?e:e?(0,i.I)(e,c,(()=>new a.y(c))):new a.y(c)}function p(e,t){n[e]=g(e).concat(t)}function g(e){return n[e]||[]}function m(t){return d[t]=d[t]||e(f,t)}function v(){return f.backlog}}(void 0,"globalEE"),d=(0,n.Zm)();d.ee||(d.ee=u)},2646:(e,t,r)=>{"use strict";r.d(t,{y:()=>n});class n{constructor(e){this.contextId=e}}},9908:(e,t,r)=>{"use strict";r.d(t,{d:()=>n,p:()=>i});var n=r(7836).ee.get("handle");function i(e,t,r,i,o){o?(o.buffer([e],i),o.emit(e,t,r)):(n.buffer([e],i),n.emit(e,t,r))}},3606:(e,t,r)=>{"use strict";r.d(t,{i:()=>o});var n=r(9908);o.on=a;var i=o.handlers={};function o(e,t,r,o){a(o||n.d,i,e,t,r)}function a(e,t,r,i,o){o||(o="feature"),e||(e=n.d);var a=t[o]=t[o]||{};(a[r]=a[r]||[]).push([e,i])}},3878:(e,t,r)=>{"use strict";function n(e,t){return{capture:e,passive:!1,signal:t}}function i(e,t,r=!1,i){window.addEventListener(e,t,n(r,i))}function o(e,t,r=!1,i){document.addEventListener(e,t,n(r,i))}r.d(t,{DD:()=>o,jT:()=>n,sp:()=>i})},5607:(e,t,r)=>{"use strict";r.d(t,{W:()=>n});const n=(0,r(9566).bz)()},9566:(e,t,r)=>{"use strict";r.d(t,{LA:()=>s,ZF:()=>c,bz:()=>a,el:()=>u});var n=r(6154);const i="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";function o(e,t){return e?15&e[t]:16*Math.random()|0}function a(){const e=n.gm?.crypto||n.gm?.msCrypto;let t,r=0;return e&&e.getRandomValues&&(t=e.getRandomValues(new Uint8Array(30))),i.split("").map((e=>"x"===e?o(t,r++).toString(16):"y"===e?(3&o()|8).toString(16):e)).join("")}function s(e){const t=n.gm?.crypto||n.gm?.msCrypto;let r,i=0;t&&t.getRandomValues&&(r=t.getRandomValues(new Uint8Array(e)));const a=[];for(var s=0;s<e;s++)a.push(o(r,i++).toString(16));return a.join("")}function c(){return s(16)}function u(){return s(32)}},2614:(e,t,r)=>{"use strict";r.d(t,{BB:()=>a,H3:()=>n,g:()=>u,iL:()=>c,tS:()=>s,uh:()=>i,wk:()=>o});const n="NRBA",i="SESSION",o=144e5,a=18e5,s={STARTED:"session-started",PAUSE:"session-pause",RESET:"session-reset",RESUME:"session-resume",UPDATE:"session-update"},c={SAME_TAB:"same-tab",CROSS_TAB:"cross-tab"},u={OFF:0,FULL:1,ERROR:2}},1863:(e,t,r)=>{"use strict";function n(){return Math.floor(performance.now())}r.d(t,{t:()=>n})},7485:(e,t,r)=>{"use strict";r.d(t,{D:()=>i});var n=r(6154);function i(e){if(0===(e||"").indexOf("data:"))return{protocol:"data"};try{const t=new URL(e,location.href),r={port:t.port,hostname:t.hostname,pathname:t.pathname,search:t.search,protocol:t.protocol.slice(0,t.protocol.indexOf(":")),sameOrigin:t.protocol===n.gm?.location?.protocol&&t.host===n.gm?.location?.host};return r.port&&""!==r.port||("http:"===t.protocol&&(r.port="80"),"https:"===t.protocol&&(r.port="443")),r.pathname&&""!==r.pathname?r.pathname.startsWith("/")||(r.pathname="/".concat(r.pathname)):r.pathname="/",r}catch(e){return{}}}},944:(e,t,r)=>{"use strict";function n(e,t){"function"==typeof console.debug&&console.debug("New Relic Warning: https://github.com/newrelic/newrelic-browser-agent/blob/main/docs/warning-codes.md#".concat(e),t)}r.d(t,{R:()=>n})},5284:(e,t,r)=>{"use strict";r.d(t,{t:()=>c,B:()=>s});var n=r(7836),i=r(6154);const o="newrelic";const a=new Set,s={};function c(e,t){const r=n.ee.get(t);s[t]??={},e&&"object"==typeof e&&(a.has(t)||(r.emit("rumresp",[e]),s[t]=e,a.add(t),function(e={}){try{i.gm.dispatchEvent(new CustomEvent(o,{detail:e}))}catch(e){}}({loaded:!0})))}},8990:(e,t,r)=>{"use strict";r.d(t,{I:()=>i});var n=Object.prototype.hasOwnProperty;function i(e,t,r){if(n.call(e,t))return e[t];var i=r();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,t,{value:i,writable:!0,enumerable:!1}),i}catch(e){}return e[t]=i,i}},6389:(e,t,r)=>{"use strict";function n(e,t=500,r={}){const n=r?.leading||!1;let i;return(...r)=>{n&&void 0===i&&(e.apply(this,r),i=setTimeout((()=>{i=clearTimeout(i)}),t)),n||(clearTimeout(i),i=setTimeout((()=>{e.apply(this,r)}),t))}}function i(e){let t=!1;return(...r)=>{t||(t=!0,e.apply(this,r))}}r.d(t,{J:()=>i,s:()=>n})},3304:(e,t,r)=>{"use strict";r.d(t,{A:()=>o});var n=r(7836);const i=()=>{const e=new WeakSet;return(t,r)=>{if("object"==typeof r&&null!==r){if(e.has(r))return;e.add(r)}return r}};function o(e){try{return JSON.stringify(e,i())??""}catch(e){try{n.ee.emit("internal-error",[e])}catch(e){}return""}}},5289:(e,t,r)=>{"use strict";r.d(t,{GG:()=>o,sB:()=>a});var n=r(3878);function i(){return"undefined"==typeof document||"complete"===document.readyState}function o(e,t){if(i())return e();(0,n.sp)("load",e,t)}function a(e){if(i())return e();(0,n.DD)("DOMContentLoaded",e)}},384:(e,t,r)=>{"use strict";r.d(t,{NT:()=>o,US:()=>d,Zm:()=>a,bQ:()=>c,dV:()=>s,nY:()=>u,pV:()=>l});var n=r(6154),i=r(1863);const o={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net"};function a(){return n.gm.NREUM||(n.gm.NREUM={}),void 0===n.gm.newrelic&&(n.gm.newrelic=n.gm.NREUM),n.gm.NREUM}function s(){let e=a();return e.o||(e.o={ST:n.gm.setTimeout,SI:n.gm.setImmediate,CT:n.gm.clearTimeout,XHR:n.gm.XMLHttpRequest,REQ:n.gm.Request,EV:n.gm.Event,PR:n.gm.Promise,MO:n.gm.MutationObserver,FETCH:n.gm.fetch,WS:n.gm.WebSocket}),e}function c(e,t){let r=a();r.initializedAgents??={},t.initializedAt={ms:(0,i.t)(),date:new Date},r.initializedAgents[e]=t}function u(e){let t=a();return t.initializedAgents?.[e]}function d(e,t){a()[e]=t}function l(){return function(){let e=a();const t=e.info||{};e.info={beacon:o.beacon,errorBeacon:o.errorBeacon,...t}}(),function(){let e=a();const t=e.init||{};e.init={...t}}(),s(),function(){let e=a();const t=e.loader_config||{};e.loader_config={...t}}(),a()}},2843:(e,t,r)=>{"use strict";r.d(t,{u:()=>i});var n=r(3878);function i(e,t=!1,r,i){(0,n.DD)("visibilitychange",(function(){if(t)return void("hidden"===document.visibilityState&&e());e(document.visibilityState)}),r,i)}},8139:(e,t,r)=>{"use strict";r.d(t,{u:()=>f});var n=r(7836),i=r(3434),o=r(8990),a=r(6154);const s={},c=a.gm.XMLHttpRequest,u="addEventListener",d="removeEventListener",l="nr@wrapped:".concat(n.P);function f(e){var t=function(e){return(e||n.ee).get("events")}(e);if(s[t.debugId]++)return t;s[t.debugId]=1;var r=(0,i.YM)(t,!0);function f(e){r.inPlace(e,[u,d],"-",p)}function p(e,t){return e[1]}return"getPrototypeOf"in Object&&(a.RI&&h(document,f),c&&h(c.prototype,f),h(a.gm,f)),t.on(u+"-start",(function(e,t){var n=e[1];if(null!==n&&("function"==typeof n||"object"==typeof n)){var i=(0,o.I)(n,l,(function(){var e={object:function(){if("function"!=typeof n.handleEvent)return;return n.handleEvent.apply(n,arguments)},function:n}[typeof n];return e?r(e,"fn-",null,e.name||"anonymous"):n}));this.wrapped=e[1]=i}})),t.on(d+"-start",(function(e){e[1]=this.wrapped||e[1]})),t}function h(e,t,...r){let n=e;for(;"object"==typeof n&&!Object.prototype.hasOwnProperty.call(n,u);)n=Object.getPrototypeOf(n);n&&t(n,...r)}},3434:(e,t,r)=>{"use strict";r.d(t,{Jt:()=>o,YM:()=>c});var n=r(7836),i=r(5607);const o="nr@original:".concat(i.W);var a=Object.prototype.hasOwnProperty,s=!1;function c(e,t){return e||(e=n.ee),r.inPlace=function(e,t,n,i,o){n||(n="");const a="-"===n.charAt(0);for(let s=0;s<t.length;s++){const c=t[s],u=e[c];d(u)||(e[c]=r(u,a?c+n:n,i,c,o))}},r.flag=o,r;function r(t,r,n,s,c){return d(t)?t:(r||(r=""),nrWrapper[o]=t,function(e,t,r){if(Object.defineProperty&&Object.keys)try{return Object.keys(e).forEach((function(r){Object.defineProperty(t,r,{get:function(){return e[r]},set:function(t){return e[r]=t,t}})})),t}catch(e){u([e],r)}for(var n in e)a.call(e,n)&&(t[n]=e[n])}(t,nrWrapper,e),nrWrapper);function nrWrapper(){var o,a,d,l;try{a=this,o=[...arguments],d="function"==typeof n?n(o,a):n||{}}catch(t){u([t,"",[o,a,s],d],e)}i(r+"start",[o,a,s],d,c);try{return l=t.apply(a,o)}catch(e){throw i(r+"err",[o,a,e],d,c),e}finally{i(r+"end",[o,a,l],d,c)}}}function i(r,n,i,o){if(!s||t){var a=s;s=!0;try{e.emit(r,n,i,t,o)}catch(t){u([t,r,n,i],e)}s=a}}}function u(e,t){t||(t=n.ee);try{t.emit("internal-error",e)}catch(e){}}function d(e){return!(e&&"function"==typeof e&&e.apply&&!e[o])}},9414:(e,t,r)=>{"use strict";r.d(t,{J:()=>c});var n=r(7836),i=r(2646),o=r(944),a=r(3434);const s=new Map;function c(e,t,r,c){if("object"!=typeof t||!t||"string"!=typeof r||!r||"function"!=typeof t[r])return(0,o.R)(29);const u=function(e){return(e||n.ee).get("logger")}(e),d=(0,a.YM)(u),l=new i.y(n.P);l.level=c.level,l.customAttributes=c.customAttributes;const f=t[r]?.[a.Jt]||t[r];return s.set(f,l),d.inPlace(t,[r],"wrap-logger-",(()=>s.get(f))),u}},9300:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K7.ajax},3333:(e,t,r)=>{"use strict";r.d(t,{$v:()=>u,TZ:()=>n,Zp:()=>i,kd:()=>c,mq:()=>s,nf:()=>a,qN:()=>o});const n=r(860).K7.genericEvents,i=["auxclick","click","copy","keydown","paste","scrollend"],o=["focus","blur"],a=4,s=1e3,c=["PageAction","UserAction","BrowserPerformance"],u={MARKS:"experimental.marks",MEASURES:"experimental.measures",RESOURCES:"experimental.resources"}},6774:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K7.jserrors},993:(e,t,r)=>{"use strict";r.d(t,{A$:()=>o,ET:()=>a,TZ:()=>s,p_:()=>i});var n=r(860);const i={ERROR:"ERROR",WARN:"WARN",INFO:"INFO",DEBUG:"DEBUG",TRACE:"TRACE"},o={OFF:0,ERROR:1,WARN:2,INFO:3,DEBUG:4,TRACE:5},a="log",s=n.K7.logging},3785:(e,t,r)=>{"use strict";r.d(t,{R:()=>c,b:()=>u});var n=r(9908),i=r(1863),o=r(860),a=r(8154),s=r(993);function c(e,t,r={},c=s.p_.INFO){(0,n.p)(a.xV,["API/logging/".concat(c.toLowerCase(),"/called")],void 0,o.K7.metrics,e),(0,n.p)(s.ET,[(0,i.t)(),t,r,c],void 0,o.K7.logging,e)}function u(e){return"string"==typeof e&&Object.values(s.p_).some((t=>t===e.toUpperCase().trim()))}},8154:(e,t,r)=>{"use strict";r.d(t,{z_:()=>o,XG:()=>s,TZ:()=>n,rs:()=>i,xV:()=>a});r(6154),r(9566),r(384);const n=r(860).K7.metrics,i="sm",o="cm",a="storeSupportabilityMetrics",s="storeEventMetrics"},6630:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K7.pageViewEvent},782:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K7.pageViewTiming},6344:(e,t,r)=>{"use strict";r.d(t,{BB:()=>d,G4:()=>o,Qb:()=>l,TZ:()=>i,Ug:()=>a,_s:()=>s,bc:()=>u,yP:()=>c});var n=r(2614);const i=r(860).K7.sessionReplay,o={RECORD:"recordReplay",PAUSE:"pauseReplay",REPLAY_RUNNING:"replayRunning",ERROR_DURING_REPLAY:"errorDuringReplay"},a=.12,s={DomContentLoaded:0,Load:1,FullSnapshot:2,IncrementalSnapshot:3,Meta:4,Custom:5},c={[n.g.ERROR]:15e3,[n.g.FULL]:3e5,[n.g.OFF]:0},u={RESET:{message:"Session was reset",sm:"Reset"},IMPORT:{message:"Recorder failed to import",sm:"Import"},TOO_MANY:{message:"429: Too Many Requests",sm:"Too-Many"},TOO_BIG:{message:"Payload was too large",sm:"Too-Big"},CROSS_TAB:{message:"Session Entity was set to OFF on another tab",sm:"Cross-Tab"},ENTITLEMENTS:{message:"Session Replay is not allowed and will not be started",sm:"Entitlement"}},d=5e3,l={API:"api"}},5270:(e,t,r)=>{"use strict";r.d(t,{Aw:()=>c,CT:()=>u,SR:()=>s});var n=r(384),i=r(9417),o=r(7767),a=r(6154);function s(e){return!!(0,n.dV)().o.MO&&(0,o.V)(e)&&!0===(0,i.gD)(e,"session_trace.enabled")}function c(e){return!0===(0,i.gD)(e,"session_replay.preload")&&s(e)}function u(e,t){const r=t.correctAbsoluteTimestamp(e);return{originalTimestamp:e,correctedTimestamp:r,timestampDiff:e-r,originTime:a.WN,correctedOriginTime:t.correctedOriginTime,originTimeDiff:Math.floor(a.WN-t.correctedOriginTime)}}},3738:(e,t,r)=>{"use strict";r.d(t,{He:()=>i,Kp:()=>s,Lc:()=>u,Rz:()=>d,TZ:()=>n,bD:()=>o,d3:()=>a,jx:()=>l,uP:()=>c});const n=r(860).K7.sessionTrace,i="bstResource",o="resource",a="-start",s="-end",c="fn"+a,u="fn"+s,d="pushState",l=1e3},3962:(e,t,r)=>{"use strict";r.d(t,{AM:()=>o,O2:()=>c,Qu:()=>u,TZ:()=>s,ih:()=>d,pP:()=>a,tC:()=>i});var n=r(860);const i=["click","keydown","submit","popstate"],o="api",a="initialPageLoad",s=n.K7.softNav,c={INITIAL_PAGE_LOAD:"",ROUTE_CHANGE:1,UNSPECIFIED:2},u={INTERACTION:1,AJAX:2,CUSTOM_END:3,CUSTOM_TRACER:4},d={IP:"in progress",FIN:"finished",CAN:"cancelled"}},7378:(e,t,r)=>{"use strict";r.d(t,{$p:()=>x,BR:()=>b,Kp:()=>R,L3:()=>y,Lc:()=>c,NC:()=>o,SG:()=>d,TZ:()=>i,U6:()=>p,UT:()=>m,d3:()=>w,dT:()=>f,e5:()=>A,gx:()=>v,l9:()=>l,oW:()=>h,op:()=>g,rw:()=>u,tH:()=>T,uP:()=>s,wW:()=>E,xq:()=>a});var n=r(384);const i=r(860).K7.spa,o=["click","submit","keypress","keydown","keyup","change"],a=999,s="fn-start",c="fn-end",u="cb-start",d="api-ixn-",l="remaining",f="interaction",h="spaNode",p="jsonpNode",g="fetch-start",m="fetch-done",v="fetch-body-",b="jsonp-end",y=(0,n.dV)().o.ST,w="-start",R="-end",x="-body",E="cb"+R,A="jsTime",T="fetch"},4234:(e,t,r)=>{"use strict";r.d(t,{W:()=>o});var n=r(7836),i=r(1687);class o{constructor(e,t){this.agentIdentifier=e,this.ee=n.ee.get(e),this.featureName=t,this.blocked=!1}deregisterDrain(){(0,i.x3)(this.agentIdentifier,this.featureName)}}},7767:(e,t,r)=>{"use strict";r.d(t,{V:()=>o});var n=r(9417),i=r(6154);const o=e=>i.RI&&!0===(0,n.gD)(e,"privacy.cookies_enabled")},8969:(e,t,r)=>{"use strict";r.d(t,{j:()=>O});var n=r(860),i=r(2555),o=r(3371),a=r(9908),s=r(7836),c=r(1687),u=r(5289),d=r(6154),l=r(944),f=r(8154),h=r(384),p=r(6344);const g=["setErrorHandler","finished","addToTrace","addRelease","recordCustomEvent","addPageAction","setCurrentRouteName","setPageViewName","setCustomAttribute","interaction","noticeError","setUserId","setApplicationVersion","start",p.G4.RECORD,p.G4.PAUSE,"log","wrapLogger"],m=["setErrorHandler","finished","addToTrace","addRelease"];var v=r(1863),b=r(2614),y=r(993),w=r(3785),R=r(9414);function x(){const e=(0,h.pV)();g.forEach((t=>{e[t]=(...r)=>function(t,...r){let n=[];return Object.values(e.initializedAgents).forEach((e=>{e&&e.api?e.exposed&&e.api[t]&&n.push(e.api[t](...r)):(0,l.R)(38,t)})),n.length>1?n:n[0]}(t,...r)}))}const E={};var A=r(9417),T=r(5603),N=r(5284);const S=e=>{const t=e.startsWith("http");e+="/",r.p=t?e:"https://"+e};let _=!1;function O(e,t={},g,O){let{init:I,info:P,loader_config:j,runtime:C={},exposed:k=!0}=t;C.loaderType=g;const L=(0,h.pV)();P||(I=L.init,P=L.info,j=L.loader_config),(0,A.xN)(e.agentIdentifier,I||{}),(0,T.a)(e.agentIdentifier,j||{}),P.jsAttributes??={},d.bv&&(P.jsAttributes.isWorker=!0),(0,i.x1)(e.agentIdentifier,P);const H=(0,A.D0)(e.agentIdentifier),M=[P.beacon,P.errorBeacon];_||(H.proxy.assets&&(S(H.proxy.assets),M.push(H.proxy.assets)),H.proxy.beacon&&M.push(H.proxy.beacon),x(),(0,h.US)("activatedFeatures",N.B),e.runSoftNavOverSpa&&=!0===H.soft_navigations.enabled&&H.feature_flags.includes("soft_nav")),C.denyList=[...H.ajax.deny_list||[],...H.ajax.block_internal?M:[]],C.ptid=e.agentIdentifier,(0,o.V)(e.agentIdentifier,C),e.ee=s.ee.get(e.agentIdentifier),void 0===e.api&&(e.api=function(e,t,h=!1){t||(0,c.Ak)(e,"api");const g={};var x=s.ee.get(e),A=x.get("tracer");E[e]=b.g.OFF,x.on(p.G4.REPLAY_RUNNING,(t=>{E[e]=t}));var T="api-",N=T+"ixn-";function S(t,r,n,o){const a=(0,i.Vp)(e);return null===r?delete a.jsAttributes[t]:(0,i.x1)(e,{...a,jsAttributes:{...a.jsAttributes,[t]:r}}),I(T,n,!0,o||null===r?"session":void 0)(t,r)}function _(){}g.log=function(e,{customAttributes:t={},level:r=y.p_.INFO}={}){(0,a.p)(f.xV,["API/log/called"],void 0,n.K7.metrics,x),(0,w.R)(x,e,t,r)},g.wrapLogger=(e,t,{customAttributes:r={},level:i=y.p_.INFO}={})=>{(0,a.p)(f.xV,["API/wrapLogger/called"],void 0,n.K7.metrics,x),(0,R.J)(x,e,t,{customAttributes:r,level:i})},m.forEach((e=>{g[e]=I(T,e,!0,"api")})),g.addPageAction=I(T,"addPageAction",!0,n.K7.genericEvents),g.recordCustomEvent=I(T,"recordCustomEvent",!0,n.K7.genericEvents),g.setPageViewName=function(t,r){if("string"==typeof t)return"/"!==t.charAt(0)&&(t="/"+t),(0,o.f)(e).customTransaction=(r||"http://custom.transaction")+t,I(T,"setPageViewName",!0)()},g.setCustomAttribute=function(e,t,r=!1){if("string"==typeof e){if(["string","number","boolean"].includes(typeof t)||null===t)return S(e,t,"setCustomAttribute",r);(0,l.R)(40,typeof t)}else(0,l.R)(39,typeof e)},g.setUserId=function(e){if("string"==typeof e||null===e)return S("enduser.id",e,"setUserId",!0);(0,l.R)(41,typeof e)},g.setApplicationVersion=function(e){if("string"==typeof e||null===e)return S("application.version",e,"setApplicationVersion",!1);(0,l.R)(42,typeof e)},g.start=()=>{try{(0,a.p)(f.xV,["API/start/called"],void 0,n.K7.metrics,x),x.emit("manual-start-all")}catch(e){(0,l.R)(23,e)}},g[p.G4.RECORD]=function(){(0,a.p)(f.xV,["API/recordReplay/called"],void 0,n.K7.metrics,x),(0,a.p)(p.G4.RECORD,[],void 0,n.K7.sessionReplay,x)},g[p.G4.PAUSE]=function(){(0,a.p)(f.xV,["API/pauseReplay/called"],void 0,n.K7.metrics,x),(0,a.p)(p.G4.PAUSE,[],void 0,n.K7.sessionReplay,x)},g.interaction=function(e){return(new _).get("object"==typeof e?e:{})};const O=_.prototype={createTracer:function(e,t){var r={},i=this,o="function"==typeof t;return(0,a.p)(f.xV,["API/createTracer/called"],void 0,n.K7.metrics,x),h||(0,a.p)(N+"tracer",[(0,v.t)(),e,r],i,n.K7.spa,x),function(){if(A.emit((o?"":"no-")+"fn-start",[(0,v.t)(),i,o],r),o)try{return t.apply(this,arguments)}catch(e){const t="string"==typeof e?new Error(e):e;throw A.emit("fn-err",[arguments,this,t],r),t}finally{A.emit("fn-end",[(0,v.t)()],r)}}}};function I(e,t,r,i){return function(){return(0,a.p)(f.xV,["API/"+t+"/called"],void 0,n.K7.metrics,x),i&&(0,a.p)(e+t,[r?(0,v.t)():performance.now(),...arguments],r?null:this,i,x),r?void 0:this}}function P(){r.e(478).then(r.bind(r,8778)).then((({setAPI:t})=>{t(e),(0,c.Ze)(e,"api")})).catch((e=>{(0,l.R)(27,e),x.abort()}))}return["actionText","setName","setAttribute","save","ignore","onEnd","getContext","end","get"].forEach((e=>{O[e]=I(N,e,void 0,h?n.K7.softNav:n.K7.spa)})),g.setCurrentRouteName=h?I(N,"routeName",void 0,n.K7.softNav):I(T,"routeName",!0,n.K7.spa),g.noticeError=function(t,r){"string"==typeof t&&(t=new Error(t)),(0,a.p)(f.xV,["API/noticeError/called"],void 0,n.K7.metrics,x),(0,a.p)("err",[t,(0,v.t)(),!1,r,!!E[e]],void 0,n.K7.jserrors,x)},d.RI?(0,u.GG)((()=>P()),!0):P(),g}(e.agentIdentifier,O,e.runSoftNavOverSpa)),void 0===e.exposed&&(e.exposed=k),_=!0}},8374:(e,t,r)=>{r.nc=(()=>{try{return document?.currentScript?.nonce}catch(e){}return""})()},860:(e,t,r)=>{"use strict";r.d(t,{$J:()=>u,K7:()=>s,P3:()=>c,XX:()=>i,qY:()=>n,v4:()=>a});const n="events",i="jserrors",o="browser/blobs",a="rum",s={ajax:"ajax",genericEvents:"generic_events",jserrors:i,logging:"logging",metrics:"metrics",pageAction:"page_action",pageViewEvent:"page_view_event",pageViewTiming:"page_view_timing",sessionReplay:"session_replay",sessionTrace:"session_trace",softNav:"soft_navigations",spa:"spa"},c={[s.pageViewEvent]:1,[s.pageViewTiming]:2,[s.metrics]:3,[s.jserrors]:4,[s.spa]:5,[s.ajax]:6,[s.sessionTrace]:7,[s.softNav]:8,[s.sessionReplay]:9,[s.logging]:10,[s.genericEvents]:11},u={[s.pageViewEvent]:a,[s.pageViewTiming]:n,[s.ajax]:n,[s.spa]:n,[s.softNav]:n,[s.metrics]:i,[s.jserrors]:i,[s.sessionTrace]:o,[s.sessionReplay]:o,[s.logging]:"browser/logs",[s.genericEvents]:"ins"}}},n={};function i(e){var t=n[e];if(void 0!==t)return t.exports;var o=n[e]={exports:{}};return r[e](o,o.exports,i),o.exports}i.m=r,i.d=(e,t)=>{for(var r in t)i.o(t,r)&&!i.o(e,r)&&Object.defineProperty(e,r,{enumerable:!0,get:t[r]})},i.f={},i.e=e=>Promise.all(Object.keys(i.f).reduce(((t,r)=>(i.f[r](e,t),t)),[])),i.u=e=>({212:"nr-spa-compressor",249:"nr-spa-recorder",478:"nr-spa"}[e]+"-1.284.1.min.js"),i.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),e={},t="NRBA-1.284.1.PROD:",i.l=(r,n,o,a)=>{if(e[r])e[r].push(n);else{var s,c;if(void 0!==o)for(var u=document.getElementsByTagName("script"),d=0;d<u.length;d++){var l=u[d];if(l.getAttribute("src")==r||l.getAttribute("data-webpack")==t+o){s=l;break}}if(!s){c=!0;var f={478:"sha512-VWXyHiJymB8Fhqc5jWlZ1CWrtlNWMPhm+PNnjWX1CfW9gQedReF5MFo3k8skdB5TlR837ofwtBeSzIE69RbCwQ==",249:"sha512-e1fXm1ZkMilKv1yUjmoJw7TU5bCKK/7Xaa/Ta9opkyurmkWtIKPd4CMfU2dRHGr9+brXynQnY3wwY838aQPhVw==",212:"sha512-zX52gEnN9DOzLrxU+zqxDl8R/t9rkdZavljSVjdAYS3BNoZH304PhVRnFEdwyYNCN9QcuGDBMMJ8U6XYtxAL0g=="};(s=document.createElement("script")).charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.setAttribute("data-webpack",t+o),s.src=r,0!==s.src.indexOf(window.location.origin+"/")&&(s.crossOrigin="anonymous"),f[a]&&(s.integrity=f[a])}e[r]=[n];var h=(t,n)=>{s.onerror=s.onload=null,clearTimeout(p);var i=e[r];if(delete e[r],s.parentNode&&s.parentNode.removeChild(s),i&&i.forEach((e=>e(n))),t)return t(n)},p=setTimeout(h.bind(null,void 0,{type:"timeout",target:s}),12e4);s.onerror=h.bind(null,s.onerror),s.onload=h.bind(null,s.onload),c&&document.head.appendChild(s)}},i.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.p="https://js-agent.newrelic.com/",(()=>{var e={38:0,788:0};i.f.j=(t,r)=>{var n=i.o(e,t)?e[t]:void 0;if(0!==n)if(n)r.push(n[2]);else{var o=new Promise(((r,i)=>n=e[t]=[r,i]));r.push(n[2]=o);var a=i.p+i.u(t),s=new Error;i.l(a,(r=>{if(i.o(e,t)&&(0!==(n=e[t])&&(e[t]=void 0),n)){var o=r&&("load"===r.type?"missing":r.type),a=r&&r.target&&r.target.src;s.message="Loading chunk "+t+" failed.\n("+o+": "+a+")",s.name="ChunkLoadError",s.type=o,s.request=a,n[1](s)}}),"chunk-"+t,t)}};var t=(t,r)=>{var n,o,[a,s,c]=r,u=0;if(a.some((t=>0!==e[t]))){for(n in s)i.o(s,n)&&(i.m[n]=s[n]);if(c)c(i)}for(t&&t(r);u<a.length;u++)o=a[u],i.o(e,o)&&e[o]&&e[o][0](),e[o]=0},r=self["webpackChunk:NRBA-1.284.1.PROD"]=self["webpackChunk:NRBA-1.284.1.PROD"]||[];r.forEach(t.bind(null,0)),r.push=t.bind(null,r.push.bind(r))})(),(()=>{"use strict";i(8374);var e=i(944),t=i(6344),r=i(9566);class n{agentIdentifier;constructor(){this.agentIdentifier=(0,r.LA)(16)}#e(t,...r){if("function"==typeof this.api?.[t])return this.api[t](...r);(0,e.R)(35,t)}addPageAction(e,t){return this.#e("addPageAction",e,t)}recordCustomEvent(e,t){return this.#e("recordCustomEvent",e,t)}setPageViewName(e,t){return this.#e("setPageViewName",e,t)}setCustomAttribute(e,t,r){return this.#e("setCustomAttribute",e,t,r)}noticeError(e,t){return this.#e("noticeError",e,t)}setUserId(e){return this.#e("setUserId",e)}setApplicationVersion(e){return this.#e("setApplicationVersion",e)}setErrorHandler(e){return this.#e("setErrorHandler",e)}addRelease(e,t){return this.#e("addRelease",e,t)}log(e,t){return this.#e("log",e,t)}}class o extends n{#e(t,...r){if("function"==typeof this.api?.[t])return this.api[t](...r);(0,e.R)(35,t)}start(){return this.#e("start")}finished(e){return this.#e("finished",e)}recordReplay(){return this.#e(t.G4.RECORD)}pauseReplay(){return this.#e(t.G4.PAUSE)}addToTrace(e){return this.#e("addToTrace",e)}setCurrentRouteName(e){return this.#e("setCurrentRouteName",e)}interaction(){return this.#e("interaction")}wrapLogger(e,t,r){return this.#e("wrapLogger",e,t,r)}}var a=i(860),s=i(9417);const c=Object.values(a.K7);function u(e){const t={};return c.forEach((r=>{t[r]=function(e,t){return!0===(0,s.gD)(t,"".concat(e,".enabled"))}(r,e)})),t}var d=i(8969);var l=i(1687),f=i(4234),h=i(5289),p=i(6154),g=i(5270),m=i(7767),v=i(6389);class b extends f.W{constructor(e,t,r=!0){super(e.agentIdentifier,t),this.auto=r,this.abortHandler=void 0,this.featAggregate=void 0,this.onAggregateImported=void 0,!1===e.init[this.featureName].autoStart&&(this.auto=!1),this.auto?(0,l.Ak)(e.agentIdentifier,t):this.ee.on("manual-start-all",(0,v.J)((()=>{(0,l.Ak)(e.agentIdentifier,this.featureName),this.auto=!0,this.importAggregator(e)})))}importAggregator(t,r={}){if(this.featAggregate||!this.auto)return;let n;this.onAggregateImported=new Promise((e=>{n=e}));const o=async()=>{let o;try{if((0,m.V)(this.agentIdentifier)){const{setupAgentSession:e}=await i.e(478).then(i.bind(i,6526));o=e(t)}}catch(t){(0,e.R)(20,t),this.ee.emit("internal-error",[t]),this.featureName===a.K7.sessionReplay&&this.abortHandler?.()}try{if(!this.#t(this.featureName,o))return(0,l.Ze)(this.agentIdentifier,this.featureName),void n(!1);const{lazyFeatureLoader:e}=await i.e(478).then(i.bind(i,6103)),{Aggregate:a}=await e(this.featureName,"aggregate");this.featAggregate=new a(t,r),t.runtime.harvester.initializedAggregates.push(this.featAggregate),n(!0)}catch(t){(0,e.R)(34,t),this.abortHandler?.(),(0,l.Ze)(this.agentIdentifier,this.featureName,!0),n(!1),this.ee&&this.ee.abort()}};p.RI?(0,h.GG)((()=>o()),!0):o()}#t(e,t){switch(e){case a.K7.sessionReplay:return(0,g.SR)(this.agentIdentifier)&&!!t;case a.K7.sessionTrace:return!!t;default:return!0}}}var y=i(6630);class w extends b{static featureName=y.T;constructor(e,t=!0){super(e,y.T,t),this.importAggregator(e)}}var R=i(384);var x=i(9908),E=i(2843),A=i(3878),T=i(782),N=i(1863);class S extends b{static featureName=T.T;constructor(e,t=!0){super(e,T.T,t),p.RI&&((0,E.u)((()=>(0,x.p)("docHidden",[(0,N.t)()],void 0,T.T,this.ee)),!0),(0,A.sp)("pagehide",(()=>(0,x.p)("winPagehide",[(0,N.t)()],void 0,T.T,this.ee))),this.importAggregator(e))}}var _=i(8154);class O extends b{static featureName=_.TZ;constructor(e,t=!0){super(e,_.TZ,t),p.RI&&document.addEventListener("securitypolicyviolation",(e=>{(0,x.p)(_.xV,["Generic/CSPViolation/Detected"],void 0,this.featureName,this.ee)})),this.importAggregator(e)}}var I=i(6774),P=i(3304);class j{constructor(e,t,r,n,i){this.name="UncaughtError",this.message="string"==typeof e?e:(0,P.A)(e),this.sourceURL=t,this.line=r,this.column=n,this.__newrelic=i}}function C(e){return H(e)?e:new j(void 0!==e?.message?e.message:e,e?.filename||e?.sourceURL,e?.lineno||e?.line,e?.colno||e?.col,e?.__newrelic)}function k(e){const t="Unhandled Promise Rejection: ";if(!e?.reason)return;if(H(e.reason)){try{e.reason.message.startsWith(t)||(e.reason.message=t+e.reason.message)}catch(e){}return C(e.reason)}const r=C(e.reason);return(r.message||"").startsWith(t)||(r.message=t+r.message),r}function L(e){if(e.error instanceof SyntaxError&&!/:\d+$/.test(e.error.stack?.trim())){const t=new j(e.message,e.filename,e.lineno,e.colno,e.error.__newrelic);return t.name=SyntaxError.name,t}return H(e.error)?e.error:C(e)}function H(e){return e instanceof Error&&!!e.stack}class M extends b{static featureName=I.T;#r=!1;constructor(e,r=!0){super(e,I.T,r);try{this.removeOnAbort=new AbortController}catch(e){}this.ee.on("internal-error",((e,t)=>{this.abortHandler&&(0,x.p)("ierr",[C(e),(0,N.t)(),!0,{},this.#r,t],void 0,this.featureName,this.ee)})),this.ee.on(t.G4.REPLAY_RUNNING,(e=>{this.#r=e})),p.gm.addEventListener("unhandledrejection",(e=>{this.abortHandler&&(0,x.p)("err",[k(e),(0,N.t)(),!1,{unhandledPromiseRejection:1},this.#r],void 0,this.featureName,this.ee)}),(0,A.jT)(!1,this.removeOnAbort?.signal)),p.gm.addEventListener("error",(e=>{this.abortHandler&&(0,x.p)("err",[L(e),(0,N.t)(),!1,{},this.#r],void 0,this.featureName,this.ee)}),(0,A.jT)(!1,this.removeOnAbort?.signal)),this.abortHandler=this.#n,this.importAggregator(e)}#n(){this.removeOnAbort?.abort(),this.abortHandler=void 0}}var D=i(8990);let K=1;const U="nr@id";function V(e){const t=typeof e;return!e||"object"!==t&&"function"!==t?-1:e===p.gm?0:(0,D.I)(e,U,(function(){return K++}))}function G(e){if("string"==typeof e&&e.length)return e.length;if("object"==typeof e){if("undefined"!=typeof ArrayBuffer&&e instanceof ArrayBuffer&&e.byteLength)return e.byteLength;if("undefined"!=typeof Blob&&e instanceof Blob&&e.size)return e.size;if(!("undefined"!=typeof FormData&&e instanceof FormData))try{return(0,P.A)(e).length}catch(e){return}}}var F=i(8139),B=i(7836),W=i(3434);const z={},q=["open","send"];function Z(t){var r=t||B.ee;const n=function(e){return(e||B.ee).get("xhr")}(r);if(void 0===p.gm.XMLHttpRequest)return n;if(z[n.debugId]++)return n;z[n.debugId]=1,(0,F.u)(r);var i=(0,W.YM)(n),o=p.gm.XMLHttpRequest,a=p.gm.MutationObserver,s=p.gm.Promise,c=p.gm.setInterval,u="readystatechange",d=["onload","onerror","onabort","onloadstart","onloadend","onprogress","ontimeout"],l=[],f=p.gm.XMLHttpRequest=function(t){const r=new o(t),a=n.context(r);try{n.emit("new-xhr",[r],a),r.addEventListener(u,(s=a,function(){var e=this;e.readyState>3&&!s.resolved&&(s.resolved=!0,n.emit("xhr-resolved",[],e)),i.inPlace(e,d,"fn-",y)}),(0,A.jT)(!1))}catch(t){(0,e.R)(15,t);try{n.emit("internal-error",[t])}catch(e){}}var s;return r};function h(e,t){i.inPlace(t,["onreadystatechange"],"fn-",y)}if(function(e,t){for(var r in e)t[r]=e[r]}(o,f),f.prototype=o.prototype,i.inPlace(f.prototype,q,"-xhr-",y),n.on("send-xhr-start",(function(e,t){h(e,t),function(e){l.push(e),a&&(g?g.then(b):c?c(b):(m=-m,v.data=m))}(t)})),n.on("open-xhr-start",h),a){var g=s&&s.resolve();if(!c&&!s){var m=1,v=document.createTextNode(m);new a(b).observe(v,{characterData:!0})}}else r.on("fn-end",(function(e){e[0]&&e[0].type===u||b()}));function b(){for(var e=0;e<l.length;e++)h(0,l[e]);l.length&&(l=[])}function y(e,t){return t}return n}var Y="fetch-",J=Y+"body-",X=["arrayBuffer","blob","json","text","formData"],Q=p.gm.Request,ee=p.gm.Response,te="prototype";const re={};function ne(e){const t=function(e){return(e||B.ee).get("fetch")}(e);if(!(Q&&ee&&p.gm.fetch))return t;if(re[t.debugId]++)return t;function r(e,r,n){var i=e[r];"function"==typeof i&&(e[r]=function(){var e,r=[...arguments],o={};t.emit(n+"before-start",[r],o),o[B.P]&&o[B.P].dt&&(e=o[B.P].dt);var a=i.apply(this,r);return t.emit(n+"start",[r,e],a),a.then((function(e){return t.emit(n+"end",[null,e],a),e}),(function(e){throw t.emit(n+"end",[e],a),e}))})}return re[t.debugId]=1,X.forEach((e=>{r(Q[te],e,J),r(ee[te],e,J)})),r(p.gm,"fetch",Y),t.on(Y+"end",(function(e,r){var n=this;if(r){var i=r.headers.get("content-length");null!==i&&(n.rxSize=i),t.emit(Y+"done",[null,r],n)}else t.emit(Y+"done",[e],n)})),t}var ie=i(7485),oe=i(5603);class ae{constructor(e){this.agentIdentifier=e}generateTracePayload(e){if(!this.shouldGenerateTrace(e))return null;var t=(0,oe.o)(this.agentIdentifier);if(!t)return null;var n=(t.accountID||"").toString()||null,i=(t.agentID||"").toString()||null,o=(t.trustKey||"").toString()||null;if(!n||!i)return null;var a=(0,r.ZF)(),s=(0,r.el)(),c=Date.now(),u={spanId:a,traceId:s,timestamp:c};return(e.sameOrigin||this.isAllowedOrigin(e)&&this.useTraceContextHeadersForCors())&&(u.traceContextParentHeader=this.generateTraceContextParentHeader(a,s),u.traceContextStateHeader=this.generateTraceContextStateHeader(a,c,n,i,o)),(e.sameOrigin&&!this.excludeNewrelicHeader()||!e.sameOrigin&&this.isAllowedOrigin(e)&&this.useNewrelicHeaderForCors())&&(u.newrelicHeader=this.generateTraceHeader(a,s,c,n,i,o)),u}generateTraceContextParentHeader(e,t){return"00-"+t+"-"+e+"-01"}generateTraceContextStateHeader(e,t,r,n,i){return i+"@nr=0-1-"+r+"-"+n+"-"+e+"----"+t}generateTraceHeader(e,t,r,n,i,o){if(!("function"==typeof p.gm?.btoa))return null;var a={v:[0,1],d:{ty:"Browser",ac:n,ap:i,id:e,tr:t,ti:r}};return o&&n!==o&&(a.d.tk=o),btoa((0,P.A)(a))}shouldGenerateTrace(e){return this.isDtEnabled()&&this.isAllowedOrigin(e)}isAllowedOrigin(e){var t=!1,r={};if((0,s.gD)(this.agentIdentifier,"distributed_tracing")&&(r=(0,s.D0)(this.agentIdentifier).distributed_tracing),e.sameOrigin)t=!0;else if(r.allowed_origins instanceof Array)for(var n=0;n<r.allowed_origins.length;n++){var i=(0,ie.D)(r.allowed_origins[n]);if(e.hostname===i.hostname&&e.protocol===i.protocol&&e.port===i.port){t=!0;break}}return t}isDtEnabled(){var e=(0,s.gD)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.enabled}excludeNewrelicHeader(){var e=(0,s.gD)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.exclude_newrelic_header}useNewrelicHeaderForCors(){var e=(0,s.gD)(this.agentIdentifier,"distributed_tracing");return!!e&&!1!==e.cors_use_newrelic_header}useTraceContextHeadersForCors(){var e=(0,s.gD)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.cors_use_tracecontext_headers}}var se=i(9300),ce=i(7295),ue=["load","error","abort","timeout"],de=ue.length,le=(0,R.dV)().o.REQ,fe=(0,R.dV)().o.XHR;const he="X-NewRelic-App-Data";class pe extends b{static featureName=se.T;constructor(e,t=!0){super(e,se.T,t),this.dt=new ae(e.agentIdentifier),this.handler=(e,t,r,n)=>(0,x.p)(e,t,r,n,this.ee);try{const e={xmlhttprequest:"xhr",fetch:"fetch",beacon:"beacon"};p.gm?.performance?.getEntriesByType("resource").forEach((t=>{if(t.initiatorType in e&&0!==t.responseStatus){const r={status:t.responseStatus},n={rxSize:t.transferSize,duration:Math.floor(t.duration),cbTime:0};ge(r,t.name),this.handler("xhr",[r,n,t.startTime,t.responseEnd,e[t.initiatorType]],void 0,a.K7.ajax)}}))}catch(e){}ne(this.ee),Z(this.ee),function(e,t,r,n){function i(e){var t=this;t.totalCbs=0,t.called=0,t.cbTime=0,t.end=R,t.ended=!1,t.xhrGuids={},t.lastSize=null,t.loadCaptureCalled=!1,t.params=this.params||{},t.metrics=this.metrics||{},e.addEventListener("load",(function(r){E(t,e)}),(0,A.jT)(!1)),p.lR||e.addEventListener("progress",(function(e){t.lastSize=e.loaded}),(0,A.jT)(!1))}function o(e){this.params={method:e[0]},ge(this,e[1]),this.metrics={}}function s(t,r){e.loader_config.xpid&&this.sameOrigin&&r.setRequestHeader("X-NewRelic-ID",e.loader_config.xpid);var i=n.generateTracePayload(this.parsedOrigin);if(i){var o=!1;i.newrelicHeader&&(r.setRequestHeader("newrelic",i.newrelicHeader),o=!0),i.traceContextParentHeader&&(r.setRequestHeader("traceparent",i.traceContextParentHeader),i.traceContextStateHeader&&r.setRequestHeader("tracestate",i.traceContextStateHeader),o=!0),o&&(this.dt=i)}}function c(e,r){var n=this.metrics,i=e[0],o=this;if(n&&i){var a=G(i);a&&(n.txSize=a)}this.startTime=(0,N.t)(),this.body=i,this.listener=function(e){try{"abort"!==e.type||o.loadCaptureCalled||(o.params.aborted=!0),("load"!==e.type||o.called===o.totalCbs&&(o.onloadCalled||"function"!=typeof r.onload)&&"function"==typeof o.end)&&o.end(r)}catch(e){try{t.emit("internal-error",[e])}catch(e){}}};for(var s=0;s<de;s++)r.addEventListener(ue[s],this.listener,(0,A.jT)(!1))}function u(e,t,r){this.cbTime+=e,t?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&"function"==typeof r.onload||"function"!=typeof this.end||this.end(r)}function d(e,t){var r=""+V(e)+!!t;this.xhrGuids&&!this.xhrGuids[r]&&(this.xhrGuids[r]=!0,this.totalCbs+=1)}function l(e,t){var r=""+V(e)+!!t;this.xhrGuids&&this.xhrGuids[r]&&(delete this.xhrGuids[r],this.totalCbs-=1)}function f(){this.endTime=(0,N.t)()}function h(e,r){r instanceof fe&&"load"===e[0]&&t.emit("xhr-load-added",[e[1],e[2]],r)}function g(e,r){r instanceof fe&&"load"===e[0]&&t.emit("xhr-load-removed",[e[1],e[2]],r)}function m(e,t,r){t instanceof fe&&("onload"===r&&(this.onload=!0),("load"===(e[0]&&e[0].type)||this.onload)&&(this.xhrCbStart=(0,N.t)()))}function v(e,r){this.xhrCbStart&&t.emit("xhr-cb-time",[(0,N.t)()-this.xhrCbStart,this.onload,r],r)}function b(e){var t,r=e[1]||{};if("string"==typeof e[0]?0===(t=e[0]).length&&p.RI&&(t=""+p.gm.location.href):e[0]&&e[0].url?t=e[0].url:p.gm?.URL&&e[0]&&e[0]instanceof URL?t=e[0].href:"function"==typeof e[0].toString&&(t=e[0].toString()),"string"==typeof t&&0!==t.length){t&&(this.parsedOrigin=(0,ie.D)(t),this.sameOrigin=this.parsedOrigin.sameOrigin);var i=n.generateTracePayload(this.parsedOrigin);if(i&&(i.newrelicHeader||i.traceContextParentHeader))if(e[0]&&e[0].headers)s(e[0].headers,i)&&(this.dt=i);else{var o={};for(var a in r)o[a]=r[a];o.headers=new Headers(r.headers||{}),s(o.headers,i)&&(this.dt=i),e.length>1?e[1]=o:e.push(o)}}function s(e,t){var r=!1;return t.newrelicHeader&&(e.set("newrelic",t.newrelicHeader),r=!0),t.traceContextParentHeader&&(e.set("traceparent",t.traceContextParentHeader),t.traceContextStateHeader&&e.set("tracestate",t.traceContextStateHeader),r=!0),r}}function y(e,t){this.params={},this.metrics={},this.startTime=(0,N.t)(),this.dt=t,e.length>=1&&(this.target=e[0]),e.length>=2&&(this.opts=e[1]);var r,n=this.opts||{},i=this.target;"string"==typeof i?r=i:"object"==typeof i&&i instanceof le?r=i.url:p.gm?.URL&&"object"==typeof i&&i instanceof URL&&(r=i.href),ge(this,r);var o=(""+(i&&i instanceof le&&i.method||n.method||"GET")).toUpperCase();this.params.method=o,this.body=n.body,this.txSize=G(n.body)||0}function w(e,t){if(this.endTime=(0,N.t)(),this.params||(this.params={}),(0,ce.iW)(this.params))return;let n;this.params.status=t?t.status:0,"string"==typeof this.rxSize&&this.rxSize.length>0&&(n=+this.rxSize);const i={txSize:this.txSize,rxSize:n,duration:(0,N.t)()-this.startTime};r("xhr",[this.params,i,this.startTime,this.endTime,"fetch"],this,a.K7.ajax)}function R(e){const t=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(let t=0;t<de;t++)e.removeEventListener(ue[t],this.listener,!1);t.aborted||(0,ce.iW)(t)||(n.duration=(0,N.t)()-this.startTime,this.loadCaptureCalled||4!==e.readyState?null==t.status&&(t.status=0):E(this,e),n.cbTime=this.cbTime,r("xhr",[t,n,this.startTime,this.endTime,"xhr"],this,a.K7.ajax))}}function E(e,r){e.params.status=r.status;var n=function(e,t){var r=e.responseType;return"json"===r&&null!==t?t:"arraybuffer"===r||"blob"===r||"json"===r?G(e.response):"text"===r||""===r||void 0===r?G(e.responseText):void 0}(r,e.lastSize);if(n&&(e.metrics.rxSize=n),e.sameOrigin&&r.getAllResponseHeaders().indexOf(he)>=0){var i=r.getResponseHeader(he);i&&((0,x.p)(_.rs,["Ajax/CrossApplicationTracing/Header/Seen"],void 0,a.K7.metrics,t),e.params.cat=i.split(", ").pop())}e.loadCaptureCalled=!0}t.on("new-xhr",i),t.on("open-xhr-start",o),t.on("open-xhr-end",s),t.on("send-xhr-start",c),t.on("xhr-cb-time",u),t.on("xhr-load-added",d),t.on("xhr-load-removed",l),t.on("xhr-resolved",f),t.on("addEventListener-end",h),t.on("removeEventListener-end",g),t.on("fn-end",v),t.on("fetch-before-start",b),t.on("fetch-start",y),t.on("fn-start",m),t.on("fetch-done",w)}(e,this.ee,this.handler,this.dt),this.importAggregator(e)}}function ge(e,t){var r=(0,ie.D)(t),n=e.params||e;n.hostname=r.hostname,n.port=r.port,n.protocol=r.protocol,n.host=r.hostname+":"+r.port,n.pathname=r.pathname,e.parsedOrigin=r,e.sameOrigin=r.sameOrigin}const me={},ve=["pushState","replaceState"];function be(e){const t=function(e){return(e||B.ee).get("history")}(e);return!p.RI||me[t.debugId]++||(me[t.debugId]=1,(0,W.YM)(t).inPlace(window.history,ve,"-")),t}var ye=i(3738);const{He:we,bD:Re,d3:xe,Kp:Ee,TZ:Ae,Lc:Te,uP:Ne,Rz:Se}=ye;class _e extends b{static featureName=Ae;constructor(e,t=!0){super(e,Ae,t);if(!(0,m.V)(this.agentIdentifier))return void this.deregisterDrain();const r=this.ee;let n;be(r),this.eventsEE=(0,F.u)(r),this.eventsEE.on(Ne,(function(e,t){this.bstStart=(0,N.t)()})),this.eventsEE.on(Te,(function(e,t){(0,x.p)("bst",[e[0],t,this.bstStart,(0,N.t)()],void 0,a.K7.sessionTrace,r)})),r.on(Se+xe,(function(e){this.time=(0,N.t)(),this.startPath=location.pathname+location.hash})),r.on(Se+Ee,(function(e){(0,x.p)("bstHist",[location.pathname+location.hash,this.startPath,this.time],void 0,a.K7.sessionTrace,r)}));try{n=new PerformanceObserver((e=>{const t=e.getEntries();(0,x.p)(we,[t],void 0,a.K7.sessionTrace,r)})),n.observe({type:Re,buffered:!0})}catch(e){}this.importAggregator(e,{resourceObserver:n})}}var Oe=i(2614);class Ie extends b{static featureName=t.TZ;#i;#o;constructor(e,r=!0){let n;super(e,t.TZ,r),this.replayRunning=!1,this.#o=e;try{n=JSON.parse(localStorage.getItem("".concat(Oe.H3,"_").concat(Oe.uh)))}catch(e){}(0,g.SR)(e.agentIdentifier)&&this.ee.on(t.G4.RECORD,(()=>this.#a())),this.#s(n)?(this.#i=n?.sessionReplayMode,this.#c()):this.importAggregator(e),this.ee.on("err",(e=>{this.replayRunning&&(this.errorNoticed=!0,(0,x.p)(t.G4.ERROR_DURING_REPLAY,[e],void 0,this.featureName,this.ee))})),this.ee.on(t.G4.REPLAY_RUNNING,(e=>{this.replayRunning=e}))}#s(e){return e&&(e.sessionReplayMode===Oe.g.FULL||e.sessionReplayMode===Oe.g.ERROR)||(0,g.Aw)(this.agentIdentifier)}#u=!1;async#c(e){if(!this.#u){this.#u=!0;try{const{Recorder:t}=await Promise.all([i.e(478),i.e(249)]).then(i.bind(i,8589));this.recorder??=new t({mode:this.#i,agentIdentifier:this.agentIdentifier,trigger:e,ee:this.ee,agentRef:this.#o}),this.recorder.startRecording(),this.abortHandler=this.recorder.stopRecording}catch(e){}this.importAggregator(this.#o,{recorder:this.recorder,errorNoticed:this.errorNoticed})}}#a(){this.featAggregate?this.featAggregate.mode!==Oe.g.FULL&&this.featAggregate.initializeRecording(Oe.g.FULL,!0):(this.#i=Oe.g.FULL,this.#c(t.Qb.API),this.recorder&&this.recorder.parent.mode!==Oe.g.FULL&&(this.recorder.parent.mode=Oe.g.FULL,this.recorder.stopRecording(),this.recorder.startRecording(),this.abortHandler=this.recorder.stopRecording))}}var Pe=i(3962);class je extends b{static featureName=Pe.TZ;constructor(e,t=!0){if(super(e,Pe.TZ,t),!p.RI||!(0,R.dV)().o.MO)return;const r=be(this.ee);Pe.tC.forEach((e=>{(0,A.sp)(e,(e=>{a(e)}),!0)}));const n=()=>(0,x.p)("newURL",[(0,N.t)(),""+window.location],void 0,this.featureName,this.ee);r.on("pushState-end",n),r.on("replaceState-end",n);try{this.removeOnAbort=new AbortController}catch(e){}(0,A.sp)("popstate",(e=>(0,x.p)("newURL",[e.timeStamp,""+window.location],void 0,this.featureName,this.ee)),!0,this.removeOnAbort?.signal);let i=!1;const o=new((0,R.dV)().o.MO)(((e,t)=>{i||(i=!0,requestAnimationFrame((()=>{(0,x.p)("newDom",[(0,N.t)()],void 0,this.featureName,this.ee),i=!1})))})),a=(0,v.s)((e=>{(0,x.p)("newUIEvent",[e],void 0,this.featureName,this.ee),o.observe(document.body,{attributes:!0,childList:!0,subtree:!0,characterData:!0})}),100,{leading:!0});this.abortHandler=function(){this.removeOnAbort?.abort(),o.disconnect(),this.abortHandler=void 0},this.importAggregator(e,{domObserver:o})}}var Ce=i(7378);const ke={},Le=["appendChild","insertBefore","replaceChild"];function He(e){const t=function(e){return(e||B.ee).get("jsonp")}(e);if(!p.RI||ke[t.debugId])return t;ke[t.debugId]=!0;var r=(0,W.YM)(t),n=/[?&](?:callback|cb)=([^&#]+)/,i=/(.*)\.([^.]+)/,o=/^(\w+)(\.|$)(.*)$/;function a(e,t){if(!e)return t;const r=e.match(o),n=r[1];return a(r[3],t[n])}return r.inPlace(Node.prototype,Le,"dom-"),t.on("dom-start",(function(e){!function(e){if(!e||"string"!=typeof e.nodeName||"script"!==e.nodeName.toLowerCase())return;if("function"!=typeof e.addEventListener)return;var o=(s=e.src,c=s.match(n),c?c[1]:null);var s,c;if(!o)return;var u=function(e){var t=e.match(i);if(t&&t.length>=3)return{key:t[2],parent:a(t[1],window)};return{key:e,parent:window}}(o);if("function"!=typeof u.parent[u.key])return;var d={};function l(){t.emit("jsonp-end",[],d),e.removeEventListener("load",l,(0,A.jT)(!1)),e.removeEventListener("error",f,(0,A.jT)(!1))}function f(){t.emit("jsonp-error",[],d),t.emit("jsonp-end",[],d),e.removeEventListener("load",l,(0,A.jT)(!1)),e.removeEventListener("error",f,(0,A.jT)(!1))}r.inPlace(u.parent,[u.key],"cb-",d),e.addEventListener("load",l,(0,A.jT)(!1)),e.addEventListener("error",f,(0,A.jT)(!1)),t.emit("new-jsonp",[e.src],d)}(e[0])})),t}const Me={};function De(e){const t=function(e){return(e||B.ee).get("promise")}(e);if(Me[t.debugId])return t;Me[t.debugId]=!0;var r=t.context,n=(0,W.YM)(t),i=p.gm.Promise;return i&&function(){function e(r){var o=t.context(),a=n(r,"executor-",o,null,!1);const s=Reflect.construct(i,[a],e);return t.context(s).getCtx=function(){return o},s}p.gm.Promise=e,Object.defineProperty(e,"name",{value:"Promise"}),e.toString=function(){return i.toString()},Object.setPrototypeOf(e,i),["all","race"].forEach((function(r){const n=i[r];e[r]=function(e){let i=!1;[...e||[]].forEach((e=>{this.resolve(e).then(a("all"===r),a(!1))}));const o=n.apply(this,arguments);return o;function a(e){return function(){t.emit("propagate",[null,!i],o,!1,!1),i=i||!e}}}})),["resolve","reject"].forEach((function(r){const n=i[r];e[r]=function(e){const r=n.apply(this,arguments);return e!==r&&t.emit("propagate",[e,!0],r,!1,!1),r}})),e.prototype=i.prototype;const o=i.prototype.then;i.prototype.then=function(...e){var i=this,a=r(i);a.promise=i,e[0]=n(e[0],"cb-",a,null,!1),e[1]=n(e[1],"cb-",a,null,!1);const s=o.apply(this,e);return a.nextPromise=s,t.emit("propagate",[i,!0],s,!1,!1),s},i.prototype.then[W.Jt]=o,t.on("executor-start",(function(e){e[0]=n(e[0],"resolve-",this,null,!1),e[1]=n(e[1],"resolve-",this,null,!1)})),t.on("executor-err",(function(e,t,r){e[1](r)})),t.on("cb-end",(function(e,r,n){t.emit("propagate",[n,!0],this.nextPromise,!1,!1)})),t.on("propagate",(function(e,r,n){this.getCtx&&!r||(this.getCtx=function(){if(e instanceof Promise)var r=t.context(e);return r&&r.getCtx?r.getCtx():this})}))}(),t}const Ke={},Ue="setTimeout",Ve="setInterval",Ge="clearTimeout",Fe="-start",Be=[Ue,"setImmediate",Ve,Ge,"clearImmediate"];function We(e){const t=function(e){return(e||B.ee).get("timer")}(e);if(Ke[t.debugId]++)return t;Ke[t.debugId]=1;var r=(0,W.YM)(t);return r.inPlace(p.gm,Be.slice(0,2),Ue+"-"),r.inPlace(p.gm,Be.slice(2,3),Ve+"-"),r.inPlace(p.gm,Be.slice(3),Ge+"-"),t.on(Ve+Fe,(function(e,t,n){e[0]=r(e[0],"fn-",null,n)})),t.on(Ue+Fe,(function(e,t,n){this.method=n,this.timerDuration=isNaN(e[1])?0:+e[1],e[0]=r(e[0],"fn-",this,n)})),t}const ze={};function qe(e){const t=function(e){return(e||B.ee).get("mutation")}(e);if(!p.RI||ze[t.debugId])return t;ze[t.debugId]=!0;var r=(0,W.YM)(t),n=p.gm.MutationObserver;return n&&(window.MutationObserver=function(e){return this instanceof n?new n(r(e,"fn-")):n.apply(this,arguments)},MutationObserver.prototype=n.prototype),t}const{TZ:Ze,d3:Ye,Kp:Je,$p:Xe,wW:$e,e5:Qe,tH:et,uP:tt,rw:rt,Lc:nt}=Ce;class it extends b{static featureName=Ze;constructor(e,t=!0){if(super(e,Ze,t),!p.RI)return;try{this.removeOnAbort=new AbortController}catch(e){}let r,n=0;const i=this.ee.get("tracer"),o=He(this.ee),a=De(this.ee),s=We(this.ee),c=Z(this.ee),u=this.ee.get("events"),d=ne(this.ee),l=be(this.ee),f=qe(this.ee);function h(e,t){l.emit("newURL",[""+window.location,t])}function g(){n++,r=window.location.hash,this[tt]=(0,N.t)()}function m(){n--,window.location.hash!==r&&h(0,!0);var e=(0,N.t)();this[Qe]=~~this[Qe]+e-this[tt],this[nt]=e}function v(e,t){e.on(t,(function(){this[t]=(0,N.t)()}))}this.ee.on(tt,g),a.on(rt,g),o.on(rt,g),this.ee.on(nt,m),a.on($e,m),o.on($e,m),this.ee.on("fn-err",((...t)=>{t[2]?.__newrelic?.[e.agentIdentifier]||(0,x.p)("function-err",[...t],void 0,this.featureName,this.ee)})),this.ee.buffer([tt,nt,"xhr-resolved"],this.featureName),u.buffer([tt],this.featureName),s.buffer(["setTimeout"+Je,"clearTimeout"+Ye,tt],this.featureName),c.buffer([tt,"new-xhr","send-xhr"+Ye],this.featureName),d.buffer([et+Ye,et+"-done",et+Xe+Ye,et+Xe+Je],this.featureName),l.buffer(["newURL"],this.featureName),f.buffer([tt],this.featureName),a.buffer(["propagate",rt,$e,"executor-err","resolve"+Ye],this.featureName),i.buffer([tt,"no-"+tt],this.featureName),o.buffer(["new-jsonp","cb-start","jsonp-error","jsonp-end"],this.featureName),v(d,et+Ye),v(d,et+"-done"),v(o,"new-jsonp"),v(o,"jsonp-end"),v(o,"cb-start"),l.on("pushState-end",h),l.on("replaceState-end",h),window.addEventListener("hashchange",h,(0,A.jT)(!0,this.removeOnAbort?.signal)),window.addEventListener("load",h,(0,A.jT)(!0,this.removeOnAbort?.signal)),window.addEventListener("popstate",(function(){h(0,n>1)}),(0,A.jT)(!0,this.removeOnAbort?.signal)),this.abortHandler=this.#n,this.importAggregator(e)}#n(){this.removeOnAbort?.abort(),this.abortHandler=void 0}}var ot=i(3333);class at extends b{static featureName=ot.TZ;constructor(e,t=!0){super(e,ot.TZ,t);const r=[e.init.page_action.enabled,e.init.performance.capture_marks,e.init.performance.capture_measures,e.init.user_actions.enabled,e.init.performance.resources.enabled];if(p.RI&&(e.init.user_actions.enabled&&(ot.Zp.forEach((e=>(0,A.sp)(e,(e=>(0,x.p)("ua",[e],void 0,this.featureName,this.ee)),!0))),ot.qN.forEach((e=>{const t=(0,v.s)((e=>{(0,x.p)("ua",[e],void 0,this.featureName,this.ee)}),500,{leading:!0});(0,A.sp)(e,t)}))),e.init.performance.resources.enabled&&p.gm.PerformanceObserver?.supportedEntryTypes.includes("resource"))){new PerformanceObserver((e=>{e.getEntries().forEach((e=>{(0,x.p)("browserPerformance.resource",[e],void 0,this.featureName,this.ee)}))})).observe({type:"resource",buffered:!0})}r.some((e=>e))?this.importAggregator(e):this.deregisterDrain()}}var st=i(993),ct=i(3785),ut=i(9414);class dt extends b{static featureName=st.TZ;constructor(e,t=!0){super(e,st.TZ,t);const r=this.ee;(0,ut.J)(r,p.gm.console,"log",{level:"info"}),(0,ut.J)(r,p.gm.console,"error",{level:"error"}),(0,ut.J)(r,p.gm.console,"warn",{level:"warn"}),(0,ut.J)(r,p.gm.console,"info",{level:"info"}),(0,ut.J)(r,p.gm.console,"debug",{level:"debug"}),(0,ut.J)(r,p.gm.console,"trace",{level:"trace"}),this.ee.on("wrap-logger-end",(function([e]){const{level:t,customAttributes:n}=this;(0,ct.R)(r,e,n,t)})),this.importAggregator(e)}}new class extends o{constructor(t){super(),p.gm?(this.features={},(0,R.bQ)(this.agentIdentifier,this),this.desiredFeatures=new Set(t.features||[]),this.desiredFeatures.add(w),this.runSoftNavOverSpa=[...this.desiredFeatures].some((e=>e.featureName===a.K7.softNav)),(0,d.j)(this,t,t.loaderType||"agent"),this.run()):(0,e.R)(21)}get config(){return{info:this.info,init:this.init,loader_config:this.loader_config,runtime:this.runtime}}run(){try{const t=u(this.agentIdentifier),r=[...this.desiredFeatures];r.sort(((e,t)=>a.P3[e.featureName]-a.P3[t.featureName])),r.forEach((r=>{if(!t[r.featureName]&&r.featureName!==a.K7.pageViewEvent)return;if(this.runSoftNavOverSpa&&r.featureName===a.K7.spa)return;if(!this.runSoftNavOverSpa&&r.featureName===a.K7.softNav)return;const n=function(e){switch(e){case a.K7.ajax:return[a.K7.jserrors];case a.K7.sessionTrace:return[a.K7.ajax,a.K7.pageViewEvent];case a.K7.sessionReplay:return[a.K7.sessionTrace];case a.K7.pageViewTiming:return[a.K7.pageViewEvent];default:return[]}}(r.featureName).filter((e=>!(e in this.features)));n.length>0&&(0,e.R)(36,{targetFeature:r.featureName,missingDependencies:n}),this.features[r.featureName]=new r(this)}))}catch(t){(0,e.R)(22,t);for(const e in this.features)this.features[e].abortHandler?.();const r=(0,R.Zm)();delete r.initializedAgents[this.agentIdentifier]?.api,delete r.initializedAgents[this.agentIdentifier]?.features,delete this.sharedAggregator;return r.ee.get(this.agentIdentifier).abort(),!1}}}({features:[pe,w,S,_e,Ie,O,M,at,dt,je,it],loaderType:"spa"})})()})();</script><link rel="stylesheet" href="/_next/static/css/a7a3d62da9223916.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script id="gpt-script" src="https://pagead2.googlesyndication.com/tag/js/gpt.js" defer="" data-nscript="beforeInteractive"></script><script defer="" src="/_next/static/chunks/2727.f55b653e99e86bd1.js"></script><script defer="" src="/_next/static/chunks/7905-2e80e8201b7198a8.js"></script><script defer="" src="/_next/static/chunks/3032-7dc6ff7e1ffa6f45.js"></script><script defer="" src="/_next/static/chunks/3919.ed30790fe9ad698b.js"></script><script defer="" src="/_next/static/chunks/3250-b60d0ac26412ade8.js"></script><script defer="" src="/_next/static/chunks/8082.d62b1cb034034aa7.js"></script><script defer="" src="/_next/static/chunks/4187.93a2a0e5b278687e.js"></script><script defer="" src="/_next/static/chunks/9102-1e2efab6d325a9ef.js"></script><script defer="" src="/_next/static/chunks/1585-f25f2f5eb98520ba.js"></script><script defer="" src="/_next/static/chunks/8665-8b14ceddb68559a0.js"></script><script defer="" src="/_next/static/chunks/8519-6af8961eb77f5fad.js"></script><script defer="" src="/_next/static/chunks/3803-b1925c4d0db877da.js"></script><script defer="" src="/_next/static/chunks/1325-7a81528b0aff3745.js"></script><script defer="" src="/_next/static/chunks/2888.315c5f0094f17987.js"></script><script defer="" src="/_next/static/chunks/459-f618fc8649b14412.js"></script><script defer="" src="/_next/static/chunks/201-86b2f47ccc319c31.js"></script><script defer="" src="/_next/static/chunks/507.24c3284a7cebfea3.js"></script><script defer="" src="/_next/static/chunks/9865-90f10272e84d8e92.js"></script><script defer="" src="/_next/static/chunks/1572.a9f76de46949f17a.js"></script><script src="/_next/static/chunks/webpack-b258d164f6fadbba.js" defer=""></script><script src="/_next/static/chunks/framework-0c5ec62873a83b09.js" defer=""></script><script src="/_next/static/chunks/main-9d04da2a3c065e40.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0441a919e7181701.js" defer=""></script><script src="/_next/static/chunks/8232-eb042c7105ce0360.js" defer=""></script><script src="/_next/static/chunks/9633-69ec5d5457104cf4.js" defer=""></script><script src="/_next/static/chunks/2715-0f44809511a9fcdb.js" defer=""></script><script src="/_next/static/chunks/5336-af44af53c63058d6.js" defer=""></script><script src="/_next/static/chunks/2076-f1a0287ba5294684.js" defer=""></script><script src="/_next/static/chunks/6463-d21aa08a634cad27.js" defer=""></script><script src="/_next/static/chunks/9217-ede24d8c4f81afdc.js" defer=""></script><script src="/_next/static/chunks/6196-fe37346bc0269fa5.js" defer=""></script><script src="/_next/static/chunks/3753-f4c70e8d08773fe5.js" defer=""></script><script src="/_next/static/chunks/pages/talks/%5B...slug%5D-1776a745de5f455e.js" defer=""></script><script src="/_next/static/Wbr9uI2wHDOXAcMZVzpse/_buildManifest.js" defer=""></script><script src="/_next/static/Wbr9uI2wHDOXAcMZVzpse/_ssgManifest.js" defer=""></script></head><body class="antialiased"><link rel="preload" as="image" imageSrcSet="https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 640w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 750w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 828w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1080w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1200w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1920w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 2048w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 3840w" imageSizes="100vw"/><div id="__next"><div id="ad-funbzaaeva"></div><div class="flex min-h-screen w-full flex-col bg-white"><header class="z-30 -mt-14 translate-y-full"><div class="z-10 w-full"><div class="h-14 bg-white px-5 shadow-md dark:bg-black dark:shadow-none md:px-10 lg:px-6"><div class="relative flex items-center justify-between"><button type="button" class="flex -translate-x-25 transform items-center justify-center p-25 dark:text-white lg:hidden" aria-label="Menu"><i aria-hidden="true" class="icon-menu text-tui-3xl"></i></button><div class="relative"><div class="hidden lg:block"><a class="text-base leading-md sr-only font-bold focus:not-sr-only outline-inside flex items-center justify-center bg-white focus:absolute focus:h-full focus:w-full" href="#maincontent">Skip to main content</a><a class="text-base leading-md sr-only font-bold focus:not-sr-only outline-inside flex items-center justify-center bg-white focus:absolute focus:h-full focus:w-full" href="#navigation-search">Skip to search</a></div><a class="outline-inside flex items-center py-4 relative" aria-label="TED Homepage - Ideas change everything" title="Return to TED Homepage" href="/"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 68 24" class="h-6 lg:mr-3" aria-hidden="true"><path fill="#EB0028" fill-rule="evenodd" d="M21.419 0v6.151h-6.763V24h-7.44V6.151H.453V0zm21.484 0v6.141l-12.918.01v2.946h12.918v5.73l-12.918-.009v3.03h12.918V24H22.546V0zm13.446 0c8.048 0 10.889 5.916 10.889 11.967 0 7.36-3.923 12.033-12.343 12.033H44.142V0zm-2.4 6.151H51.58V17.85h2.908c4.633 0 5.31-3.731 5.31-5.983 0-1.513-.474-5.715-5.85-5.715" clip-rule="evenodd"></path></svg><div class="text-sm hidden text-gray-500 dark:text-white lg:block">Ideas change everything</div></a></div><div class="flex items-center lg:mr-14"><div class="flex items-center transition-opacity opacity-100"><nav class="hidden lg:block"><ul class="flex"><li class="navigation-header-menu-wrapper group relative"><button class="menuButton outline-inside border-4 px-3 py-5 group-hover:bg-gray-50 dark:rounded-sm dark:py-4 dark:group-hover:bg-black dark:group-hover:bg-gray-800" aria-expanded="false" aria-haspopup="true" aria-controls=":R5pcm6:-menu-0-list"><div class="text-xs font-bold text-gray-900 dark:text-white">WATCH</div></button><div role="menu" id=":R5pcm6:-menu-0"><div id=":R5pcm6:-menu-0-list" role="menu" class="absolute left-1/2 top-full z-10 -translate-x-1/2 transform overflow-hidden rounded-md border-thin border-gray-300 bg-white shadow-xl outline-none dark:border-x-[0px] dark:border-b-[0px] dark:border-t-thickest dark:border-t-black dark:bg-gray-800 hidden"><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/talks"><div class="text-base leading-md w-60 font-bold dark:text-white">TED Talks</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Browse the library of TED talks and speakers</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/playlists"><div class="text-base leading-md w-60 font-bold dark:text-white">Playlists</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">100+ collections of TED Talks, for curious minds</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/series"><div class="text-base leading-md w-60 font-bold dark:text-white">TED Series</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Go deeper into fascinating topics with original video series from TED</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/watch/ted-ed"><div class="text-base leading-md w-60 font-bold dark:text-white">TED-Ed videos</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Watch, share and create lessons with TED-Ed</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/watch/tedx-talks"><div class="text-base leading-md w-60 font-bold dark:text-white">TEDx Talks</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Talks from independently organized local events</div></a></div></div></li><li class="navigation-header-menu-wrapper group relative"><button class="menuButton outline-inside border-4 px-3 py-5 group-hover:bg-gray-50 dark:rounded-sm dark:py-4 dark:group-hover:bg-black dark:group-hover:bg-gray-800" aria-expanded="false" aria-haspopup="true" aria-controls=":R5pcm6:-menu-1-list"><div class="text-xs font-bold text-gray-900 dark:text-white">DISCOVER</div></button><div role="menu" id=":R5pcm6:-menu-1"><div id=":R5pcm6:-menu-1-list" role="menu" class="absolute left-1/2 top-full z-10 -translate-x-1/2 transform overflow-hidden rounded-md border-thin border-gray-300 bg-white shadow-xl outline-none dark:border-x-[0px] dark:border-b-[0px] dark:border-t-thickest dark:border-t-black dark:bg-gray-800 hidden"><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/topics"><div class="text-base leading-md w-60 font-bold dark:text-white">Topics</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Explore TED offerings by topic</div></a><a href="https://audiocollective.ted.com" class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700" role="menuitem"><div class="text-base leading-md w-60 font-bold dark:text-white">Podcasts</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Explore the TED Audio Collective</div></a><a href="https://ideas.ted.com" class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700" role="menuitem"><div class="text-base leading-md w-60 font-bold dark:text-white">Ideas Blog</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Our daily coverage of the world of ideas</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/newsletters"><div class="text-base leading-md w-60 font-bold dark:text-white">Newsletters</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Inspiration delivered straight to your inbox</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/games"><div class="text-base leading-md w-60 font-bold dark:text-white">TED Games</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Innovative games that challenge you to think differently</div></a></div></div></li><li class="navigation-header-menu-wrapper group relative"><button class="menuButton outline-inside border-4 px-3 py-5 group-hover:bg-gray-50 dark:rounded-sm dark:py-4 dark:group-hover:bg-black dark:group-hover:bg-gray-800" aria-expanded="false" aria-haspopup="true" aria-controls=":R5pcm6:-menu-2-list"><div class="text-xs font-bold text-gray-900 dark:text-white">ATTEND</div></button><div role="menu" id=":R5pcm6:-menu-2"><div id=":R5pcm6:-menu-2-list" role="menu" class="absolute left-1/2 top-full z-10 -translate-x-1/2 transform overflow-hidden rounded-md border-thin border-gray-300 bg-white shadow-xl outline-none dark:border-x-[0px] dark:border-b-[0px] dark:border-t-thickest dark:border-t-black dark:bg-gray-800 hidden"><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/attend/conferences"><div class="text-base leading-md w-60 font-bold dark:text-white">Conferences</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Take part in our events: TED, TEDGlobal and more</div></a><a href="https://ted.com/tedx/events" class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700" role="menuitem"><div class="text-base leading-md w-60 font-bold dark:text-white">TEDx Events</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Find and attend local, independently organized events</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/attend/ted-on-screen"><div class="text-base leading-md w-60 font-bold dark:text-white">TED on Screen</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Experience TED from home</div></a><a href="https://courses.ted.com/" class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700" role="menuitem"><div class="text-base leading-md w-60 font-bold dark:text-white">TED Courses</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Learn from TED speakers who expand on their world-changing ideas</div></a></div></div></li><li class="navigation-header-menu-wrapper group relative"><button class="menuButton outline-inside border-4 px-3 py-5 group-hover:bg-gray-50 dark:rounded-sm dark:py-4 dark:group-hover:bg-black dark:group-hover:bg-gray-800" aria-expanded="false" aria-haspopup="true" aria-controls=":R5pcm6:-menu-3-list"><div class="text-xs font-bold text-gray-900 dark:text-white">PARTICIPATE</div></button><div role="menu" id=":R5pcm6:-menu-3"><div id=":R5pcm6:-menu-3-list" role="menu" class="absolute left-1/2 top-full z-10 -translate-x-1/2 transform overflow-hidden rounded-md border-thin border-gray-300 bg-white shadow-xl outline-none dark:border-x-[0px] dark:border-b-[0px] dark:border-t-thickest dark:border-t-black dark:bg-gray-800 hidden"><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/participate/nominate"><div class="text-base leading-md w-60 font-bold dark:text-white">Nominate</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Recommend speakers, TED Prize recipients, Fellows and more</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/participate/organize-a-local-tedx-event"><div class="text-base leading-md w-60 font-bold dark:text-white">Organize a local TEDx Event</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Rules and resources to help you plan a local TEDx event</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/about/programs/ted-ed"><div class="text-base leading-md w-60 font-bold dark:text-white">TED-Ed</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Award-winning educational content and programs</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/participate/translate"><div class="text-base leading-md w-60 font-bold dark:text-white">Translate</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Bring TED to the non-English speaking world</div></a><a href="https://fellows.ted.com/" class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700" role="menuitem"><div class="text-base leading-md w-60 font-bold dark:text-white">TED Fellows</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Join or support innovators from around the globe</div></a></div></div></li><li class="navigation-header-menu-wrapper group relative"><button class="menuButton outline-inside border-4 px-3 py-5 group-hover:bg-gray-50 dark:rounded-sm dark:py-4 dark:group-hover:bg-black dark:group-hover:bg-gray-800" aria-expanded="false" aria-haspopup="true" aria-controls=":R5pcm6:-menu-4-list"><div class="text-xs font-bold text-gray-900 dark:text-white">ABOUT</div></button><div role="menu" id=":R5pcm6:-menu-4"><div id=":R5pcm6:-menu-4-list" role="menu" class="absolute left-1/2 top-full z-10 -translate-x-1/2 transform overflow-hidden rounded-md border-thin border-gray-300 bg-white shadow-xl outline-none dark:border-x-[0px] dark:border-b-[0px] dark:border-t-thickest dark:border-t-black dark:bg-gray-800 hidden"><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/about"><div class="text-base leading-md w-60 font-bold dark:text-white">Our Organization</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Our mission, history, team, and more</div></a><a href="/about/conferences" class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700" role="menuitem"><div class="text-base leading-md w-60 font-bold dark:text-white">Conferences</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">TED Conferences, past, present, and future</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/about/programs-initiatives"><div class="text-base leading-md w-60 font-bold dark:text-white">Programs &amp; Initiatives</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Details about TED&#x27;s world-changing initiatives</div></a><a class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700 relative" role="menuitem" href="/about/partner-with-ted"><div class="text-base leading-md w-60 font-bold dark:text-white">Partner with TED</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Learn how you can partner with us</div></a><a href="https://blog.ted.com/" class="menuItem block px-8 py-4 first:pt-8 last:pb-8 hover:bg-gray-50 dark:hover:bg-gray-700" role="menuitem"><div class="text-base leading-md w-60 font-bold dark:text-white">TED Blog</div><div class="text-sm w-60 text-gray-700 dark:text-[#CFCFD0]">Updates from TED and highlights from our global community</div></a></div></div></li></ul></nav><a id="" rel="noreferrer noopener" href="https://auth.ted.com/users/new" class="outline-inside flex translate-x-2 transform items-center p-2 hover:underline dark:my-1 dark:rounded-sm dark:hover:bg-gray-800 dark:hover:text-gray-300 lg:transform-none lg:px-3 lg:py-5 lg:hover:no-underline dark:lg:py-4 relative"><div class="text-xs font-bold text-black dark:text-white dark:hover:hover:text-gray-300 lg:text-gray-900 dark:lg:text-white">SIGN IN</div></a><a id="vwo-membership-standard" class="vwo-membership-standard ml-3 hidden lg:block rounded-sm relative" href="/membership?utm_medium=website&amp;utm_source=main-nav-header&amp;utm_campaign=membership-ted"><div class="relative inline-flex min-w-button select-none items-center justify-center rounded-sm py-3 transition-colors duration-1000 min-h-button-sm px-4 bg-red-700 text-white hover:bg-red-900"><div class="inline-flex items-center transition-opacity duration-300 opacity-100"><div class="text-xs font-bold">MEMBERSHIP</div></div></div></a></div><div class="lg:hidden"></div><div class="absolute right-0 hidden lg:block"><form class="-mr-6 box-content flex h-14 shrink-0 flex-row flex-nowrap items-center justify-start overflow-hidden pr-6 transition-all w-11 bg-white dark:bg-black"><button type="button" class="outline-inside flex h-11 shrink-0 translate-x-15 items-center justify-center transition-width hover:text-gray-700 dark:text-white dark:hover:text-gray-350 w-11 opacity-100" aria-label="Type to search"><i aria-hidden="true" class="icon-search text-tui-4xl"></i></button><div class="flex size-full min-w-0 items-center"><label for="navigation-search" class="sr-only">Type to search</label><input type="search" id="navigation-search" placeholder="Type to search" aria-label="Type to search" class="h-full min-w-0 flex-1 border-none bg-transparent pl-6 pr-05 text-sm outline-none focus:ring-0 dark:text-gray-350" disabled="" value=""/><button type="button" class="translate-x-15 p-15 transition-opacity hover:text-gray-700 opacity-0" disabled="" aria-label="Cancel"><i aria-hidden="true" class="icon-x text-tui-4xl dark:text-gray-350"></i></button></div></form></div></div></div></div></div></header><main id="maincontent" class="relative flex w-full flex-grow flex-col pt-14 bg-gray-50 min-h-screen"><div class="w-full max-w-full bg-white px-5 md:px-10 xl:px-16"><div class="mx-auto w-full sm:px-0 max-w-[1200px]"><div class="flex min-h-[600px] w-full flex-col lg:flex-row" style="flex:1;padding-left:0px;padding-right:0px"><div class="flex w-full flex-1 flex-col lg-tui:pt-4"><div class="w-full overflow-hidden aspect-h-9 sticky top-0 z-20 bg-white lg:relative lg:top-[unset] lg:block"><div class="relative"><div class="md:hidden"></div><media-controller autohide="2" defaultstreamtype="ondemand" style="aspect-ratio:16/9;--media-tooltip-display:none;min-width:0;width:100%;height:100%" class="size-full"><div slot="centered-chrome" id="media-wrapper" class="pointer-events-none" style="border-radius:50%;--media-control-padding:48px;--media-control-height:48px"><media-play-button aria-label="play" tabindex="0" role="button"><span slot="icon"><div class="group absolute inset-0 flex size-full cursor-pointer items-center justify-center"><div type="button" class="w-20 h-20 relative flex items-center justify-center rounded-full bg-white"><div class="absolute size-full rounded-full p-1 opacity-0 transition-transform delay-1000 duration-150 ease-in-out group-hover:scale-[1.15] animate-playButton"></div><svg fill="none" viewBox="0 -4 20 31" class="h-auto translate-x-[12%] text-black w-8"><path fill="currentColor" fill-rule="evenodd" d="m17.274 12.25-15 9.52A1.48 1.48 0 0 1 0 20.52V1.48A1.48 1.48 0 0 1 2.273.23l15.001 9.52a1.48 1.48 0 0 1 0 2.5" clip-rule="evenodd"></path></svg></div></div></span></media-play-button></div><div slot="poster" class="relative"><section class="relative flex size-full items-stretch"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full w-full" style="padding-top:56.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block"><img alt="" decoding="async" data-nimg="fill" class="object-cover object-top" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 640w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 750w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 828w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1080w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1200w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 1920w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 2048w, https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800 3840w" src="https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?u%5Br%5D=2&amp;u%5Bs%5D=0.5&amp;u%5Ba%5D=0.8&amp;u%5Bt%5D=0.03&amp;quality=80&amp;w=800"/></div></div></div></section></div><!--$--><!--/$--></media-controller></div></div><div class="px-5 sm-tui:px-0"><div class="flex w-full flex-col md:mb-4"><div class="flex w-full flex-col justify-between"><div id="talk-title" class="mb-1 mt-2 flex w-full flex-col"><h1 class="mr-5 text-textPrimary-onLight font-normal text-tui-2xl leading-tui-sm tracking-tui-tight md:text-tui-3xl md:tracking-tui-tightest lg:text-tui-4xl" dir="ltr">The race to build AI that benefits humanity with Sam Altman (from April 2021)</h1></div><div class="flex flex-1 flex-wrap items-center overflow-hidden"><div class="text-sm text-gray-900"><div class="mr-1 flex items-center gap-1">1,051,995<!-- --> plays<button type="button" id="info-button-talk-page" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="17" fill="none" class="mb-[0.5px] h-4"><path stroke="#121212" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8 15.167A6.667 6.667 0 1 0 8 1.834a6.667 6.667 0 0 0 0 13.333M8 11.167V8.5M8 5.834h.007"></path></svg></button>|</div></div><div class="text-sm mr-1 text-gray-900">The TED Interview <!-- --> |</div><div class="text-sm mr-1 text-gray-900"> TED Audio Collective </div><div class="text-sm text-gray-900">  April 2021</div></div><div class="mb-2 mt-3 w-full bg-black dark:bg-[#898989] h-px opacity-16"></div><div class="flex w-full flex-col md:mb-4 md:flex-row md:items-center md:justify-between"><div class="flex overflow-x-scroll scroll-smooth scrollbar-hide md:mb-0 md:overflow-x-hidden"><div class="relative flex items-center gap-3 md:gap-2"><div></div><div data-state="closed"></div><div data-state="closed"></div></div></div><button id="transcript-control" class="mt-4 items-center justify-center whitespace-nowrap rounded-sm border-thin border-solid border-black px-4 py-3 text-tui-xs font-bold lg:my-0" type="button">Read transcript</button></div></div><div class="flex"><div class="mr-2 flex w-full lg:flex-col"><div class="hidden lg:block"><div class="flex w-full flex-col"><div class="mb-4"><span class="text-textPrimary-onLight font-normal text-tui-sm leading-tui-md tracking-tui-tight lg:leading-tui-lg" dir="ltr">In this season of The TED Interview, conversations with people who make a case for ... optimism. Not some blind, hopeful feeling, but the conviction that somewhere out there are solutions that, given the right attention and resources, can guide us out of the dark place we&#x27;re in. For the first episode: artificial intelligence. Will innovation in AI drastically improve our lives, or destroy humanity as we know it? Head of TED Chris Anderson sits down with OpenAI CEO Sam Altman, who makes a case for AI&#x27;s potential to make the future better for all of us -- and explains how his company is leading that charge with an unusual new business model. Listen and subscribe to The TED Interview and more podcasts from the TED Audio Collective wherever you&#x27;re listening to this.</span></div><ul class="mb-6 inline-block"><li class="mr-2 inline-block after:content-[&#x27;,&#x27;] last:mr-0 last:after:content-[&#x27;_&#x27;]"><a class="inline-block py-1 text-tui-sm capitalize underline relative" href="/topics/science">science</a></li><li class="mr-2 inline-block after:content-[&#x27;,&#x27;] last:mr-0 last:after:content-[&#x27;_&#x27;]"><a class="inline-block py-1 text-tui-sm capitalize underline relative" href="/topics/technology">technology</a></li><li class="mr-2 inline-block after:content-[&#x27;,&#x27;] last:mr-0 last:after:content-[&#x27;_&#x27;]"><a class="inline-block py-1 text-tui-sm capitalize underline relative" href="/topics/future">future</a></li><li class="mr-2 inline-block after:content-[&#x27;,&#x27;] last:mr-0 last:after:content-[&#x27;_&#x27;]"><a class="inline-block py-1 text-tui-sm capitalize underline relative" href="/topics/ai">AI</a></li></ul></div></div></div></div></div><div class="mx-auto mb-10 flex w-[320px]"><div id="talkpage-billboard-btf" class="ted-ads-test-id"></div></div></div></div><div class="order-last px-5 lg:order-none sm-tui:px-0" style="contain:layout style paint"><div class="lg:sticky lg:top-4 lg:max-w-[425px] xl:max-w-[536px]"><!--$--><aside class="group/aside relative top-0 w-full lg:sticky lg:max-w-[425px] xl:max-w-[536px]"><div class="lg:absolute lg:right-0 lg:top-0 lg:z-[1] lg:h-full lg:w-[17px] lg:bg-white"></div><div class="h-full pt-6 transition-all duration-1000 ease-in-out lg:sticky lg:top-0 lg:max-h-screen lg:overflow-y-scroll lg:pl-8 lg:pr-2 xl:pl-12 xl:pr-4"><div><div class=""><div class="mb-6"><div><div class="w-full" role="tablist"><div class="tabs-overflow-container"><div role="tablist"><button role="tab" aria-selected="true" aria-controls="panel-:Rlcqmkm6:-0" id="tab-:Rlcqmkm6:-0" class="tab outline-inside group pl-3 pr-3 text-gray-500 transition-colors first:pl-0 last:pr-0 hover:text-gray-700 focus:text-gray-700 group md:pl-4 md:pr-4 xl:pl-5 xl:pr-5" tabindex="0" type="button"><div class="tab-text relative pb-2 group-aria-selected:font-bold group-aria-selected:text-textPrimary-onLight text-lg xl:text-2xl font-bold tracking-normal out whitespace-nowrap">Watch next</div></button></div></div><div><div role="tabpanel" aria-labelledby="tab-:Rpcqmkm6:-0" id="panel-:Rpcqmkm6:-0" tabindex="0"><div class="mt-3"><div id="related-videos-standard"><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div></div><div id="related-videos-personalized" class="hidden h-0"><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div><div class="mb-6 animate-pulse"><div class="relative block w-full" aria-hidden="false"><div class="relative block size-full" style="padding-top:31.25%"><div class="absolute bottom-0 left-0 right-0 top-0 block bg-gray-300"></div></div></div></div></div></div></div></div></div><div class="w-full pt-3"><div class="mx-auto pb-2 text-center hidden"><span class="text-textPrimary-onLight text-tui-sm leading-tui-lg tracking-tui-tight font-normal" style="color:#717171" dir="ltr">TED is supported by ads and partners</span></div><div class="mx-auto w-[300px]"><div id="talkpage-sidebar-btf" class="ted-ads-test-id"></div></div></div></div></div><div class="mb-6"><h4 class="text-sm xl:text-base font-bold tracking-normal mb-2 uppercase text-red-700">Related Topics</h4><ul class="mb-6 inline-block text-black"><li class="mr-2 inline-block after:content-[&#x27;,&#x27;] last:mr-0 last:after:content-[&#x27;_&#x27;]"><a class="inline-block py-1 text-tui-sm capitalize underline relative" href="/topics/science">science</a></li><li class="mr-2 inline-block after:content-[&#x27;,&#x27;] last:mr-0 last:after:content-[&#x27;_&#x27;]"><a class="inline-block py-1 text-tui-sm capitalize underline relative" href="/topics/technology">technology</a></li><li class="mr-2 inline-block after:content-[&#x27;,&#x27;] last:mr-0 last:after:content-[&#x27;_&#x27;]"><a class="inline-block py-1 text-tui-sm capitalize underline relative" href="/topics/future">future</a></li><li class="mr-2 inline-block after:content-[&#x27;,&#x27;] last:mr-0 last:after:content-[&#x27;_&#x27;]"><a class="inline-block py-1 text-tui-sm capitalize underline relative" href="/topics/ai">AI</a></li></ul></div></div></div><div style="position:fixed;top:0;bottom:0;left:0;right:0;overflow:hidden;pointer-events:none;z-index:-1;visibility:hidden"></div></div></aside><!--/$--></div></div></div></div></div></main><footer class="flex w-full"><footer class="w-full bg-black"><div class="w-full px-5 md:px-10 xl:px-16"><div class="mx-auto w-full sm:px-0"><svg xmlns="http://www.w3.org/2000/svg" width="67" height="24" fill="none" class="my-12 h-6"><g clip-path="url(#ted-white-logo_svg__a)"><path fill="#fff" fill-rule="evenodd" d="M20.966 0v6.151h-6.763V24h-7.44V6.151H0V0zM42.45 0v6.141l-12.918.01v2.946H42.45v5.73l-12.918-.009v3.03H42.45V24H22.092V0zm13.446 0c8.048 0 10.888 5.916 10.888 11.967 0 7.36-3.922 12.033-12.343 12.033H43.689V0zm-2.401 6.151h-2.367V17.85h2.908c4.633 0 5.31-3.731 5.31-5.983 0-1.513-.474-5.715-5.851-5.715" clip-rule="evenodd"></path></g><defs><clipPath id="ted-white-logo_svg__a"><path fill="#fff" d="M0 0h67v24H0z"></path></clipPath></defs></svg><div class="pr-5 ted-grid -mr-4 flex flex-wrap md:-mr-5"><div class="pr-4 md:pr-5 col-sm-4 col-md-6 col-xl-4"><div><nav><div class="ted-grid -mr-4 flex flex-wrap md:-mr-5"><div class="pr-4 md:pr-5 col-sm-8 col-md-6"><div class="pb-4"><div class="lg:pb-0 lg:pt-0"><div class="text-sm mb-6 font-bold text-white">Explore</div><ul class="ted-grid -mr-4 flex flex-wrap md:-mr-5"><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline relative" href="/about/programs-initiatives/tedx-program"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TEDx</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline" href="https://fellows.ted.com"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Fellows</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline relative" href="/about/programs/ted-ed"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Ed</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline relative" href="/about/programs-initiatives/ted-translators"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Translators</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline relative" href="/about/programs-initiatives/ted-institute"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Institute</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline relative" href="/about/programs-initiatives/the-audacious-project"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">The Audacious Project</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline" href="https://tedatwork.ted.com"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED@Work</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline relative" href="/podcasts"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">Podcasts</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline" href="https://blog.ted.com"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Blog</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline relative" href="/about/programs-initiatives/ted-talks/ways-to-get-ted-talks"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">More ways to get TED</span></a></div></li></ul></div></div></div><div class="pr-4 md:pr-5 col-sm-8 border-t-thin border-textTertiary-onDark w-full lg:w-auto pt-10 md:border-none md:pt-0 col-md-6"><div class="pb-4"><div class="lg:pb-0 lg:pt-0"><div class="text-sm mb-6 font-bold text-white">Our community</div><ul class="ted-grid -mr-4 flex flex-wrap md:-mr-5"><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline" href="/people/speakers"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Speakers</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline" href="https://fellows.ted.com/directory"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Fellows</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline" href="/people/translators"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Translators</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline" href="/people/tedx"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TEDx Organizers</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-12 col-lg-8 col-xl-12"><div class="mb-5 max-w-max"><a class="text-white hover:underline" href="/people"><span class="!text-tui-sm text-white text-textPrimary-onDark font-normal text-tui-base leading-tui-lg tracking-tui-tight" dir="ltr">TED Community</span></a></div></li></ul></div></div></div></div></nav></div></div><div class="pr-4 md:pr-5 col-sm-12 w-full border-t-thin border-textTertiary-onDark pt-10 md:border-none md:pt-0 pr-6 sm:pr-0 lg:  col-md-6 col-lg-6 col-xl-4"><div><div class="text-sm bold mb-6 font-bold text-white">Newsletters</div><div class="flex flex-row align-middle text-white"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" class="mr-4 h-6"><g fill="#B5B5B5" fill-rule="evenodd" clip-path="url(#email_svg__a)" clip-rule="evenodd"><path d="M4 4.75c-.686 0-1.25.564-1.25 1.25v12c0 .686.564 1.25 1.25 1.25h16c.686 0 1.25-.564 1.25-1.25V6c0-.686-.564-1.25-1.25-1.25zM1.25 6A2.756 2.756 0 0 1 4 3.25h16A2.756 2.756 0 0 1 22.75 6v12A2.756 2.756 0 0 1 20 20.75H4A2.756 2.756 0 0 1 1.25 18z"></path><path d="M1.386 5.57a.75.75 0 0 1 1.044-.184L12 12.085l9.57-6.7a.75.75 0 1 1 .86 1.23l-10 7a.75.75 0 0 1-.86 0l-10-7a.75.75 0 0 1-.184-1.045"></path></g><defs><clipPath id="email_svg__a"><path fill="#fff" d="M0 0h24v24H0z"></path></clipPath></defs></svg><div class="text-sm bold pt-1 font-bold">Get the latest talks</div></div><div class="text-sm my-4 text-white">Get a daily email featuring the latest talk, plus a quick mix of trending content.</div><form><div class="mb-3 flex rounded-sm bg-white flex-row justify-between border-thin border-white"><input type="email" tabindex="-1" aria-hidden="true" style="opacity:0;width:0;height:0;padding:0;margin:0;border:none"/><input type="checkbox" aria-hidden="true" tabindex="-1" style="opacity:0;width:0;height:0;padding:0;margin:0;border:none"/><div class="relative w-full"><input placeholder="What&#x27;s your email?" id="newsletter-email" class="flex h-11 w-full flex-1 rounded-sm border-none bg-white p-4" type="email"/></div><div class="flex justify-center rounded-r-sm align-middle h-11 bg-black"><button type="submit" class="group relative flex items-center justify-center overflow-hidden ring-blue-300 ring-offset-2 transition-all duration-300 ease-in-out focus-visible:ring-1 active:transition-none active:hover:bg-opacity-100 hover:bg-white hover:bg-opacity-16 active:bg-[#4D3136] w-max rounded-sm h-11 gap-2 px-4 h-11 px-10"><p class="text-textSecondary-onDark font-bold text-tui-sm leading-tui-md tracking-tui-tight lg:leading-tui-lg" dir="ltr"><span class="z-1 relative whitespace-nowrap">Subscribe</span></p></button></div></div></form><div class="text-sm mb-10 text-textTertiary-onDark">By subscribing, you understand and agree that we will store, process and manage your personal information according to our<!-- --> <a class="underline relative" href="/about/our-organization/our-policies-terms/privacy-policy">Privacy Policy</a></div></div></div><div class="pr-4 md:pr-5 col-sm-1 col-xl-1"><div></div></div><div class="pr-4 md:pr-5 col-sm-12 border-t-thin border-textTertiary-onDark pt-10 xl:border-none xl:pt-0 col-xl-3"><div class="flex flex-row pb-10 xl:flex-col xl:pl-4"><div class="ted-grid -mr-4 flex flex-wrap md:-mr-5"><div class="pr-4 md:pr-5 col-sm-12 col-md-6 col-xl-12"><div class="pr-15 md:mr-5"></div></div><div class="pr-4 md:pr-5 col-sm-12 border-t-thin border-textTertiary-onDark w-full lg:w-auto pt-10 md:border-none md:pt-0 col-md-6 col-xl-12"><div><div class="text-sm bold mb-6 font-bold text-white">Follow TED</div><div class="mb-10 flex flex-row gap-5"><a id="" rel="noreferrer noopener" href="https://www.linkedin.com/company/ted-conferences/" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="none" aria-label="LinkedIn" class="h-8"><g clip-path="url(#linkedin-circle-white_svg__a)"><path fill="#F6F6F6" fill-rule="evenodd" d="M16 0C7.163 0 0 7.163 0 16s7.163 16 16 16 16-7.163 16-16S24.837 0 16 0M9.496 11.765c1.265 0 2.052-.837 2.052-1.883C11.524 8.814 10.76 8 9.52 8 8.28 8 7.47 8.814 7.47 9.882c0 1.046.787 1.883 2.004 1.883zm1.813 12.381V13.253H7.683v10.894zm13.078-6.245v6.247h-3.626v-5.829c0-1.464-.524-2.463-1.837-2.463-1.002 0-1.598.674-1.86 1.324-.096.233-.12.558-.12.884v6.084h-3.626s.048-9.872 0-10.894h3.626v1.543c.481-.742 1.343-1.8 3.268-1.8 2.386 0 4.175 1.558 4.175 4.904" clip-rule="evenodd"></path></g><defs><clipPath id="linkedin-circle-white_svg__a"><path fill="#fff" d="M0 0h32v32H0z"></path></clipPath></defs></svg></a><a id="" rel="noreferrer noopener" href="https://twitter.com/tedtalks" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="none" aria-label="Twitter" class="h-8"><g clip-path="url(#twitter-circle-white_svg__a)"><path fill="#F6F6F6" fill-rule="evenodd" d="M16 0C7.163 0 0 7.163 0 16s7.163 16 16 16 16-7.163 16-16S24.837 0 16 0m.888 13.292-.034-.554c-.1-1.435.784-2.745 2.183-3.254.514-.18 1.387-.203 1.958-.045.224.068.65.294.951.497l.549.373.604-.192c.336-.102.784-.271.985-.384.19-.102.358-.158.358-.124 0 .192-.414.847-.76 1.209-.471.508-.337.553.615.214.57-.192.582-.192.47.023-.067.113-.414.508-.784.87-.626.621-.66.689-.66 1.209 0 .802-.38 2.474-.761 3.39-.705 1.717-2.216 3.49-3.727 4.383-2.126 1.254-4.958 1.57-7.342.836-.794-.248-2.16-.88-2.16-.994 0-.034.414-.079.918-.09a6.3 6.3 0 0 0 3-.836l.604-.362-.694-.237c-.985-.34-1.87-1.119-2.093-1.853-.067-.237-.045-.249.582-.249l.65-.011-.55-.26c-.648-.328-1.242-.881-1.533-1.446-.212-.407-.48-1.435-.402-1.514.022-.034.257.034.526.124.772.283.873.215.425-.26-.84-.859-1.097-2.135-.694-3.344l.19-.543.739.735c1.51 1.48 3.29 2.361 5.327 2.621z" clip-rule="evenodd"></path></g><defs><clipPath id="twitter-circle-white_svg__a"><path fill="#fff" d="M0 0h32v32H0z"></path></clipPath></defs></svg></a><a id="" rel="noreferrer noopener" href="https://instagram.com/ted" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="none" aria-label="Instagram" class="h-8"><g fill="#F6F6F6" fill-rule="evenodd" clip-path="url(#instagram-circle-white_svg__a)" clip-rule="evenodd"><path d="M16 0C7.163 0 0 7.163 0 16s7.163 16 16 16 16-7.163 16-16S24.837 0 16 0m-3.518 7.518c.91-.041 1.201-.051 3.519-.051h-.003c2.318 0 2.608.01 3.518.051.909.042 1.53.186 2.073.397a4.2 4.2 0 0 1 1.511.984c.475.474.767.95.985 1.511.21.543.354 1.163.397 2.072.04.91.051 1.2.051 3.518s-.01 2.608-.051 3.518c-.043.908-.187 1.528-.397 2.071a4.2 4.2 0 0 1-.985 1.512c-.474.474-.95.767-1.51.985-.543.21-1.164.354-2.072.396-.91.041-1.2.052-3.518.052s-2.608-.01-3.518-.052-1.53-.185-2.072-.396a4.2 4.2 0 0 1-1.511-.985 4.2 4.2 0 0 1-.984-1.512c-.211-.543-.355-1.163-.397-2.071-.041-.91-.051-1.2-.051-3.518s.01-2.608.051-3.519c.04-.908.185-1.528.396-2.07.22-.562.51-1.038.985-1.512.475-.475.95-.766 1.512-.984.543-.211 1.163-.355 2.071-.397"></path><path d="M15.236 9.004H16c2.278 0 2.548.009 3.448.05.832.038 1.284.177 1.585.293.398.155.682.34.98.639.299.298.484.583.639.981.117.3.256.752.294 1.584.04.9.05 1.17.05 3.447s-.01 2.548-.05 3.447c-.038.832-.177 1.284-.294 1.584-.155.398-.34.682-.639.98a2.64 2.64 0 0 1-.98.639c-.3.117-.753.256-1.585.294-.9.04-1.17.05-3.448.05s-2.549-.01-3.448-.05c-.832-.039-1.284-.178-1.585-.294a2.64 2.64 0 0 1-.981-.639 2.65 2.65 0 0 1-.639-.98c-.116-.3-.256-.752-.294-1.584-.04-.9-.049-1.17-.049-3.449s.009-2.548.05-3.447c.037-.832.177-1.284.293-1.585.155-.398.34-.682.639-.98.299-.3.583-.484.981-.64.301-.117.753-.255 1.585-.294.787-.035 1.092-.046 2.682-.048zm5.32 1.417a1.024 1.024 0 1 0 0 2.049 1.024 1.024 0 0 0 0-2.049m-8.937 5.58a4.382 4.382 0 1 1 8.765 0 4.382 4.382 0 0 1-8.765 0"></path><path d="M16 13.156a2.844 2.844 0 1 1 0 5.689 2.844 2.844 0 0 1 0-5.69"></path></g><defs><clipPath id="instagram-circle-white_svg__a"><path fill="#fff" d="M0 0h32v32H0z"></path></clipPath></defs></svg></a><a id="" rel="noreferrer noopener" href="https://www.youtube.com/ted" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="none" aria-label="YouTube" class="h-8"><g fill="#F6F6F6" fill-rule="evenodd" clip-path="url(#youtube-circle-white_svg__a)" clip-rule="evenodd"><path d="M16 0C7.163 0 0 7.163 0 16s7.163 16 16 16 16-7.163 16-16S24.837 0 16 0m6.668 10.5a2.17 2.17 0 0 1 1.509 1.549c.357 1.366.357 4.218.357 4.218s0 2.85-.357 4.218a2.17 2.17 0 0 1-1.51 1.549C21.338 22.4 16 22.4 16 22.4s-5.337 0-6.668-.366a2.17 2.17 0 0 1-1.509-1.55c-.356-1.366-.356-4.217-.356-4.217s0-2.852.356-4.218a2.17 2.17 0 0 1 1.51-1.55c1.33-.366 6.667-.366 6.667-.366s5.337 0 6.668.367"></path><path d="M14.4 19.2v-5.333l4.267 2.666z"></path></g><defs><clipPath id="youtube-circle-white_svg__a"><path fill="#fff" d="M0 0h32v32H0z"></path></clipPath></defs></svg></a><a id="" rel="noreferrer noopener" href="https://www.facebook.com/TED" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="none" aria-label="Facebook" class="h-8"><g clip-path="url(#facebook-circle-white_svg__a)"><path fill="#F6F6F6" fill-rule="evenodd" d="M16 0C7.163 0 0 7.163 0 16s7.163 16 16 16 16-7.163 16-16S24.837 0 16 0m1.668 16.703v8.705h-3.602v-8.705h-1.8v-3h1.8v-1.8C14.066 9.454 15.082 8 17.97 8h2.403v3H18.87c-1.124 0-1.198.42-1.198 1.202l-.004 1.501h2.721l-.318 3z" clip-rule="evenodd"></path></g><defs><clipPath id="facebook-circle-white_svg__a"><path fill="#fff" d="M0 0h32v32H0z"></path></clipPath></defs></svg></a></div><div class="flex-col sm:hidden md:flex"><div><div class="text-sm bold mb-6 font-bold text-white">Download the TED App</div><div class="flex flex-row gap-5"><a id="" rel="noreferrer noopener" href="https://play.google.com/store/apps/details?id=com.ted.android&amp;utm_medium=email&amp;utm_source=ted_talks_daily_newsletter&amp;utm_campaign=daily&amp;user_email_address=f325af12e52a26ff58bb171ffc448307" data-mixpanel-context="download_google-play" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="120" height="40" fill="none" aria-label="Google Play" class="h-11 drop-shadow-sm"><g clip-path="url(#google-play-icon_svg__a)"><path fill="#A6A6A6" d="M110.135 0H9.535Q8.986 0 8.44.002q-.457.004-.919.013-1.008.012-2.004.177a6.7 6.7 0 0 0-1.9.627A6.44 6.44 0 0 0 .193 5.522a13 13 0 0 0-.179 2.002c-.01.306-.01.614-.015.92V31.56c.005.31.006.611.015.922q.012 1.007.18 2.002c.11.663.32 1.305.624 1.904.303.598.701 1.143 1.179 1.614.473.477 1.019.875 1.618 1.179a6.7 6.7 0 0 0 1.901.63q.996.165 2.004.177c.31.007.613.011.919.011.366.002.728.002 1.095.002h100.6c.359 0 .724 0 1.084-.002.304 0 .617-.004.922-.01a13 13 0 0 0 2-.177 6.8 6.8 0 0 0 1.908-.631A6.3 6.3 0 0 0 117.666 38a6.4 6.4 0 0 0 1.182-1.614c.302-.6.51-1.242.619-1.904q.167-.994.185-2.002c.004-.311.004-.612.004-.922.008-.364.008-.725.008-1.094V9.536q.002-.549-.008-1.092.001-.46-.004-.92a13.6 13.6 0 0 0-.185-2.002 6.7 6.7 0 0 0-.619-1.904 6.47 6.47 0 0 0-2.799-2.8 6.8 6.8 0 0 0-1.908-.627q-.993-.165-2-.176c-.305-.005-.618-.011-.922-.013-.36-.002-.725-.002-1.084-.002"></path><path fill="#000" d="M8.445 39.125c-.305 0-.602-.004-.904-.01a13 13 0 0 1-1.87-.164 5.9 5.9 0 0 1-1.656-.548 5.4 5.4 0 0 1-1.397-1.016 5.3 5.3 0 0 1-1.02-1.397 5.7 5.7 0 0 1-.544-1.657 12.4 12.4 0 0 1-.166-1.875c-.007-.21-.015-.913-.015-.913v-23.1s.009-.692.015-.895a12.4 12.4 0 0 1 .165-1.872 5.8 5.8 0 0 1 .544-1.662c.26-.518.603-.99 1.015-1.398A5.57 5.57 0 0 1 5.668 1.05Q6.6.9 7.543.887l.902-.012h102.769l.913.013a12.4 12.4 0 0 1 1.858.162 6 6 0 0 1 1.671.548 5.6 5.6 0 0 1 2.415 2.42c.261.52.441 1.076.536 1.649q.155.937.173 1.887c.003.283.003.588.003.89.008.375.008.732.008 1.092v20.929c0 .363 0 .718-.008 1.075 0 .325 0 .623-.004.93q-.017.932-.171 1.853a5.7 5.7 0 0 1-.54 1.67 5.5 5.5 0 0 1-1.015 1.386 5.4 5.4 0 0 1-1.4 1.022 5.9 5.9 0 0 1-1.668.55q-.928.152-1.869.163c-.293.007-.599.011-.897.011l-1.084.002z"></path><g clip-path="url(#google-play-icon_svg__b)"><path fill="#6C6C6C" d="M18.811 20.207 9.091 30.52a2.63 2.63 0 0 0 3.87 1.58l10.952-6.307z"></path><path fill="#C6C6C6" d="m28.658 18.455-4.727-2.738-5.312 4.728 5.34 5.339 4.7-2.738a2.62 2.62 0 0 0 0-4.618z"></path><path fill="#7F7F7F" d="M9.091 10.944q-.087.331-.091.675v18.253q.003.343.091.675l10.04-10.039z"></path><path fill="#939393" d="m18.875 20.737 5.038-5.03L12.96 9.375a2.628 2.628 0 0 0-3.888 1.57z"></path></g><path fill="#fff" d="M60.037 22.162a4 4 0 0 0-2.172.64 3.8 3.8 0 0 0-1.436 1.695 3.64 3.64 0 0 0-.217 2.178c.152.73.527 1.401 1.075 1.927a3.96 3.96 0 0 0 2.005 1.027 4.05 4.05 0 0 0 2.26-.22 3.9 3.9 0 0 0 1.752-1.392 3.67 3.67 0 0 0 .654-2.095 3.6 3.6 0 0 0-.284-1.451 3.7 3.7 0 0 0-.849-1.23 3.9 3.9 0 0 0-1.28-.812 4 4 0 0 0-1.508-.267m0 6.043a2.44 2.44 0 0 1-1.358-.305c-.41-.23-.74-.575-.945-.988a2.2 2.2 0 0 1-.204-1.33 2.25 2.25 0 0 1 .607-1.209 2.4 2.4 0 0 1 1.209-.67 2.45 2.45 0 0 1 1.39.1c.443.167.823.458 1.091.836.269.378.413.825.414 1.283.015.29-.031.58-.135.854a2.2 2.2 0 0 1-.467.735 2.3 2.3 0 0 1-.728.503 2.3 2.3 0 0 1-.874.19m-8.558-6.043a4 4 0 0 0-2.171.64 3.8 3.8 0 0 0-1.437 1.695 3.64 3.64 0 0 0-.217 2.178c.153.73.527 1.401 1.075 1.927a3.96 3.96 0 0 0 2.005 1.027 4.05 4.05 0 0 0 2.26-.22 3.9 3.9 0 0 0 1.752-1.392 3.67 3.67 0 0 0 .654-2.095 3.6 3.6 0 0 0-.283-1.451 3.7 3.7 0 0 0-.85-1.23 3.9 3.9 0 0 0-1.28-.812 4 4 0 0 0-1.508-.267m0 6.043a2.44 2.44 0 0 1-1.358-.305c-.41-.23-.74-.575-.945-.988a2.2 2.2 0 0 1-.204-1.33 2.25 2.25 0 0 1 .608-1.209 2.4 2.4 0 0 1 1.208-.67 2.45 2.45 0 0 1 1.39.1c.443.167.823.458 1.091.836.269.378.413.825.414 1.283.015.29-.03.58-.134.854a2.2 2.2 0 0 1-.468.735 2.3 2.3 0 0 1-.728.503 2.3 2.3 0 0 1-.874.19M41.306 23.32v1.593h3.966a3.28 3.28 0 0 1-.918 2.008c-.398.39-.876.696-1.405.898a4.2 4.2 0 0 1-1.662.27 4.5 4.5 0 0 1-3.116-1.244 4.17 4.17 0 0 1-1.291-3.003 4.17 4.17 0 0 1 1.29-3.002 4.5 4.5 0 0 1 3.117-1.244 4.35 4.35 0 0 1 3.012 1.141l1.166-1.123a5.8 5.8 0 0 0-1.911-1.21 6 6 0 0 0-2.248-.4 6.3 6.3 0 0 0-2.395.377c-.763.28-1.46.705-2.049 1.251a5.85 5.85 0 0 0-1.377 1.925 5.66 5.66 0 0 0 0 4.58c.32.724.788 1.379 1.377 1.924a6.1 6.1 0 0 0 2.05 1.251c.763.28 1.577.409 2.394.378.785.03 1.568-.1 2.298-.382a5.6 5.6 0 0 0 1.934-1.255 5.2 5.2 0 0 0 1.433-3.742 5.4 5.4 0 0 0-.083-.99zM82.91 24.56a3.55 3.55 0 0 0-1.284-1.7 3.76 3.76 0 0 0-2.058-.698c-.503 0-1 .098-1.461.29a3.7 3.7 0 0 0-1.221.824 3.5 3.5 0 0 0-.78 1.224c-.17.455-.243.94-.211 1.422a3.65 3.65 0 0 0 .751 2.248 3.9 3.9 0 0 0 1.998 1.366 4.02 4.02 0 0 0 2.45-.058 3.87 3.87 0 0 0 1.926-1.459l-1.331-.885c-.199.319-.48.582-.817.764a2.3 2.3 0 0 1-1.102.271 2.03 2.03 0 0 1-1.13-.288 1.92 1.92 0 0 1-.762-.853l5.225-2.079zm-5.326 1.256a2 2 0 0 1 .114-.81c.093-.26.24-.5.43-.706s.422-.371.68-.49c.258-.117.538-.184.824-.197.296-.017.59.05.847.193.256.143.463.356.594.612zm-4.242 3.645h1.717V18.403h-1.717zm-2.81-6.458h-.064a2.7 2.7 0 0 0-.928-.66 2.8 2.8 0 0 0-1.129-.225 3.98 3.98 0 0 0-2.642 1.165 3.7 3.7 0 0 0-1.08 2.6c0 .967.387 1.898 1.08 2.599.693.7 1.639 1.118 2.642 1.165.39.007.776-.068 1.131-.221a2.6 2.6 0 0 0 .926-.664h.064v.54c0 1.442-.798 2.211-2.084 2.211-.43-.009-.846-.14-1.198-.378a2.1 2.1 0 0 1-.776-.957l-1.488.592a3.57 3.57 0 0 0 1.372 1.633 3.78 3.78 0 0 0 2.09.597c2.01 0 3.673-1.142 3.673-3.92v-6.696h-1.589zm-1.974 5.202a2.4 2.4 0 0 1-1.579-.715 2.24 2.24 0 0 1-.642-1.563c0-.581.23-1.14.642-1.563a2.4 2.4 0 0 1 1.58-.715c.293.012.582.081.847.203.266.122.504.294.7.506s.344.46.438.729c.093.268.13.553.108.835.025.284-.01.57-.103.84s-.242.52-.437.733a2.2 2.2 0 0 1-.701.508c-.268.122-.558.19-.853.202m22.396-9.802H86.84V29.46h1.717v-4.193h2.397c.489.034.98-.03 1.443-.187a3.6 3.6 0 0 0 1.246-.727c.36-.322.646-.712.841-1.145a3.33 3.33 0 0 0 0-2.748 3.4 3.4 0 0 0-.841-1.145 3.6 3.6 0 0 0-1.246-.727 3.7 3.7 0 0 0-1.443-.186m0 5.308h-2.397v-3.77h2.433c.258 0 .514.05.752.145.239.095.455.235.638.41.182.176.327.385.426.615a1.83 1.83 0 0 1-.426 2.063 2 2 0 0 1-.638.41 2 2 0 0 1-.752.144zm10.596-1.584a3.3 3.3 0 0 0-1.792.403 3.14 3.14 0 0 0-1.266 1.287l1.515.61c.153-.266.383-.483.662-.626.278-.143.594-.205.908-.179.218-.024.439-.007.65.052s.408.157.578.29a1.563 1.563 0 0 1 .609 1.083v.106a3.96 3.96 0 0 0-1.791-.425c-1.634 0-3.305.885-3.305 2.495.016.342.103.678.257.987.153.309.37.585.636.812s.577.4.914.509.694.151 1.048.125c.431.023.861-.065 1.245-.255a2.4 2.4 0 0 0 .941-.825h.055v.885h1.653v-4.255c0-1.938-1.525-3.061-3.48-3.061zm-.211 6.06c-.56 0-1.341-.274-1.341-.938 0-.884.973-1.185 1.836-1.185a3.14 3.14 0 0 1 1.561.371c-.057.48-.294.925-.665 1.25a2.12 2.12 0 0 1-1.355.52zm9.751-5.803-1.965 4.794h-.055l-2.038-4.794h-1.837l3.058 6.705-1.745 3.725h1.791L113 22.384zM95.664 29.46h1.708V18.403h-1.708zM37.893 10.871h1.627a1.972 1.972 0 1 1-.451-1.582l.398-.455a2.571 2.571 0 1 0 .659 2.038h.017v-.6h-2.25z"></path><path fill="#fff" fill-rule="evenodd" d="M60.516 10.575q0 .503.161.97.17.462.498.82.33.358.82.571.491.207 1.143.207.651 0 1.142-.207.49-.213.82-.571a2.3 2.3 0 0 0 .49-.82 2.84 2.84 0 0 0 0-1.934 2.3 2.3 0 0 0-.49-.826 2.4 2.4 0 0 0-.82-.572A2.8 2.8 0 0 0 63.138 8q-.652 0-1.142.213-.491.214-.82.572-.33.358-.499.826a2.9 2.9 0 0 0-.16.964m.728 0q0-.372.108-.73.107-.365.337-.654t.59-.461q.36-.18.859-.18.498 0 .858.18.36.172.59.461t.338.654a2.53 2.53 0 0 1 0 1.467 1.9 1.9 0 0 1-.338.647q-.23.289-.59.468-.36.172-.858.172t-.859-.172a1.7 1.7 0 0 1-.59-.468 1.9 1.9 0 0 1-.337-.647 2.6 2.6 0 0 1-.108-.737" clip-rule="evenodd"></path><path fill="#fff" d="M41.084 13.033V8.117h3.78v.55h-3.051v1.564h2.843v.55h-2.843v1.7h3.073v.552zM47.012 13.033V8.668h-1.825v-.551h4.377v.55H47.74v4.366zM52.335 8.117v4.916h.728V8.117zM55.53 13.033V8.668h-1.824v-.551h4.376v.55h-1.824v4.366zM66.639 13.033V8.117h.774l2.882 3.986h.015V8.117H71v4.916h-.797l-2.86-3.945h-.014v3.945z"></path></g><defs><clipPath id="google-play-icon_svg__a"><path fill="#fff" d="M0 0h119.664v40H0z"></path></clipPath><clipPath id="google-play-icon_svg__b"><path fill="#fff" d="M9 9h21v23.473H9z"></path></clipPath></defs></svg></a><a id="" rel="noreferrer noopener" href="https://apps.apple.com/app/apple-store/id376183339?user_email_address=f325af12e52a26ff58bb171ffc448307&amp;utm_campaign=daily&amp;utm_medium=email&amp;utm_source=ted_talks_daily_newsletter" data-mixpanel-context="download_apple-store" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="119.664" height="40" aria-label="Apple Store" class="h-11 drop-shadow-sm"><path d="M110.135 0H9.535Q8.986 0 8.44.002q-.457.004-.919.013A13 13 0 0 0 5.517.19a6.7 6.7 0 0 0-1.9.627 6.4 6.4 0 0 0-1.62 1.18A6.3 6.3 0 0 0 .82 3.617a6.6 6.6 0 0 0-.625 1.903 13 13 0 0 0-.179 2.002c-.01.307-.01.615-.015.921V31.56c.005.31.006.61.015.921a13 13 0 0 0 .18 2.002 6.6 6.6 0 0 0 .624 1.905A6.2 6.2 0 0 0 1.998 38a6.3 6.3 0 0 0 1.618 1.179 6.7 6.7 0 0 0 1.901.63 13.5 13.5 0 0 0 2.004.177c.31.007.613.011.919.011.366.002.728.002 1.095.002h100.6c.36 0 .724 0 1.084-.002.304 0 .617-.004.922-.01a13 13 0 0 0 2-.178 6.8 6.8 0 0 0 1.908-.63A6.3 6.3 0 0 0 117.666 38a6.4 6.4 0 0 0 1.182-1.614 6.6 6.6 0 0 0 .619-1.905 13.5 13.5 0 0 0 .185-2.002c.004-.31.004-.61.004-.921.008-.364.008-.725.008-1.094V9.536q.002-.549-.008-1.092.001-.46-.004-.92a13.5 13.5 0 0 0-.185-2.003 6.6 6.6 0 0 0-.62-1.903 6.47 6.47 0 0 0-2.798-2.8 6.8 6.8 0 0 0-1.908-.627 13 13 0 0 0-2-.176c-.305-.005-.618-.011-.922-.013-.36-.002-.725-.002-1.084-.002Z" style="fill:#a6a6a6"></path><path d="M8.445 39.125c-.305 0-.602-.004-.904-.01a13 13 0 0 1-1.87-.164 5.9 5.9 0 0 1-1.656-.548 5.4 5.4 0 0 1-1.397-1.016 5.3 5.3 0 0 1-1.02-1.397 5.7 5.7 0 0 1-.544-1.657 12.4 12.4 0 0 1-.166-1.875c-.007-.21-.015-.913-.015-.913v-23.1s.009-.692.015-.895a12.4 12.4 0 0 1 .165-1.872 5.8 5.8 0 0 1 .544-1.662 5.4 5.4 0 0 1 1.015-1.398 5.6 5.6 0 0 1 1.402-1.023 5.8 5.8 0 0 1 1.653-.544A12.6 12.6 0 0 1 7.543.887l.902-.012h102.769l.913.013a12.4 12.4 0 0 1 1.858.162 6 6 0 0 1 1.671.548 5.6 5.6 0 0 1 2.415 2.42 5.8 5.8 0 0 1 .535 1.649 13 13 0 0 1 .174 1.887c.003.283.003.588.003.89.008.375.008.732.008 1.092v20.929c0 .363 0 .718-.008 1.075 0 .325 0 .623-.004.93a13 13 0 0 1-.17 1.853 5.7 5.7 0 0 1-.54 1.67 5.5 5.5 0 0 1-1.016 1.386 5.4 5.4 0 0 1-1.4 1.022 5.9 5.9 0 0 1-1.668.55 12.5 12.5 0 0 1-1.869.163c-.293.007-.6.011-.897.011l-1.084.002Z"></path><g data-name="&lt;Group&gt;"><g data-name="&lt;Group&gt;"><path d="M24.769 20.3a4.95 4.95 0 0 1 2.356-4.151 5.07 5.07 0 0 0-3.99-2.158c-1.68-.176-3.308 1.005-4.164 1.005-.872 0-2.19-.988-3.608-.958a5.32 5.32 0 0 0-4.473 2.728c-1.934 3.348-.491 8.269 1.361 10.976.927 1.325 2.01 2.805 3.428 2.753 1.387-.058 1.905-.885 3.58-.885 1.658 0 2.144.885 3.59.852 1.489-.025 2.426-1.332 3.32-2.67a11 11 0 0 0 1.52-3.092 4.78 4.78 0 0 1-2.92-4.4M22.037 12.21a4.87 4.87 0 0 0 1.115-3.49 4.96 4.96 0 0 0-3.208 1.66A4.64 4.64 0 0 0 18.8 13.74a4.1 4.1 0 0 0 3.237-1.53" data-name="&lt;Path&gt;" style="fill:#fff"></path></g><path d="M42.302 27.14H37.57l-1.137 3.356h-2.005l4.484-12.418h2.083l4.483 12.418h-2.039Zm-4.243-1.55h3.752l-1.85-5.446h-.051ZM55.16 25.97c0 2.813-1.506 4.62-3.779 4.62a3.07 3.07 0 0 1-2.848-1.583h-.043v4.484H46.63V21.442h1.8v1.506h.033a3.21 3.21 0 0 1 2.883-1.6c2.298 0 3.813 1.816 3.813 4.622m-1.91 0c0-1.833-.948-3.038-2.393-3.038-1.42 0-2.375 1.23-2.375 3.038 0 1.824.955 3.046 2.375 3.046 1.445 0 2.393-1.197 2.393-3.046M65.125 25.97c0 2.813-1.506 4.62-3.779 4.62a3.07 3.07 0 0 1-2.848-1.583h-.043v4.484h-1.859V21.442h1.799v1.506h.034a3.21 3.21 0 0 1 2.883-1.6c2.298 0 3.813 1.816 3.813 4.622m-1.91 0c0-1.833-.948-3.038-2.393-3.038-1.42 0-2.375 1.23-2.375 3.038 0 1.824.955 3.046 2.375 3.046 1.445 0 2.392-1.197 2.392-3.046M71.71 27.036c.138 1.232 1.334 2.04 2.97 2.04 1.566 0 2.693-.808 2.693-1.919 0-.964-.68-1.54-2.29-1.936l-1.609-.388c-2.28-.55-3.339-1.617-3.339-3.348 0-2.142 1.867-3.614 4.519-3.614 2.624 0 4.423 1.472 4.483 3.614h-1.876c-.112-1.239-1.136-1.987-2.634-1.987s-2.521.757-2.521 1.858c0 .878.654 1.395 2.255 1.79l1.368.336c2.548.603 3.606 1.626 3.606 3.443 0 2.323-1.85 3.778-4.793 3.778-2.754 0-4.614-1.42-4.734-3.667ZM83.346 19.3v2.142h1.722v1.472h-1.722v4.991c0 .776.345 1.137 1.102 1.137a6 6 0 0 0 .611-.043v1.463a5 5 0 0 1-1.032.086c-1.833 0-2.548-.689-2.548-2.445v-5.189h-1.316v-1.472h1.316V19.3ZM86.065 25.97c0-2.849 1.678-4.639 4.294-4.639 2.625 0 4.295 1.79 4.295 4.639 0 2.856-1.661 4.638-4.295 4.638s-4.294-1.782-4.294-4.638m6.695 0c0-1.954-.895-3.108-2.401-3.108s-2.4 1.162-2.4 3.108c0 1.962.894 3.106 2.4 3.106s2.401-1.144 2.401-3.106M96.186 21.442h1.773v1.541h.043a2.16 2.16 0 0 1 2.177-1.635 3 3 0 0 1 .637.069v1.738a2.6 2.6 0 0 0-.835-.112 1.873 1.873 0 0 0-1.937 2.083v5.37h-1.858ZM109.384 27.837c-.25 1.643-1.85 2.771-3.898 2.771-2.634 0-4.269-1.764-4.269-4.595 0-2.84 1.644-4.682 4.19-4.682 2.506 0 4.08 1.72 4.08 4.466v.637h-6.394v.112a2.358 2.358 0 0 0 2.436 2.564 2.05 2.05 0 0 0 2.09-1.273Zm-6.282-2.702h4.526a2.177 2.177 0 0 0-2.22-2.298 2.29 2.29 0 0 0-2.306 2.298" style="fill:#fff"></path></g><g data-name="&lt;Group&gt;"><path d="M37.826 8.731a2.64 2.64 0 0 1 2.808 2.965c0 1.906-1.03 3.002-2.808 3.002h-2.155V8.73Zm-1.228 5.123h1.125a1.876 1.876 0 0 0 1.967-2.146 1.88 1.88 0 0 0-1.967-2.134h-1.125ZM41.68 12.444a2.133 2.133 0 1 1 4.248 0 2.134 2.134 0 1 1-4.247 0m3.334 0c0-.976-.439-1.547-1.208-1.547-.773 0-1.207.571-1.207 1.547 0 .984.434 1.55 1.207 1.55.77 0 1.208-.57 1.208-1.55M51.573 14.698h-.922l-.93-3.317h-.07l-.927 3.317h-.913l-1.242-4.503h.902l.806 3.436h.067l.926-3.436h.852l.926 3.436h.07l.803-3.436h.889ZM53.854 10.195h.855v.715h.066a1.35 1.35 0 0 1 1.344-.802 1.465 1.465 0 0 1 1.559 1.675v2.915h-.889v-2.692c0-.724-.314-1.084-.972-1.084a1.033 1.033 0 0 0-1.075 1.141v2.635h-.888ZM59.094 8.437h.888v6.26h-.888ZM61.218 12.444a2.133 2.133 0 1 1 4.247 0 2.134 2.134 0 1 1-4.247 0m3.333 0c0-.976-.439-1.547-1.208-1.547-.773 0-1.207.571-1.207 1.547 0 .984.434 1.55 1.207 1.55.77 0 1.208-.57 1.208-1.55M66.4 13.424c0-.81.604-1.278 1.676-1.344l1.22-.07v-.389c0-.475-.315-.744-.922-.744-.497 0-.84.182-.939.5h-.86c.09-.773.818-1.27 1.84-1.27 1.128 0 1.765.563 1.765 1.514v3.077h-.855v-.633h-.07a1.52 1.52 0 0 1-1.353.707 1.36 1.36 0 0 1-1.501-1.348m2.895-.384v-.377l-1.1.07c-.62.042-.9.253-.9.65 0 .405.351.64.834.64a1.06 1.06 0 0 0 1.166-.983M71.348 12.444c0-1.423.732-2.324 1.87-2.324a1.48 1.48 0 0 1 1.38.79h.067V8.437h.888v6.26h-.851v-.71h-.07a1.56 1.56 0 0 1-1.415.785c-1.145 0-1.869-.901-1.869-2.328m.918 0c0 .955.45 1.53 1.203 1.53.75 0 1.212-.583 1.212-1.526 0-.938-.468-1.53-1.212-1.53-.748 0-1.203.58-1.203 1.526M79.23 12.444a2.133 2.133 0 1 1 4.247 0 2.134 2.134 0 1 1-4.247 0m3.333 0c0-.976-.438-1.547-1.208-1.547-.772 0-1.207.571-1.207 1.547 0 .984.435 1.55 1.207 1.55.77 0 1.208-.57 1.208-1.55M84.67 10.195h.855v.715h.066a1.35 1.35 0 0 1 1.344-.802 1.465 1.465 0 0 1 1.559 1.675v2.915h-.889v-2.692c0-.724-.314-1.084-.972-1.084a1.033 1.033 0 0 0-1.075 1.141v2.635h-.889ZM93.515 9.074v1.141h.976v.749h-.976v2.315c0 .472.194.679.637.679a3 3 0 0 0 .339-.021v.74a3 3 0 0 1-.484.046c-.988 0-1.381-.348-1.381-1.216v-2.543h-.715v-.749h.715V9.074ZM95.705 8.437h.88v2.481h.07a1.39 1.39 0 0 1 1.374-.806 1.483 1.483 0 0 1 1.55 1.679v2.907h-.889V12.01c0-.72-.335-1.084-.963-1.084a1.052 1.052 0 0 0-1.134 1.142v2.63h-.888ZM104.761 13.482a1.83 1.83 0 0 1-1.95 1.303 2.045 2.045 0 0 1-2.081-2.325 2.077 2.077 0 0 1 2.076-2.352c1.253 0 2.009.856 2.009 2.27v.31h-3.18v.05a1.19 1.19 0 0 0 1.2 1.29 1.08 1.08 0 0 0 1.07-.546Zm-3.126-1.451h2.275a1.086 1.086 0 0 0-1.109-1.167 1.15 1.15 0 0 0-1.166 1.167" style="fill:#fff"></path></g></svg></a></div></div></div></div></div><div class="pr-4 md:pr-5 col-sm-12 sm:border-t-thin border-textTertiary-onDark w-full lg:w-auto sm:pt-10 md:border-none md:pt-0 md:hidden"><div><div><div class="text-sm bold mb-6 font-bold text-white">Download the TED App</div><div class="flex flex-row gap-5"><a id="" rel="noreferrer noopener" href="https://play.google.com/store/apps/details?id=com.ted.android&amp;utm_medium=email&amp;utm_source=ted_talks_daily_newsletter&amp;utm_campaign=daily&amp;user_email_address=f325af12e52a26ff58bb171ffc448307" data-mixpanel-context="download_google-play" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="120" height="40" fill="none" aria-label="Google Play" class="h-11 drop-shadow-sm"><g clip-path="url(#google-play-icon_svg__a)"><path fill="#A6A6A6" d="M110.135 0H9.535Q8.986 0 8.44.002q-.457.004-.919.013-1.008.012-2.004.177a6.7 6.7 0 0 0-1.9.627A6.44 6.44 0 0 0 .193 5.522a13 13 0 0 0-.179 2.002c-.01.306-.01.614-.015.92V31.56c.005.31.006.611.015.922q.012 1.007.18 2.002c.11.663.32 1.305.624 1.904.303.598.701 1.143 1.179 1.614.473.477 1.019.875 1.618 1.179a6.7 6.7 0 0 0 1.901.63q.996.165 2.004.177c.31.007.613.011.919.011.366.002.728.002 1.095.002h100.6c.359 0 .724 0 1.084-.002.304 0 .617-.004.922-.01a13 13 0 0 0 2-.177 6.8 6.8 0 0 0 1.908-.631A6.3 6.3 0 0 0 117.666 38a6.4 6.4 0 0 0 1.182-1.614c.302-.6.51-1.242.619-1.904q.167-.994.185-2.002c.004-.311.004-.612.004-.922.008-.364.008-.725.008-1.094V9.536q.002-.549-.008-1.092.001-.46-.004-.92a13.6 13.6 0 0 0-.185-2.002 6.7 6.7 0 0 0-.619-1.904 6.47 6.47 0 0 0-2.799-2.8 6.8 6.8 0 0 0-1.908-.627q-.993-.165-2-.176c-.305-.005-.618-.011-.922-.013-.36-.002-.725-.002-1.084-.002"></path><path fill="#000" d="M8.445 39.125c-.305 0-.602-.004-.904-.01a13 13 0 0 1-1.87-.164 5.9 5.9 0 0 1-1.656-.548 5.4 5.4 0 0 1-1.397-1.016 5.3 5.3 0 0 1-1.02-1.397 5.7 5.7 0 0 1-.544-1.657 12.4 12.4 0 0 1-.166-1.875c-.007-.21-.015-.913-.015-.913v-23.1s.009-.692.015-.895a12.4 12.4 0 0 1 .165-1.872 5.8 5.8 0 0 1 .544-1.662c.26-.518.603-.99 1.015-1.398A5.57 5.57 0 0 1 5.668 1.05Q6.6.9 7.543.887l.902-.012h102.769l.913.013a12.4 12.4 0 0 1 1.858.162 6 6 0 0 1 1.671.548 5.6 5.6 0 0 1 2.415 2.42c.261.52.441 1.076.536 1.649q.155.937.173 1.887c.003.283.003.588.003.89.008.375.008.732.008 1.092v20.929c0 .363 0 .718-.008 1.075 0 .325 0 .623-.004.93q-.017.932-.171 1.853a5.7 5.7 0 0 1-.54 1.67 5.5 5.5 0 0 1-1.015 1.386 5.4 5.4 0 0 1-1.4 1.022 5.9 5.9 0 0 1-1.668.55q-.928.152-1.869.163c-.293.007-.599.011-.897.011l-1.084.002z"></path><g clip-path="url(#google-play-icon_svg__b)"><path fill="#6C6C6C" d="M18.811 20.207 9.091 30.52a2.63 2.63 0 0 0 3.87 1.58l10.952-6.307z"></path><path fill="#C6C6C6" d="m28.658 18.455-4.727-2.738-5.312 4.728 5.34 5.339 4.7-2.738a2.62 2.62 0 0 0 0-4.618z"></path><path fill="#7F7F7F" d="M9.091 10.944q-.087.331-.091.675v18.253q.003.343.091.675l10.04-10.039z"></path><path fill="#939393" d="m18.875 20.737 5.038-5.03L12.96 9.375a2.628 2.628 0 0 0-3.888 1.57z"></path></g><path fill="#fff" d="M60.037 22.162a4 4 0 0 0-2.172.64 3.8 3.8 0 0 0-1.436 1.695 3.64 3.64 0 0 0-.217 2.178c.152.73.527 1.401 1.075 1.927a3.96 3.96 0 0 0 2.005 1.027 4.05 4.05 0 0 0 2.26-.22 3.9 3.9 0 0 0 1.752-1.392 3.67 3.67 0 0 0 .654-2.095 3.6 3.6 0 0 0-.284-1.451 3.7 3.7 0 0 0-.849-1.23 3.9 3.9 0 0 0-1.28-.812 4 4 0 0 0-1.508-.267m0 6.043a2.44 2.44 0 0 1-1.358-.305c-.41-.23-.74-.575-.945-.988a2.2 2.2 0 0 1-.204-1.33 2.25 2.25 0 0 1 .607-1.209 2.4 2.4 0 0 1 1.209-.67 2.45 2.45 0 0 1 1.39.1c.443.167.823.458 1.091.836.269.378.413.825.414 1.283.015.29-.031.58-.135.854a2.2 2.2 0 0 1-.467.735 2.3 2.3 0 0 1-.728.503 2.3 2.3 0 0 1-.874.19m-8.558-6.043a4 4 0 0 0-2.171.64 3.8 3.8 0 0 0-1.437 1.695 3.64 3.64 0 0 0-.217 2.178c.153.73.527 1.401 1.075 1.927a3.96 3.96 0 0 0 2.005 1.027 4.05 4.05 0 0 0 2.26-.22 3.9 3.9 0 0 0 1.752-1.392 3.67 3.67 0 0 0 .654-2.095 3.6 3.6 0 0 0-.283-1.451 3.7 3.7 0 0 0-.85-1.23 3.9 3.9 0 0 0-1.28-.812 4 4 0 0 0-1.508-.267m0 6.043a2.44 2.44 0 0 1-1.358-.305c-.41-.23-.74-.575-.945-.988a2.2 2.2 0 0 1-.204-1.33 2.25 2.25 0 0 1 .608-1.209 2.4 2.4 0 0 1 1.208-.67 2.45 2.45 0 0 1 1.39.1c.443.167.823.458 1.091.836.269.378.413.825.414 1.283.015.29-.03.58-.134.854a2.2 2.2 0 0 1-.468.735 2.3 2.3 0 0 1-.728.503 2.3 2.3 0 0 1-.874.19M41.306 23.32v1.593h3.966a3.28 3.28 0 0 1-.918 2.008c-.398.39-.876.696-1.405.898a4.2 4.2 0 0 1-1.662.27 4.5 4.5 0 0 1-3.116-1.244 4.17 4.17 0 0 1-1.291-3.003 4.17 4.17 0 0 1 1.29-3.002 4.5 4.5 0 0 1 3.117-1.244 4.35 4.35 0 0 1 3.012 1.141l1.166-1.123a5.8 5.8 0 0 0-1.911-1.21 6 6 0 0 0-2.248-.4 6.3 6.3 0 0 0-2.395.377c-.763.28-1.46.705-2.049 1.251a5.85 5.85 0 0 0-1.377 1.925 5.66 5.66 0 0 0 0 4.58c.32.724.788 1.379 1.377 1.924a6.1 6.1 0 0 0 2.05 1.251c.763.28 1.577.409 2.394.378.785.03 1.568-.1 2.298-.382a5.6 5.6 0 0 0 1.934-1.255 5.2 5.2 0 0 0 1.433-3.742 5.4 5.4 0 0 0-.083-.99zM82.91 24.56a3.55 3.55 0 0 0-1.284-1.7 3.76 3.76 0 0 0-2.058-.698c-.503 0-1 .098-1.461.29a3.7 3.7 0 0 0-1.221.824 3.5 3.5 0 0 0-.78 1.224c-.17.455-.243.94-.211 1.422a3.65 3.65 0 0 0 .751 2.248 3.9 3.9 0 0 0 1.998 1.366 4.02 4.02 0 0 0 2.45-.058 3.87 3.87 0 0 0 1.926-1.459l-1.331-.885c-.199.319-.48.582-.817.764a2.3 2.3 0 0 1-1.102.271 2.03 2.03 0 0 1-1.13-.288 1.92 1.92 0 0 1-.762-.853l5.225-2.079zm-5.326 1.256a2 2 0 0 1 .114-.81c.093-.26.24-.5.43-.706s.422-.371.68-.49c.258-.117.538-.184.824-.197.296-.017.59.05.847.193.256.143.463.356.594.612zm-4.242 3.645h1.717V18.403h-1.717zm-2.81-6.458h-.064a2.7 2.7 0 0 0-.928-.66 2.8 2.8 0 0 0-1.129-.225 3.98 3.98 0 0 0-2.642 1.165 3.7 3.7 0 0 0-1.08 2.6c0 .967.387 1.898 1.08 2.599.693.7 1.639 1.118 2.642 1.165.39.007.776-.068 1.131-.221a2.6 2.6 0 0 0 .926-.664h.064v.54c0 1.442-.798 2.211-2.084 2.211-.43-.009-.846-.14-1.198-.378a2.1 2.1 0 0 1-.776-.957l-1.488.592a3.57 3.57 0 0 0 1.372 1.633 3.78 3.78 0 0 0 2.09.597c2.01 0 3.673-1.142 3.673-3.92v-6.696h-1.589zm-1.974 5.202a2.4 2.4 0 0 1-1.579-.715 2.24 2.24 0 0 1-.642-1.563c0-.581.23-1.14.642-1.563a2.4 2.4 0 0 1 1.58-.715c.293.012.582.081.847.203.266.122.504.294.7.506s.344.46.438.729c.093.268.13.553.108.835.025.284-.01.57-.103.84s-.242.52-.437.733a2.2 2.2 0 0 1-.701.508c-.268.122-.558.19-.853.202m22.396-9.802H86.84V29.46h1.717v-4.193h2.397c.489.034.98-.03 1.443-.187a3.6 3.6 0 0 0 1.246-.727c.36-.322.646-.712.841-1.145a3.33 3.33 0 0 0 0-2.748 3.4 3.4 0 0 0-.841-1.145 3.6 3.6 0 0 0-1.246-.727 3.7 3.7 0 0 0-1.443-.186m0 5.308h-2.397v-3.77h2.433c.258 0 .514.05.752.145.239.095.455.235.638.41.182.176.327.385.426.615a1.83 1.83 0 0 1-.426 2.063 2 2 0 0 1-.638.41 2 2 0 0 1-.752.144zm10.596-1.584a3.3 3.3 0 0 0-1.792.403 3.14 3.14 0 0 0-1.266 1.287l1.515.61c.153-.266.383-.483.662-.626.278-.143.594-.205.908-.179.218-.024.439-.007.65.052s.408.157.578.29a1.563 1.563 0 0 1 .609 1.083v.106a3.96 3.96 0 0 0-1.791-.425c-1.634 0-3.305.885-3.305 2.495.016.342.103.678.257.987.153.309.37.585.636.812s.577.4.914.509.694.151 1.048.125c.431.023.861-.065 1.245-.255a2.4 2.4 0 0 0 .941-.825h.055v.885h1.653v-4.255c0-1.938-1.525-3.061-3.48-3.061zm-.211 6.06c-.56 0-1.341-.274-1.341-.938 0-.884.973-1.185 1.836-1.185a3.14 3.14 0 0 1 1.561.371c-.057.48-.294.925-.665 1.25a2.12 2.12 0 0 1-1.355.52zm9.751-5.803-1.965 4.794h-.055l-2.038-4.794h-1.837l3.058 6.705-1.745 3.725h1.791L113 22.384zM95.664 29.46h1.708V18.403h-1.708zM37.893 10.871h1.627a1.972 1.972 0 1 1-.451-1.582l.398-.455a2.571 2.571 0 1 0 .659 2.038h.017v-.6h-2.25z"></path><path fill="#fff" fill-rule="evenodd" d="M60.516 10.575q0 .503.161.97.17.462.498.82.33.358.82.571.491.207 1.143.207.651 0 1.142-.207.49-.213.82-.571a2.3 2.3 0 0 0 .49-.82 2.84 2.84 0 0 0 0-1.934 2.3 2.3 0 0 0-.49-.826 2.4 2.4 0 0 0-.82-.572A2.8 2.8 0 0 0 63.138 8q-.652 0-1.142.213-.491.214-.82.572-.33.358-.499.826a2.9 2.9 0 0 0-.16.964m.728 0q0-.372.108-.73.107-.365.337-.654t.59-.461q.36-.18.859-.18.498 0 .858.18.36.172.59.461t.338.654a2.53 2.53 0 0 1 0 1.467 1.9 1.9 0 0 1-.338.647q-.23.289-.59.468-.36.172-.858.172t-.859-.172a1.7 1.7 0 0 1-.59-.468 1.9 1.9 0 0 1-.337-.647 2.6 2.6 0 0 1-.108-.737" clip-rule="evenodd"></path><path fill="#fff" d="M41.084 13.033V8.117h3.78v.55h-3.051v1.564h2.843v.55h-2.843v1.7h3.073v.552zM47.012 13.033V8.668h-1.825v-.551h4.377v.55H47.74v4.366zM52.335 8.117v4.916h.728V8.117zM55.53 13.033V8.668h-1.824v-.551h4.376v.55h-1.824v4.366zM66.639 13.033V8.117h.774l2.882 3.986h.015V8.117H71v4.916h-.797l-2.86-3.945h-.014v3.945z"></path></g><defs><clipPath id="google-play-icon_svg__a"><path fill="#fff" d="M0 0h119.664v40H0z"></path></clipPath><clipPath id="google-play-icon_svg__b"><path fill="#fff" d="M9 9h21v23.473H9z"></path></clipPath></defs></svg></a><a id="" rel="noreferrer noopener" href="https://apps.apple.com/app/apple-store/id376183339?user_email_address=f325af12e52a26ff58bb171ffc448307&amp;utm_campaign=daily&amp;utm_medium=email&amp;utm_source=ted_talks_daily_newsletter" data-mixpanel-context="download_apple-store" class="relative"><svg xmlns="http://www.w3.org/2000/svg" width="119.664" height="40" aria-label="Apple Store" class="h-11 drop-shadow-sm"><path d="M110.135 0H9.535Q8.986 0 8.44.002q-.457.004-.919.013A13 13 0 0 0 5.517.19a6.7 6.7 0 0 0-1.9.627 6.4 6.4 0 0 0-1.62 1.18A6.3 6.3 0 0 0 .82 3.617a6.6 6.6 0 0 0-.625 1.903 13 13 0 0 0-.179 2.002c-.01.307-.01.615-.015.921V31.56c.005.31.006.61.015.921a13 13 0 0 0 .18 2.002 6.6 6.6 0 0 0 .624 1.905A6.2 6.2 0 0 0 1.998 38a6.3 6.3 0 0 0 1.618 1.179 6.7 6.7 0 0 0 1.901.63 13.5 13.5 0 0 0 2.004.177c.31.007.613.011.919.011.366.002.728.002 1.095.002h100.6c.36 0 .724 0 1.084-.002.304 0 .617-.004.922-.01a13 13 0 0 0 2-.178 6.8 6.8 0 0 0 1.908-.63A6.3 6.3 0 0 0 117.666 38a6.4 6.4 0 0 0 1.182-1.614 6.6 6.6 0 0 0 .619-1.905 13.5 13.5 0 0 0 .185-2.002c.004-.31.004-.61.004-.921.008-.364.008-.725.008-1.094V9.536q.002-.549-.008-1.092.001-.46-.004-.92a13.5 13.5 0 0 0-.185-2.003 6.6 6.6 0 0 0-.62-1.903 6.47 6.47 0 0 0-2.798-2.8 6.8 6.8 0 0 0-1.908-.627 13 13 0 0 0-2-.176c-.305-.005-.618-.011-.922-.013-.36-.002-.725-.002-1.084-.002Z" style="fill:#a6a6a6"></path><path d="M8.445 39.125c-.305 0-.602-.004-.904-.01a13 13 0 0 1-1.87-.164 5.9 5.9 0 0 1-1.656-.548 5.4 5.4 0 0 1-1.397-1.016 5.3 5.3 0 0 1-1.02-1.397 5.7 5.7 0 0 1-.544-1.657 12.4 12.4 0 0 1-.166-1.875c-.007-.21-.015-.913-.015-.913v-23.1s.009-.692.015-.895a12.4 12.4 0 0 1 .165-1.872 5.8 5.8 0 0 1 .544-1.662 5.4 5.4 0 0 1 1.015-1.398 5.6 5.6 0 0 1 1.402-1.023 5.8 5.8 0 0 1 1.653-.544A12.6 12.6 0 0 1 7.543.887l.902-.012h102.769l.913.013a12.4 12.4 0 0 1 1.858.162 6 6 0 0 1 1.671.548 5.6 5.6 0 0 1 2.415 2.42 5.8 5.8 0 0 1 .535 1.649 13 13 0 0 1 .174 1.887c.003.283.003.588.003.89.008.375.008.732.008 1.092v20.929c0 .363 0 .718-.008 1.075 0 .325 0 .623-.004.93a13 13 0 0 1-.17 1.853 5.7 5.7 0 0 1-.54 1.67 5.5 5.5 0 0 1-1.016 1.386 5.4 5.4 0 0 1-1.4 1.022 5.9 5.9 0 0 1-1.668.55 12.5 12.5 0 0 1-1.869.163c-.293.007-.6.011-.897.011l-1.084.002Z"></path><g data-name="&lt;Group&gt;"><g data-name="&lt;Group&gt;"><path d="M24.769 20.3a4.95 4.95 0 0 1 2.356-4.151 5.07 5.07 0 0 0-3.99-2.158c-1.68-.176-3.308 1.005-4.164 1.005-.872 0-2.19-.988-3.608-.958a5.32 5.32 0 0 0-4.473 2.728c-1.934 3.348-.491 8.269 1.361 10.976.927 1.325 2.01 2.805 3.428 2.753 1.387-.058 1.905-.885 3.58-.885 1.658 0 2.144.885 3.59.852 1.489-.025 2.426-1.332 3.32-2.67a11 11 0 0 0 1.52-3.092 4.78 4.78 0 0 1-2.92-4.4M22.037 12.21a4.87 4.87 0 0 0 1.115-3.49 4.96 4.96 0 0 0-3.208 1.66A4.64 4.64 0 0 0 18.8 13.74a4.1 4.1 0 0 0 3.237-1.53" data-name="&lt;Path&gt;" style="fill:#fff"></path></g><path d="M42.302 27.14H37.57l-1.137 3.356h-2.005l4.484-12.418h2.083l4.483 12.418h-2.039Zm-4.243-1.55h3.752l-1.85-5.446h-.051ZM55.16 25.97c0 2.813-1.506 4.62-3.779 4.62a3.07 3.07 0 0 1-2.848-1.583h-.043v4.484H46.63V21.442h1.8v1.506h.033a3.21 3.21 0 0 1 2.883-1.6c2.298 0 3.813 1.816 3.813 4.622m-1.91 0c0-1.833-.948-3.038-2.393-3.038-1.42 0-2.375 1.23-2.375 3.038 0 1.824.955 3.046 2.375 3.046 1.445 0 2.393-1.197 2.393-3.046M65.125 25.97c0 2.813-1.506 4.62-3.779 4.62a3.07 3.07 0 0 1-2.848-1.583h-.043v4.484h-1.859V21.442h1.799v1.506h.034a3.21 3.21 0 0 1 2.883-1.6c2.298 0 3.813 1.816 3.813 4.622m-1.91 0c0-1.833-.948-3.038-2.393-3.038-1.42 0-2.375 1.23-2.375 3.038 0 1.824.955 3.046 2.375 3.046 1.445 0 2.392-1.197 2.392-3.046M71.71 27.036c.138 1.232 1.334 2.04 2.97 2.04 1.566 0 2.693-.808 2.693-1.919 0-.964-.68-1.54-2.29-1.936l-1.609-.388c-2.28-.55-3.339-1.617-3.339-3.348 0-2.142 1.867-3.614 4.519-3.614 2.624 0 4.423 1.472 4.483 3.614h-1.876c-.112-1.239-1.136-1.987-2.634-1.987s-2.521.757-2.521 1.858c0 .878.654 1.395 2.255 1.79l1.368.336c2.548.603 3.606 1.626 3.606 3.443 0 2.323-1.85 3.778-4.793 3.778-2.754 0-4.614-1.42-4.734-3.667ZM83.346 19.3v2.142h1.722v1.472h-1.722v4.991c0 .776.345 1.137 1.102 1.137a6 6 0 0 0 .611-.043v1.463a5 5 0 0 1-1.032.086c-1.833 0-2.548-.689-2.548-2.445v-5.189h-1.316v-1.472h1.316V19.3ZM86.065 25.97c0-2.849 1.678-4.639 4.294-4.639 2.625 0 4.295 1.79 4.295 4.639 0 2.856-1.661 4.638-4.295 4.638s-4.294-1.782-4.294-4.638m6.695 0c0-1.954-.895-3.108-2.401-3.108s-2.4 1.162-2.4 3.108c0 1.962.894 3.106 2.4 3.106s2.401-1.144 2.401-3.106M96.186 21.442h1.773v1.541h.043a2.16 2.16 0 0 1 2.177-1.635 3 3 0 0 1 .637.069v1.738a2.6 2.6 0 0 0-.835-.112 1.873 1.873 0 0 0-1.937 2.083v5.37h-1.858ZM109.384 27.837c-.25 1.643-1.85 2.771-3.898 2.771-2.634 0-4.269-1.764-4.269-4.595 0-2.84 1.644-4.682 4.19-4.682 2.506 0 4.08 1.72 4.08 4.466v.637h-6.394v.112a2.358 2.358 0 0 0 2.436 2.564 2.05 2.05 0 0 0 2.09-1.273Zm-6.282-2.702h4.526a2.177 2.177 0 0 0-2.22-2.298 2.29 2.29 0 0 0-2.306 2.298" style="fill:#fff"></path></g><g data-name="&lt;Group&gt;"><path d="M37.826 8.731a2.64 2.64 0 0 1 2.808 2.965c0 1.906-1.03 3.002-2.808 3.002h-2.155V8.73Zm-1.228 5.123h1.125a1.876 1.876 0 0 0 1.967-2.146 1.88 1.88 0 0 0-1.967-2.134h-1.125ZM41.68 12.444a2.133 2.133 0 1 1 4.248 0 2.134 2.134 0 1 1-4.247 0m3.334 0c0-.976-.439-1.547-1.208-1.547-.773 0-1.207.571-1.207 1.547 0 .984.434 1.55 1.207 1.55.77 0 1.208-.57 1.208-1.55M51.573 14.698h-.922l-.93-3.317h-.07l-.927 3.317h-.913l-1.242-4.503h.902l.806 3.436h.067l.926-3.436h.852l.926 3.436h.07l.803-3.436h.889ZM53.854 10.195h.855v.715h.066a1.35 1.35 0 0 1 1.344-.802 1.465 1.465 0 0 1 1.559 1.675v2.915h-.889v-2.692c0-.724-.314-1.084-.972-1.084a1.033 1.033 0 0 0-1.075 1.141v2.635h-.888ZM59.094 8.437h.888v6.26h-.888ZM61.218 12.444a2.133 2.133 0 1 1 4.247 0 2.134 2.134 0 1 1-4.247 0m3.333 0c0-.976-.439-1.547-1.208-1.547-.773 0-1.207.571-1.207 1.547 0 .984.434 1.55 1.207 1.55.77 0 1.208-.57 1.208-1.55M66.4 13.424c0-.81.604-1.278 1.676-1.344l1.22-.07v-.389c0-.475-.315-.744-.922-.744-.497 0-.84.182-.939.5h-.86c.09-.773.818-1.27 1.84-1.27 1.128 0 1.765.563 1.765 1.514v3.077h-.855v-.633h-.07a1.52 1.52 0 0 1-1.353.707 1.36 1.36 0 0 1-1.501-1.348m2.895-.384v-.377l-1.1.07c-.62.042-.9.253-.9.65 0 .405.351.64.834.64a1.06 1.06 0 0 0 1.166-.983M71.348 12.444c0-1.423.732-2.324 1.87-2.324a1.48 1.48 0 0 1 1.38.79h.067V8.437h.888v6.26h-.851v-.71h-.07a1.56 1.56 0 0 1-1.415.785c-1.145 0-1.869-.901-1.869-2.328m.918 0c0 .955.45 1.53 1.203 1.53.75 0 1.212-.583 1.212-1.526 0-.938-.468-1.53-1.212-1.53-.748 0-1.203.58-1.203 1.526M79.23 12.444a2.133 2.133 0 1 1 4.247 0 2.134 2.134 0 1 1-4.247 0m3.333 0c0-.976-.438-1.547-1.208-1.547-.772 0-1.207.571-1.207 1.547 0 .984.435 1.55 1.207 1.55.77 0 1.208-.57 1.208-1.55M84.67 10.195h.855v.715h.066a1.35 1.35 0 0 1 1.344-.802 1.465 1.465 0 0 1 1.559 1.675v2.915h-.889v-2.692c0-.724-.314-1.084-.972-1.084a1.033 1.033 0 0 0-1.075 1.141v2.635h-.889ZM93.515 9.074v1.141h.976v.749h-.976v2.315c0 .472.194.679.637.679a3 3 0 0 0 .339-.021v.74a3 3 0 0 1-.484.046c-.988 0-1.381-.348-1.381-1.216v-2.543h-.715v-.749h.715V9.074ZM95.705 8.437h.88v2.481h.07a1.39 1.39 0 0 1 1.374-.806 1.483 1.483 0 0 1 1.55 1.679v2.907h-.889V12.01c0-.72-.335-1.084-.963-1.084a1.052 1.052 0 0 0-1.134 1.142v2.63h-.888ZM104.761 13.482a1.83 1.83 0 0 1-1.95 1.303 2.045 2.045 0 0 1-2.081-2.325 2.077 2.077 0 0 1 2.076-2.352c1.253 0 2.009.856 2.009 2.27v.31h-3.18v.05a1.19 1.19 0 0 0 1.2 1.29 1.08 1.08 0 0 0 1.07-.546Zm-3.126-1.451h2.275a1.086 1.086 0 0 0-1.109-1.167 1.15 1.15 0 0 0-1.166 1.167" style="fill:#fff"></path></g></svg></a></div></div></div></div></div></div></div></div></div></div><div class="w-full pr-5"><div class="w-full px-5 md:px-10 xl:px-16"><div class="mx-auto w-full sm:px-0"><div class="border-t-thin border-textTertiary-onDark ted-grid -mr-4 flex flex-wrap md:-mr-5"><div class="pr-4 md:pr-5 col-sm-12 col-xl-8"><div><nav><ul class="pb-3 pt-8 md:pt-10 lg:pb-5 lg:pt-10 ted-grid -mr-4 flex flex-wrap md:-mr-5"><li class="pr-4 md:pr-5 col-sm-2 col-md-3 col-lg-3 col-xl-3"><div class="mb-5 max-w-max"><a class="text-textTertiary-onDark hover:underline relative" href="/about/our-organization/our-policies-terms/ted-talks-usage-policy"><span class="text-sm">TED Talks Usage Policy</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-3 col-lg-3 col-xl-3"><div class="mb-5 max-w-max"><a class="text-textTertiary-onDark hover:underline relative" href="/about/our-organization/our-policies-terms/privacy-policy"><span class="text-sm">Privacy Policy</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-3 col-lg-3 col-xl-3"><div class="mb-5 max-w-max"><a class="text-textTertiary-onDark hover:underline relative" href="/about/partner-with-ted"><span class="text-sm">Advertising / Partnership</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-3 col-lg-3 col-xl-3"><div class="mb-5 max-w-max"><a class="text-textTertiary-onDark hover:underline relative" href="/about/our-organization/our-policies-terms/ted-com-terms-of-use"><span class="text-sm">TED.com Terms of Use</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-3 col-lg-3 col-xl-3"><div class="mb-5 max-w-max"><a class="text-textTertiary-onDark hover:underline relative" href="/about/our-organization/jobs-at-ted"><span class="text-sm">Jobs</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-3 col-lg-3 col-xl-3"><div class="mb-5 max-w-max"><a class="text-textTertiary-onDark hover:underline relative" href="/about/our-organization/contact-us/press-and-media-information"><span class="text-sm">Press</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-3 col-lg-3 col-xl-3"><div class="mb-5 max-w-max"><a href="https://support.ted.com" class="text-textTertiary-onDark hover:underline"><span class="text-sm">Help</span></a></div></li><li class="pr-4 md:pr-5 col-sm-2 col-md-3 col-lg-3 col-xl-3"><div class="mb-5 max-w-max"><button type="button" class="text-textTertiary-onDark hover:underline"><span class="text-sm">Privacy Preferences</span></button></div></li></ul></nav></div></div><div class="pr-0 md:pr-0 col-sm-0 col-xl-1"><div></div></div><div class="pr-4 md:pr-5 col-sm-12 col-lg-4 col-xl-3"><div><div class="xl:hidden bg-black dark:bg-[#898989] h-px opacity-16"></div><div class="text-sm pb-12 text-textTertiary-onDark xl:pl-3 xl:pt-10"> TED Conferences, LLC. All rights reserved.</div></div></div></div></div></div></div></footer></footer></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section></div><script src="https://ak.sail-horizon.com/spm/spm.v1.min.js"></script><script>
            window.addEventListener('load', () => {
                Sailthru.init({
                    hostWebsiteType: "spa",
                    customerId: 'a107d7f43ad64daa9a7ef2f8bfde9d97',
                    isCustom: true,
                    autoTrackPageview: false,
                    useStoredTags: true
                });
            });
        </script><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"shortenedUrl":"https://go.ted.com/eopyQ","action":null,"videoData":{"__typename":"Video","playerData":"{\"id\":\"75257\",\"mediaIdentifier\":\"consus-pm6628\",\"mediaProjectVersionIdentifier\":\"consus-pm6628\",\"duration\":4154,\"languages\":[{\"languageName\":\"English\",\"endonym\":\"English\",\"languageCode\":\"en\",\"ianaCode\":\"en\",\"isRtl\":false}],\"nativeLanguage\":\"en\",\"isSubtitleRequired\":false,\"resources\":{\"h264\":[{\"bitrate\":1200,\"file\":\"https://py.tedcdn.com/consus/projects/00/54/46/002/products/2021v-the-ted-interview-season-06-altman-002-fallback-582ec8e4b9609999e80a2d3251952d6b-1200k.mp4\"}],\"hls\":{\"adUrl\":\"https://pubads.g.doubleclick.net/gampad/ads?ciu_szs=300x250%2C512x288%2C120x60%2C320x50%2C6x7%2C6x8\\u0026correlator=%5Bcorrelator%5D\\u0026cust_params=event%3DTED%2BAudio%2BCollective%26id%3D75257%26tag%3Dtechnology%2CAI%2Cfuture%2Cscience%26talk%3Dthe_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021%26year%3D2021\\u0026env=vp\\u0026gdfp_req=1\\u0026impl=s\\u0026iu=%2F5641%2Fmobile%2Fios%2Fweb\\u0026output=xml_vast2\\u0026sz=640x360\\u0026unviewed_position_start=1\\u0026url=%5Breferrer%5D\",\"maiTargeting\":{\"id\":\"75257\",\"talk\":\"the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021\",\"tag\":\"technology,AI,future,science\",\"year\":\"2021\",\"event\":\"TED Audio Collective\"},\"stream\":\"https://hls.ted.com/project_masters/6628/manifest.m3u8\",\"metadata\":\"https://hls.ted.com/project_masters/6628/metadata.json\"}},\"targeting\":{\"id\":\"75257\",\"talk\":\"the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021\",\"tag\":\"technology,AI,future,science\",\"year\":\"2021\",\"event\":\"TED Audio Collective\"},\"canonical\":\"https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021\",\"name\":\"The TED Interview: The race to build AI that benefits humanity with Sam Altman (from April 2021)\",\"title\":\"The race to build AI that benefits humanity with Sam Altman (from April 2021)\",\"speaker\":\"The TED Interview\",\"thumb\":\"https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg?quality=89\\u0026w=600\",\"slug\":\"the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021\",\"event\":\"TED Audio Collective\",\"published\":1619188231}","takeaways":{"__typename":"TalkTakeawayConnection","nodes":[{"__typename":"TalkTakeaway","id":"14698","text":"AI's potential goes beyond dystopian views. OpenAI envisions AI benefiting humanity, with innovation leading the charge.","talkstarTalkId":75257,"offsetSeconds":115},{"__typename":"TalkTakeaway","id":"14699","text":"Determining AI alignment with human values is paramount, ensuring technology's exponential growth serves global well-being.","talkstarTalkId":75257,"offsetSeconds":920},{"__typename":"TalkTakeaway","id":"22342","text":"Harnessing AI's potential responsibly could guide humanity to a future where technology amplifies our ability to solve global challenges.","talkstarTalkId":75257,"offsetSeconds":160}]},"talkExtras":{"__typename":"TalkExtras","learnModules":[{"__typename":"LearnModule","author":null,"blurb":"The TED Audio Collective is a collection of podcasts for the curious. They're for listeners as excited by psychology and design as science and technology -- who want to dig deep into today's most exciting ideas. Subscribe, like, favorite or just listen, anywhere you get your podcasts.","eyebrow":"","headline":"TED Audio Collective","imageUrl":"http://page-builder.ted.com/system/baubles/files/000/008/970/original/TAC.png","linkUrl":"https://audiocollective.ted.com/","published":true,"publisher":null,"status":"APPROVED","type":"EXTERNAL_WEBSITE","visibleUrl":"audiocollective.ted.com","year":null},{"__typename":"LearnModule","author":null,"blurb":"Head of TED Chris Anderson speaks with some of the world's most interesting people to dig into the provocative and powerful ideas of our time.","eyebrow":"","headline":"Listen and subscribe to *The TED Interview*","imageUrl":"","linkUrl":"https://link.chtbl.com/TheTEDInterview","published":true,"publisher":null,"status":"APPROVED","type":"EXTERNAL_WEBSITE","visibleUrl":"link.chtbl.com","year":null}],"takeAction":[],"recommendations":[]},"relatedVideos":[{"__typename":"Video","slug":"the_ted_interview_sir_ken_robinson_still_wants_an_education_revolution","id":"30217"},{"__typename":"Video","slug":"the_ted_interview_michael_tubbs_on_politics_as_a_force_for_good","id":"51780"},{"__typename":"Video","slug":"genevieve_bell_6_big_ethical_questions_about_the_future_of_ai","id":"69385"}],"speakers":{"__typename":"AcmeSpeakerConnection","nodes":[{"__typename":"AcmeSpeaker","photoUrl":"https://ted-conferences-speaker-photos-production.s3.amazonaws.com/mmtz9i24r1o32vbi2945o11r78m5","firstname":"Sam","middlename":"","lastname":"Altman","description":"CEO of OpenAI","isLive":false,"title":"","whatOthersSay":"","whoTheyAre":"","whyListen":"\u003cdiv\u003eSam Altman is the CEO of OpenAI, developer of influential AI models like ChatGPT, DALLE, Sora and o1. OpenAIs mission is to ensure that all of humanity benefits from artificial general intelligence, or AGI  which they define as highly autonomous systems that outperform humans at most economically valuable work. Before his time at OpenAI, Altman was president of the startup accelerator Y Combinator.\u003c/div\u003e","slug":"sam_altman"},{"__typename":"AcmeSpeaker","photoUrl":"https://pe.tedcdn.com/images/ted/15a78b07d0340fcf8f391e7529b6f4b050a60cc0_254x191.jpg","firstname":"Chris","middlename":"","lastname":"Anderson","description":"Head of TED","isLive":true,"title":"","whatOthersSay":"","whoTheyAre":"","whyListen":"\u003cdiv\u003eAfter a long career in journalism and publishing, Chris Anderson became the curator of the TED Conference in 2002 and has developed it as a platform for identifying and disseminating ideas worth spreading. TED is a nonprofit devoted to sharing valuable ideas, primarily through the medium of TED Talks  short talks that are offered free online to a global audience.\u003cbr\u003e\u003cbr\u003e\u003c/div\u003e\u003cdiv\u003eChris was born in a remote village in Pakistan in 1957. He spent his early years in India, Pakistan and Afghanistan, where his parents worked as medical missionaries, and he attended an American school in the Himalayas for his early education. After boarding school in Bath, England, he went on to Oxford University, graduating in 1978 with a degree in philosophy, politics and economics.\u003cbr\u003e\u003cbr\u003e\u003c/div\u003e\u003cdiv\u003eChris then trained as a journalist, working in newspapers and radio, including two years producing a world news service in the Seychelles Islands. Back in the UK in 1984, Chris was captivated by the personal computer revolution and became an editor at one of the UK's early computer magazines. A year later he founded Future Publishing with a $25,000 bank loan. The new company initially focused on specialist computer publications but eventually expanded into other areas such as cycling, music, video games, technology and design, doubling in size every year for seven years. In 1994, Chris moved to the United States where he built Imagine Media, publisher of \u003cem\u003eBusiness 2.0\u003c/em\u003e magazine and creator of the popular video game users website IGN. Chris eventually merged Imagine and Future, taking the combined entity public in London in 1999, under the Future name. At its peak, it published 150 magazines and websites and employed 2,000 people.\u003cbr\u003e\u003cbr\u003eThis success allowed Chris to create a private nonprofit organization, the Sapling Foundation, with the hope of finding new ways to tackle tough global issues through media, technology, entrepreneurship and, most of all, ideas. In 2001, the foundation acquired the TED Conference, then an annual meeting of luminaries in the fields of Technology, Entertainment and Design held in Monterey, California, and Chris left Future to work full time on TED. He expanded the conference's remit to cover all topics, including science, business and key global issues, while adding a Fellows program, which now has some 300 alumni, and the TED Prize, which grants its recipients \"one wish to change the world.\" The TED stage has become a place for thinkers and doers from all fields to share their ideas and their work, capturing imaginations, sparking conversation and encouraging discovery along the way.\u003cbr\u003e\u003cbr\u003e\u003c/div\u003e\u003cdiv\u003eIn 2006, TED experimented with posting some of its talks on the Internet. Their viral success encouraged Chris to begin positioning the organization as a global media initiative devoted to 'ideas worth spreading,' part of a new era of information dissemination using the power of online video. In June 2015, the organization posted its 2,000th talk online. The talks are free to view, and they have been translated into more than 100 languages with the help of volunteers from around the world. Viewership has grown to approximately one billion views per year. Continuing a strategy of 'radical openness,' in 2009 Chris introduced the TEDx initiative, allowing free licenses to local organizers who wished to organize their own TED-like events. More than 8,000 such events have been held, generating an archive of 60,000 TEDx talks. And three years later, the TED-Ed program was launched, offering free educational videos and tools to students and teachers.\u003c/div\u003e","slug":"chris_anderson_ted"}]},"type":{"__typename":"TypeOfVideo","id":"9","name":"Podcast (audio only)"},"description":"In this season of The TED Interview, conversations with people who make a case for ... optimism. Not some blind, hopeful feeling, but the conviction that somewhere out there are solutions that, given the right attention and resources, can guide us out of the dark place we're in. For the first episode: artificial intelligence. Will innovation in AI drastically improve our lives, or destroy humanity as we know it? Head of TED Chris Anderson sits down with OpenAI CEO Sam Altman, who makes a case for AI's potential to make the future better for all of us -- and explains how his company is leading that charge with an unusual new business model. Listen and subscribe to The TED Interview and more podcasts from the TED Audio Collective wherever you're listening to this.","socialTitle":"The race to build AI that benefits humanity with Sam Altman (from April 2021)","internalLanguageCode":"en","commentsEnabled":false,"commentsLoggedInOnly":false,"recordedOn":"2021-04-05","curatorApproved":true,"socialDescription":"In this season of The TED Interview, conversations with people who make a case for ... optimism. Not some blind, hopeful feeling, but the conviction that somewhere out there are solutions that, given the right attention and resources, can guide us out of the dark place we're in. For the first episode: artificial intelligence. Will innovation in AI drastically improve our lives, or destroy humanity as we know it? Head of TED Chris Anderson sits down with OpenAI CEO Sam Altman, who makes a case for AI's potential to make the future better for all of us -- and explains how his company is leading that charge with an unusual new business model. Listen and subscribe to The TED Interview and more podcasts from the TED Audio Collective wherever you're listening to this.","partnerName":null,"videoContext":"TED Audio Collective","audioInternalLanguageCode":"en","language":"en","hasTranslations":true,"featured":false,"customContentDetails":{"__typename":"CustomContentDetails","partnerName":null},"topics":{"__typename":"TopicConnection","nodes":[{"__typename":"Topic","id":"8","name":"science","slug":"science"},{"__typename":"Topic","id":"10","name":"technology","slug":"technology"},{"__typename":"Topic","id":"80","name":"future","slug":"future"},{"__typename":"Topic","id":"184","name":"AI","slug":"ai"}]},"presenterDisplayName":"The TED Interview","duration":4154,"canonicalUrl":"https://www.ted.com/talks/the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021","viewedCount":1051995,"tedcomPercentage":"0.024","youtubePercentage":"0.0","podcastsPercentage":"0.971","tedappsPercentage":"0.005","publishedAt":"2021-04-23T14:30:31Z","id":"75257","title":"The race to build AI that benefits humanity with Sam Altman (from April 2021)","slug":"the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021","primaryImageSet":[{"__typename":"PhotoSize","url":"https://talkstar-photos.s3.amazonaws.com/uploads/c96fc6b2-4c1d-425c-8cf2-e3ff130d3d58/TAC_Interview_SamAltman_2021V-embed.jpg","aspectRatioName":"16x9"},{"__typename":"PhotoSize","url":"https://talkstar-photos.s3.amazonaws.com/uploads/06ec1c4e-c0bb-434e-a3bb-625b39cd2f34/TAC_Interview_SamAltman_2021V-stageshot.jpg","aspectRatioName":"4x3"},{"__typename":"PhotoSize","url":"https://talkstar-photos.s3.amazonaws.com/uploads/f47d3815-453d-4ee6-b2ac-b07902809f09/TAC_Interview_SamAltman_2021V-1350x675.jpg","aspectRatioName":"2x1"}]},"transcriptData":{"translation":{"__typename":"Translation","paragraphs":[{"__typename":"Paragraph","cues":[{"__typename":"Cue","text":"Hello there, this is Chris Anderson,\nand I am hugely, hugely,","time":5121},{"__typename":"Cue","text":"tremendously excited to welcome you to\na new series of the TED interview.","time":9246},{"__typename":"Cue","text":"Now, then, this season, we're\ntrying something new.","time":14621},{"__typename":"Cue","text":"We're organising the whole season\naround a single theme,","time":17579},{"__typename":"Cue","text":"albeit a theme that some of you may\nconsider inappropriate. But hear me out.","time":21454},{"__typename":"Cue","text":"The theme is the case for optimism.","time":26663},{"__typename":"Cue","text":"And yes, I know the world has been hit\nwith extraordinarily ugly things in","time":31204},{"__typename":"Cue","text":"the last few years. Political division, a\nracial reckoning, technology run amuck,","time":36121},{"__typename":"Cue","text":"not to mention a global pandemic and\nimpending climate catastrophe.","time":41746},{"__typename":"Cue","text":"What on earth are we thinking\nin this context?","time":46329},{"__typename":"Cue","text":"Optimism just seems so naive and unwanted,\nalmost annoying.","time":48996},{"__typename":"Cue","text":"So here's my position. Don't think\nof optimism as a feeling.","time":54371},{"__typename":"Cue","text":"It's not just this sort of shallow feeling\nof hope. Optimism is a search.","time":57162},{"__typename":"Cue","text":"It's a determination to look for a pathway\nforward somewhere out there.","time":63246},{"__typename":"Cue","text":"I believe I truly believe there are\namazing people whose minds contain","time":69079},{"__typename":"Cue","text":"the ideas, the visions, the solutions that\ncan actually create that pathway","time":75246},{"__typename":"Cue","text":"forward. If given the support\nand resources they need,","time":81454},{"__typename":"Cue","text":"they may very well light the path\nout of this dark place we're in.","time":84746},{"__typename":"Cue","text":"So these are the people who can present\nnot optimism, but a case for optimism.","time":89746},{"__typename":"Cue","text":"They're the people I'm talking\nto this season.","time":96079},{"__typename":"Cue","text":"So let's see if they can persuade us now.","time":98746},{"__typename":"Cue","text":"Then the place I want to start is with\nA.I. artificial intelligence.","time":102413},{"__typename":"Cue","text":"This, of course, is the next innovative\ntechnology that is going to change","time":107746},{"__typename":"Cue","text":"everything as we know it,\nfor better or for worse.","time":111371},{"__typename":"Cue","text":"Today was painted not with the\nusual dystopian brush,","time":115496},{"__typename":"Cue","text":"but by someone who truly believes\nin its potential.","time":119746},{"__typename":"Cue","text":"Sam Altman is the former president\nof Y Combinator,","time":122954},{"__typename":"Cue","text":"the legendary startup accelerator.","time":127413},{"__typename":"Cue","text":"And in 2015, he and a team launched\na company called Open Eye,","time":130163},{"__typename":"Cue","text":"dedicated to one noble purpose to develop\nA.I. so that it benefits humanity as","time":135163},{"__typename":"Cue","text":"a whole. You may have heard, by the way,","time":141788},{"__typename":"Cue","text":"recently a lot of buzz around in A.I.\ntechnology called T3 that was developed","time":144371},{"__typename":"Cue","text":"by open eye improve the quality of\nthe amazing team of researchers","time":149704},{"__typename":"Cue","text":"and developers they have work in.","time":153829},{"__typename":"Cue","text":"There will be hearing a lot about\nthree in the conversation ahead.","time":155413},{"__typename":"Cue","text":"But sticking to this lofty mission of\ndeveloping A.I. for humanity and finding","time":160496},{"__typename":"Cue","text":"the resources to realize\nit haven't been simple.","time":167079},{"__typename":"Cue","text":"Open A.I. is certainly not\nwithout its critics,","time":169663},{"__typename":"Cue","text":"but their goal couldn't be more important.","time":172663},{"__typename":"Cue","text":"And honestly, I found it really quite\nexciting to hear Sam's vision","time":175746},{"__typename":"Cue","text":"for where all this could lead. OK,\nlet's do this.","time":180288},{"__typename":"Cue","text":"So, Sam Altman, welcome.","time":196246},{"__typename":"Cue","text":"Thank you for having me.","time":198579},{"__typename":"Cue","text":"So, Sam, here we are in 2021.","time":200871},{"__typename":"Cue","text":"A lot of people are fearful of","time":204746},{"__typename":"Cue","text":"the future at this moment\nin world history.","time":206371},{"__typename":"Cue","text":"How would you describe your\nattitude to the future?","time":208496},{"__typename":"Cue","text":"I think that the combination of scientific\nand technological progress","time":212621},{"__typename":"Cue","text":"and better societal decision making,","time":217538},{"__typename":"Cue","text":"better societal governance\nis going to solve in","time":221538},{"__typename":"Cue","text":"the next couple of decades all of our\ncurrent most pressing problems,","time":225163},{"__typename":"Cue","text":"there will be new ones. But I think\nwe are going to get very safe,","time":229663},{"__typename":"Cue","text":"very inexpensive, carbon free\nnuclear energy to work.","time":233454},{"__typename":"Cue","text":"And I think we're going to talk about that\ntime that the climate disaster looks","time":237079},{"__typename":"Cue","text":"so bad and how lucky we are. We got saved\nby science and technology, I think.","time":240996},{"__typename":"Cue","text":"And we've already now seen this with","time":245079},{"__typename":"Cue","text":"the rapidity that we were able\nto get vaccines deployed.","time":246288},{"__typename":"Cue","text":"We are going to find that we are\nable to cure or at least treat","time":250621},{"__typename":"Cue","text":"a significant percentage of human disease,","time":255413},{"__typename":"Cue","text":"including I think we'll just actually make\nprogress in helping people have much","time":258371},{"__typename":"Cue","text":"longer decades, longer health spans.","time":263204},{"__typename":"Cue","text":"And I think in the next couple of decades,\nthat will look pretty clear.","time":265579},{"__typename":"Cue","text":"I think we will build systems with AI\nand otherwise that make access to","time":269579},{"__typename":"Cue","text":"an incredibly high quality education\nmore possible than ever before.","time":274538},{"__typename":"Cue","text":"I think the lives we look forward like\none hundred years, fifty years,","time":278788},{"__typename":"Cue","text":"even the quality of life available to\nanyone then will be much better than","time":282704},{"__typename":"Cue","text":"the quality of life available in the\nvery best case to anyone today,","time":288913},{"__typename":"Cue","text":"to any single person today. So, yeah,\nI'm super optimistic.","time":293663},{"__typename":"Cue","text":"I think, like, it's always easy to do\nscroll and think about how bad are","time":297121},{"__typename":"Cue","text":"the bad things are, but the good things\nare really good and getting much better.","time":303079},{"__typename":"Cue","text":"Is it your sincere belief that artificial\nintelligence can actually make that","time":307371},{"__typename":"Cue","text":"future better?","time":313163},{"__typename":"Cue","text":"Certainly. How look, with any technology.\nI don't think it will all be better.","time":315121},{"__typename":"Cue","text":"I think there are always positive and\nnegative use cases of anything new,","time":320996},{"__typename":"Cue","text":"and it's our job to maximize\nthe positive ones,","time":325496},{"__typename":"Cue","text":"minimize the negative ones.","time":327746},{"__typename":"Cue","text":"But I truly, genuinely believe that","time":328913},{"__typename":"Cue","text":"the positive impacts will be orders of\nmagnitude bigger than the negative ones.","time":331371},{"__typename":"Cue","text":"I think we're seeing a\nglimpse of that now.","time":336163},{"__typename":"Cue","text":"Now that we have the first general\npurpose built out in the world","time":338538},{"__typename":"Cue","text":"and available via things like RPI,","time":341579},{"__typename":"Cue","text":"I think we are seeing evidence of just","time":343829},{"__typename":"Cue","text":"the breadth of services that\nwe will be able to offer as","time":345621},{"__typename":"Cue","text":"the sort of technological revolution\nreally takes hold.","time":349246},{"__typename":"Cue","text":"And we will have people interact with\nservices that are smart, really smart,","time":352079},{"__typename":"Cue","text":"and it will feel like as strange as","time":357788},{"__typename":"Cue","text":"the world before mobile phones\nfeels now to us.","time":360496},{"__typename":"Cue","text":"Hmm, yeah, you mentioned your API,\nI guess that stands for what,","time":363746},{"__typename":"Cue","text":"application programming interface?","time":367913},{"__typename":"Cue","text":"It's the technology that allows complex\ntechnology to be accessible to others.","time":370204},{"__typename":"Cue","text":"So give me a sense of a couple of things\nthat have got you most excited that","time":377829},{"__typename":"Cue","text":"are already out there and then how\nthat gives you visibility to","time":382038},{"__typename":"Cue","text":"a pathway forward that is\neven more exciting.","time":386079},{"__typename":"Cue","text":"So I think that the things that we're\nseeing now are very much glimpse of","time":388621},{"__typename":"Cue","text":"the future. We released three,","time":392579},{"__typename":"Cue","text":"which is a general-purpose natural\nlanguage text model in","time":395288},{"__typename":"Cue","text":"the summer of twenty twenty.","time":399996},{"__typename":"Cue","text":"You know, there's hundreds of applications\nthat are now using it in","time":401663},{"__typename":"Cue","text":"production that's ramping\nup all of the time.","time":404496},{"__typename":"Cue","text":"But there are things where people\nuse three to really understand","time":407121},{"__typename":"Cue","text":"the intent behind the search\nquery and deliver results","time":411329},{"__typename":"Cue","text":"and sort of understand not only intent,","time":414704},{"__typename":"Cue","text":"but all of the data and deliver\nthe thing of what you want.","time":417038},{"__typename":"Cue","text":"So you can sort of describe a fuzzy thing\nand it'll understand documents.","time":419788},{"__typename":"Cue","text":"It can understand, you know, short\ndocuments, not full books yet,","time":423496},{"__typename":"Cue","text":"but bring you back to the context\nof what you want.","time":427163},{"__typename":"Cue","text":"There's been a lot of excitement\nabout using","time":429579},{"__typename":"Cue","text":"the generative capabilities\nto create sort of games","time":431371},{"__typename":"Cue","text":"or sort of interactive stories or letting\npeople develop characters","time":434913},{"__typename":"Cue","text":"or chat with a sort of virtual friend.","time":438913},{"__typename":"Cue","text":"There are applications that, for example,","time":441704},{"__typename":"Cue","text":"help a job seeker polish a tailored\napplication for each individual company.","time":443996},{"__typename":"Cue","text":"There's the beginning of tutors that can\nsort of teach people about different","time":448788},{"__typename":"Cue","text":"concepts and take on different personas.\nAnd we can go on for a long time.","time":452788},{"__typename":"Cue","text":"But I think anything that you can imagine\nthat you do today via computer that","time":456371},{"__typename":"Cue","text":"you would like to really understand\nand get to know you.","time":461246},{"__typename":"Cue","text":"And not only that, but understand all of\nthe data and knowledge in the world","time":463621},{"__typename":"Cue","text":"and help you have the best experience that\nis is possible that that will happen.","time":467704},{"__typename":"Cue","text":"So what gets opened up? What new adjacent\npossible state is that as","time":473996},{"__typename":"Cue","text":"a result of these powers\nfrom this question,","time":480079},{"__typename":"Cue","text":"from the point of view of someone who's\nstarting out on a career, for example,","time":482288},{"__typename":"Cue","text":"they're trying to figure out what would\nbe a really interesting thing to do in","time":486788},{"__typename":"Cue","text":"the future that has only recently\nbecome possible.","time":490329},{"__typename":"Cue","text":"What are some new things\nthat this opens up","time":493579},{"__typename":"Cue","text":"in a world where you can\ntalk to a computer?","time":497079},{"__typename":"Cue","text":"And get. The output that would\nnormally require you hiring","time":503871},{"__typename":"Cue","text":"the world experts back immediately\nfor almost no money,","time":509996},{"__typename":"Cue","text":"I would say think about\nwhat's possible there.","time":516079},{"__typename":"Cue","text":"So that could be like, as you said,","time":519246},{"__typename":"Cue","text":"what can normally only the best\nprogrammer in the world or","time":521621},{"__typename":"Cue","text":"a really great programmer do for me.","time":524662},{"__typename":"Cue","text":"And can I now instead just ask in English\nand have that program written?","time":526829},{"__typename":"Cue","text":"So all these people that, you know, want\nto develop an app and they have an idea,","time":531329},{"__typename":"Cue","text":"but they don't know how to program.\nNow they can have it.","time":534537},{"__typename":"Cue","text":"You know, what is the service\nlook like when anyone","time":536912},{"__typename":"Cue","text":"on Earth who wants really\ngreat medical advice?","time":540329},{"__typename":"Cue","text":"Can get better medical advice than\nany single doctor could ever get,","time":544204},{"__typename":"Cue","text":"because this has the total medical\nknowledge and reasoning ability that","time":547579},{"__typename":"Cue","text":"the some humanity has ever produced.","time":552162},{"__typename":"Cue","text":"When you want to learn something,","time":555371},{"__typename":"Cue","text":"you have sort of a tutor that understands\nyour exact style,","time":556829},{"__typename":"Cue","text":"how you best learn everything you know,","time":561537},{"__typename":"Cue","text":"and custom teaches you whatever concept\nyou want to learn someday.","time":563787},{"__typename":"Cue","text":"You can imagine that like.","time":566996},{"__typename":"Cue","text":"You have an eye that reads your email\nand your task list and your calendar","time":569579},{"__typename":"Cue","text":"and the documents you've been sent","time":573746},{"__typename":"Cue","text":"and in any meeting maximally perfectly\nprepares you and has all of","time":575412},{"__typename":"Cue","text":"the information that you need in all","time":578746},{"__typename":"Cue","text":"the context of your entire career right\nthere for you to go on for a long time.","time":580162},{"__typename":"Cue","text":"But I think this will just\nbe powerful systems.","time":584746},{"__typename":"Cue","text":"So it's really fun playing around\nwith Chapatti three,","time":587662},{"__typename":"Cue","text":"one compelling example of someone\nwho's more tax base is try","time":591287},{"__typename":"Cue","text":"Googling The Guardian essay that was\nwritten entirely by different GP2 three","time":594746},{"__typename":"Cue","text":"queries and stitched together.","time":601412},{"__typename":"Cue","text":"It's an essay on why artificial\nintelligence isn't a threat to humanity.","time":602954},{"__typename":"Cue","text":"And that's impressive.\nIt's very compelling.","time":607787},{"__typename":"Cue","text":"I actually tried inputting one\nof the three online uses.","time":609871},{"__typename":"Cue","text":"I asked the question what is interesting\nabout some ollman? Oh no.","time":615037},{"__typename":"Cue","text":"Here's what it came back with. It was it\nwas rather philosophical, actually.","time":620496},{"__typename":"Cue","text":"Came back with. I don't understand\nwhat you mean by interesting.","time":623537},{"__typename":"Cue","text":"It seems to me that this word has\nno unambiguous meaning in","time":626246},{"__typename":"Cue","text":"the context of human society beyond its\nliteral definition as somewhat pleasing","time":629371},{"__typename":"Cue","text":"or entertaining. I do not believe\nthere to be any such thing","time":633246},{"__typename":"Cue","text":"as INTERESTINGNESS except\nin the mind of a human","time":636287},{"__typename":"Cue","text":"or other sentient being\nthat to my knowledge,","time":638871},{"__typename":"Cue","text":"this is an entirely subjective state\nthat varies from person to person.","time":640996},{"__typename":"Cue","text":"However, I will grant that this is\nan interesting question to ask.","time":644454},{"__typename":"Cue","text":"This does not mean it has been answered.\nThere is no answer to be found.","time":648246},{"__typename":"Cue","text":"Well, so you can agree that somewhere\nbetween profound and gibberish","time":651579},{"__typename":"Cue","text":"is that almost well, with the\nstate of play is I mean,","time":660412},{"__typename":"Cue","text":"that's where we are today.","time":663496},{"__typename":"Cue","text":"I think somewhere between profound\nand jibberish is","time":664704},{"__typename":"Cue","text":"the right way to think about the current\ncapabilities of CGP three.","time":668329},{"__typename":"Cue","text":"I think they would definitely had a bubble\nof hype about three last summer.","time":673704},{"__typename":"Cue","text":"But the thing about bubbles is the\nreason that smart people fall","time":679621},{"__typename":"Cue","text":"for them is there's a kernel\nof something really real","time":682787},{"__typename":"Cue","text":"and really interesting that people\nget overexcited about.","time":685912},{"__typename":"Cue","text":"And I think people definitely got","time":689871},{"__typename":"Cue","text":"and still are overexcited\nabout 3:00 today,","time":692079},{"__typename":"Cue","text":"but still probably underestimated","time":695204},{"__typename":"Cue","text":"the potential of where these models\nwill go in the future.","time":697204},{"__typename":"Cue","text":"And so maybe there's this like short term\noverhyped and long term under hype","time":700162},{"__typename":"Cue","text":"for the entire field, for tax models, for\nwhatever you'd like. It's going on.","time":704121},{"__typename":"Cue","text":"And as you said, there's clearly\nsome gibberish in there.","time":708412},{"__typename":"Cue","text":"But on the other hand, those were\nlike well-formed sentences.","time":711704},{"__typename":"Cue","text":"And there were a couple of ideas\nand there that I was like, oh,","time":714204},{"__typename":"Cue","text":"like they actually maybe that's right.","time":716412},{"__typename":"Cue","text":"And I think if artificial intelligence,\neven in its current very larval state,","time":718996},{"__typename":"Cue","text":"can make us confront new things\nand sort of inspire new ideas,","time":724704},{"__typename":"Cue","text":"that's already pretty impressive.","time":728246},{"__typename":"Cue","text":"Give us a sense of what's actually\nhappening in the background there.","time":730454},{"__typename":"Cue","text":"I think it's hard to understand","time":733454},{"__typename":"Cue","text":"because you read these words seem like\nsomeone is trying to mean something.","time":735246},{"__typename":"Cue","text":"Obviously, I think you believe that\nthere's whatever you've built there,","time":739662},{"__typename":"Cue","text":"that there's a sort of thinking, sentient\nthing that's going, oh,","time":743746},{"__typename":"Cue","text":"I must answer this question. So so what\nhow would you describe what's going on?","time":747537},{"__typename":"Cue","text":"You've got something that has\nread the entire Internet,","time":752996},{"__typename":"Cue","text":"essentially all of Wikipedia, etc. We've","time":755121},{"__typename":"Cue","text":"read something that's read like a small\nfraction of a random sampling of","time":757662},{"__typename":"Cue","text":"the Internet. We will eventually train\nsomething that has read as much of","time":761287},{"__typename":"Cue","text":"the Internet or more of the Internet\nthan we've done right now.","time":764579},{"__typename":"Cue","text":"But we have a very long way to go.","time":767871},{"__typename":"Cue","text":"I mean, we're still, I think,","time":769371},{"__typename":"Cue","text":"relative to what we will have operated\nat quite small scale with quite small","time":770371},{"__typename":"Cue","text":"eyes. But what is happening is there is\na model that is ingesting lots of text","time":774746},{"__typename":"Cue","text":"and it is trying to predict the next word.","time":782329},{"__typename":"Cue","text":"So we use Transformer's they\ntake in a context,","time":787621},{"__typename":"Cue","text":"which is a particular architecture\nof an A.I. model,","time":790954},{"__typename":"Cue","text":"they take in a context of a lot of words,","time":793537},{"__typename":"Cue","text":"let's say like a thousand\nor something like that.","time":795537},{"__typename":"Cue","text":"And they try to predict the word that\ncomes next in the sequence.","time":797621},{"__typename":"Cue","text":"And there's like a lot of other\nthings that happen,","time":802162},{"__typename":"Cue","text":"but fundamentally that's it,","time":803829},{"__typename":"Cue","text":"and I think this is interesting\nbecause in","time":805787},{"__typename":"Cue","text":"the process of playing that little game\nof trying to predict the next word,","time":808496},{"__typename":"Cue","text":"these models have to develop\na representation","time":812454},{"__typename":"Cue","text":"and understanding of what is\nlikely to come next and.","time":816829},{"__typename":"Cue","text":"I think it is maybe not\nperfectly accurate,","time":823787},{"__typename":"Cue","text":"but certainly worth considering to say\nthat intelligence is very near","time":827496},{"__typename":"Cue","text":"the ability to make accurate predictions.","time":832454},{"__typename":"Cue","text":"What's confusing about this is that\nthere are so many words on","time":834787},{"__typename":"Cue","text":"the Internet which are foolish as\nwell as the words that are wise.","time":838162},{"__typename":"Cue","text":"And and how do you build a model that\ncan distinguish between those two?","time":842996},{"__typename":"Cue","text":"And this is prompted actually by another\nexample that I typed in.","time":847621},{"__typename":"Cue","text":"Like I asked, you know, what is a powerful\nidea, very interested in ideas.","time":850121},{"__typename":"Cue","text":"That was my question as a powerful idea.","time":855621},{"__typename":"Cue","text":"And it came back with several things,","time":857246},{"__typename":"Cue","text":"some of which seemed moderately\npronouncements,","time":858579},{"__typename":"Cue","text":"which seemed moderately gibberish.","time":861079},{"__typename":"Cue","text":"But then he was he was one that it\ncame back with the idea that","time":862621},{"__typename":"Cue","text":"the human race has, quote, evolved,\nunquote,","time":866204},{"__typename":"Cue","text":"is false evolution or adaptation within\na species was abandoned by biology","time":869496},{"__typename":"Cue","text":"and genetics long ago. Wait a sec. That's\nnews to me. What have you been reading?","time":875371},{"__typename":"Cue","text":"And I presume this has been pulled out\nof some recesses of the Internet,","time":882537},{"__typename":"Cue","text":"but how is it possible, even in theory,","time":887204},{"__typename":"Cue","text":"to imagine how a model can gravitate\ntowards truth, wisdom,","time":891662},{"__typename":"Cue","text":"as opposed to just like majority views?","time":897454},{"__typename":"Cue","text":"Or how how how do you avoid something\ntaking us further into the sort of","time":900871},{"__typename":"Cue","text":"the maze of errors and bad thinking\nand so forth that has already been","time":909537},{"__typename":"Cue","text":"a worrying feature for the last few years\n?","time":913121},{"__typename":"Cue","text":"It's a fantastic question,","time":914746},{"__typename":"Cue","text":"and I think it is the most interesting\narea of research that we need to pursue.","time":915829},{"__typename":"Cue","text":"Now, I think at this point,","time":920162},{"__typename":"Cue","text":"the questions of whether we can build\nreally powerful general-purpose AI system,","time":922121},{"__typename":"Cue","text":"I won't say there in the rearview mirror.","time":928412},{"__typename":"Cue","text":"We still have a lot of hard\nengineering work to do,","time":929829},{"__typename":"Cue","text":"but I'm pretty confident we're\ngoing to be able to.","time":931829},{"__typename":"Cue","text":"And now the questions are like,\nwhat should we build?","time":934329},{"__typename":"Cue","text":"And how and why and what\ndata should we train on","time":938454},{"__typename":"Cue","text":"and how do we build systems not just that\ncan do these like phenomenally impressive","time":941871},{"__typename":"Cue","text":"things, but that we can ensure\ndo the things that we want","time":945912},{"__typename":"Cue","text":"and that understand the concepts of\ntruth and falsehood and, you know,","time":949204},{"__typename":"Cue","text":"alignment with human values and\nmisalignment with human values.","time":954371},{"__typename":"Cue","text":"One of the pieces of research that\nwe put out last year that","time":959329},{"__typename":"Cue","text":"I was most proud of and most excited\nabout is what we call reinforcement","time":962746},{"__typename":"Cue","text":"learning from human feedback.","time":967204},{"__typename":"Cue","text":"And we showed that we can take these\ngiant models that are trained on","time":968787},{"__typename":"Cue","text":"a bunch of stuff, some of it good,\nsome of the bad,","time":971621},{"__typename":"Cue","text":"and then with a really quite small amount\nof feedback from human judgment about,","time":974662},{"__typename":"Cue","text":"hey, this is good, this\nis bad, this is wrong,","time":979996},{"__typename":"Cue","text":"this is the behavior I want I\ndon't want this behavior.","time":981996},{"__typename":"Cue","text":"We can feed that information from the\nhuman judges back into the model","time":984662},{"__typename":"Cue","text":"and we can teach the model, behave more\nlike this and less like that.","time":989662},{"__typename":"Cue","text":"And it works better than I\never imagined it would.","time":993454},{"__typename":"Cue","text":"And that gives me a lot of hope that\nwe can build an aligned system.","time":996246},{"__typename":"Cue","text":"We'll do other things, too,","time":1000912},{"__typename":"Cue","text":"like I think curating data sets where\nthere's just less sort of bad data to","time":1001996},{"__typename":"Cue","text":"train on. It will go a very long way.","time":1007704},{"__typename":"Cue","text":"And as these models get smarter,","time":1009662},{"__typename":"Cue","text":"I think they inherently develop","time":1011329},{"__typename":"Cue","text":"the ability to sort out bad\ndata from good data.","time":1013996},{"__typename":"Cue","text":"And as they get really smart,","time":1016496},{"__typename":"Cue","text":"they'll even start to do something\nwe call active learning,","time":1017996},{"__typename":"Cue","text":"which is where they ask us for exactly","time":1020829},{"__typename":"Cue","text":"the data they need when they're missing\nsomething, when they're unsure,","time":1022662},{"__typename":"Cue","text":"when they don't understand.","time":1025038},{"__typename":"Cue","text":"But I think as a result of simply scaling\nthese models up, building better,","time":1027121},{"__typename":"Cue","text":"I hate to use the word cognition because\nit sounds so anthropomorphic,","time":1031996},{"__typename":"Cue","text":"but let's say building a better ability\nto reason into the models, to think,","time":1034746},{"__typename":"Cue","text":"to challenge, to try to understand","time":1038996},{"__typename":"Cue","text":"and combining that with this idea of\nonline into human values via this","time":1042079},{"__typename":"Cue","text":"technique we developed, that's\ngoing to go a very long way.","time":1047371},{"__typename":"Cue","text":"Now, there's another question,","time":1050288},{"__typename":"Cue","text":"which you sort of just kicked the\nball down the field, too,","time":1051538},{"__typename":"Cue","text":"which is how do we as a society decide to\nwhich set of human values do we align","time":1053788},{"__typename":"Cue","text":"these powerful systems?","time":1058954},{"__typename":"Cue","text":"Yeah, indeed. So if I if I understand\nrightly what you're saying,","time":1061913},{"__typename":"Cue","text":"that you're saying that it's\npossible to look at","time":1065371},{"__typename":"Cue","text":"the output at any one time of three.","time":1069038},{"__typename":"Cue","text":"And if we don't like what it's coming up\nwith, some ways human can say, no,","time":1072579},{"__typename":"Cue","text":"that was off, don't do that.","time":1077871},{"__typename":"Cue","text":"Whatever algorithm or process led\nyou to that, undo it. Yeah.","time":1079038},{"__typename":"Cue","text":"And that the system is that incredibly\npowerful at avoiding that same kind of","time":1084663},{"__typename":"Cue","text":"mistake in future because it sort\nof replicates the instructions","time":1089621},{"__typename":"Cue","text":", correct? Yeah.","time":1093788},{"__typename":"Cue","text":"And eventually and not much longer,","time":1094329},{"__typename":"Cue","text":"I believe that we'll be able to not only\nsay that was good, that was bad,","time":1096579},{"__typename":"Cue","text":"but say that was bad for this reason.","time":1099996},{"__typename":"Cue","text":"And also tell me how you got to that\nanswer so I can make sure I understand.","time":1102621},{"__typename":"Cue","text":"But at the end of the day,","time":1107371},{"__typename":"Cue","text":"someone needs to decide\nwho is the wise human","time":1108371},{"__typename":"Cue","text":"or short humans who are looking at the\nresults. So it's a big difference.","time":1111954},{"__typename":"Cue","text":"Someone who who grew up with intelligent\ndesign world view could look at that","time":1114913},{"__typename":"Cue","text":"and go, that's a brilliant outcome. Well,\nGoldstar done.","time":1118746},{"__typename":"Cue","text":"And someone else would say something\nis done awfully wrong here. So","time":1122079},{"__typename":"Cue","text":"how do you avoid and this is a version of\nthe problem that a lot of the, I guess,","time":1129163},{"__typename":"Cue","text":"Silicon Valley companies are facing\nright now in terms of","time":1134746},{"__typename":"Cue","text":"the pushback they're getting on the output\nof social media and so forth.","time":1137996},{"__typename":"Cue","text":"How do you assemble that pool\nof experts who stand","time":1141788},{"__typename":"Cue","text":"for human values that we actually want?","time":1146538},{"__typename":"Cue","text":"I mean, we talk about this all the time,","time":1152204},{"__typename":"Cue","text":"I don't think this is like solely","time":1153538},{"__typename":"Cue","text":"or even not even close to majorly\nup to opening night to decide,","time":1155371},{"__typename":"Cue","text":"I think we need to begin a societal\nconversation now about how we're going to","time":1158788},{"__typename":"Cue","text":"make those decisions, how we're going to\nmake sure we have representational input","time":1163121},{"__typename":"Cue","text":"in that, and how we sort of make these\nvery difficult global governance systems.","time":1166621},{"__typename":"Cue","text":"My personal belief is that we should have\npretty broad rules about what these","time":1171329},{"__typename":"Cue","text":"systems will never do and will always do.","time":1176746},{"__typename":"Cue","text":"But then the individual user should get","time":1181413},{"__typename":"Cue","text":"a system that kind of behaves\nlike they want.","time":1183746},{"__typename":"Cue","text":"And there will be people do have\nvery different value systems.","time":1186871},{"__typename":"Cue","text":"Some of them are just fundamentally\nincompatible.","time":1191121},{"__typename":"Cue","text":"No one gets to use eye to, like, exploit\nother people, for example,","time":1193454},{"__typename":"Cue","text":"and hopefully we can all agree on.\nBut do you want the AI to like.","time":1196663},{"__typename":"Cue","text":"You know, support you and your belief\nof intelligent design, like,","time":1201413},{"__typename":"Cue","text":"do I think openly, I should say it can't,","time":1204121},{"__typename":"Cue","text":"even though I disagree with that is\nlike a scientific conclusion.","time":1206454},{"__typename":"Cue","text":"No, I wouldn't take that stance.","time":1209538},{"__typename":"Cue","text":"I think the thing to remember about all\nof this is that history is still quite","time":1212121},{"__typename":"Cue","text":"extraordinarily weak. It's still has\nsuch big problems and it's still","time":1217496},{"__typename":"Cue","text":"so unreliable that for most use\ncases it's still unsuitable.","time":1222829},{"__typename":"Cue","text":"But when we think about a system that is\nlike a thousand times more powerful","time":1227704},{"__typename":"Cue","text":"and let's say a million\ntimes more reliable,","time":1231496},{"__typename":"Cue","text":"it just doesn't it doesn't\nsay gibberish very often.","time":1233788},{"__typename":"Cue","text":"It doesn't totally lose the\nplot and get distracted","time":1236913},{"__typename":"Cue","text":"or system like that is going to be one\nthat a lot of the economic activity in","time":1239871},{"__typename":"Cue","text":"the world comes to rely on.","time":1243246},{"__typename":"Cue","text":"And I think it's very important\nthat we don't have","time":1245246},{"__typename":"Cue","text":"a small group of people sort of saying you\ncan never use it for this thing that,","time":1248996},{"__typename":"Cue","text":"like most of the world\nwants to use it for","time":1253038},{"__typename":"Cue","text":"because it doesn't match\nour personal beliefs.","time":1255704},{"__typename":"Cue","text":"Talk a bit more about some\nof the other uses of it,","time":1259079},{"__typename":"Cue","text":"because one of the things that's most\nsurprising is it's not just about sort","time":1261079},{"__typename":"Cue","text":"of text responses. It's it can take\ngeneralized human instructions","time":1264496},{"__typename":"Cue","text":"and build things up. For example,\nyou can say to it,","time":1269454},{"__typename":"Cue","text":"write a Python program that\nis designed to put","time":1273038},{"__typename":"Cue","text":"a flashing cursor in one\ncorner of the screen,","time":1277246},{"__typename":"Cue","text":"in the Google logo in the other corner.","time":1280579},{"__typename":"Cue","text":"And and it can go your way and\ndo something like that.","time":1282496},{"__typename":"Cue","text":"Shockingly, quite well, effectively.","time":1286413},{"__typename":"Cue","text":"Yeah, I it can.","time":1289371},{"__typename":"Cue","text":"That's amazing. I mean, this is amazing\nto me. That opens the door to.","time":1292038},{"__typename":"Cue","text":"An entirely way to think about\nprogramers for the future,","time":1296621},{"__typename":"Cue","text":"that you could you could have people who\ncan program just in human natural","time":1299329},{"__typename":"Cue","text":"language potentially and gain rapid\nefficiency. I do the engineering.","time":1303538},{"__typename":"Cue","text":"We're not that far away from that world.","time":1310704},{"__typename":"Cue","text":"We're not that far away from the world\nwhere you will write a spec in English.","time":1312288},{"__typename":"Cue","text":"And for a simple enough program, I\nwill just write the code for you.","time":1315538},{"__typename":"Cue","text":"As you said, you can see glimpses of that\neven in this very week three which","time":1319913},{"__typename":"Cue","text":"was not trained to code like. I think\nthis is important to remember.","time":1323871},{"__typename":"Cue","text":"We trained it on the language on the\nInternet very rarely, you know,","time":1326454},{"__typename":"Cue","text":"Internet let language on the Internet\nalso includes some code snippets.","time":1330163},{"__typename":"Cue","text":"And that was enough, so if we really try\nto go train a model on code itself","time":1334829},{"__typename":"Cue","text":"and that's where we decide to put the\nhorsepower of the model into,","time":1339996},{"__typename":"Cue","text":"just imagine what will be possible\nwill be quite impressive.","time":1342829},{"__typename":"Cue","text":"But I think what you're pointing\nto there is that","time":1345954},{"__typename":"Cue","text":"because models like three\nto some degree or other,","time":1348413},{"__typename":"Cue","text":"and it's like very hard to know\nexactly how much understand","time":1351538},{"__typename":"Cue","text":"the underlying concepts\nof what's going on.","time":1355329},{"__typename":"Cue","text":"And they're not just regurgitating\nthings they found in a website,","time":1357788},{"__typename":"Cue","text":"but they can really apply\nthem and say, oh, yeah,","time":1360663},{"__typename":"Cue","text":"I kind of like know about this\nword and this idea and code.","time":1362371},{"__typename":"Cue","text":"And this is probably what you're trying to\ndo. And I won't get it right always.","time":1365746},{"__typename":"Cue","text":"But sometimes I will just generate\nthis like a brand new program","time":1368496},{"__typename":"Cue","text":"for nothing that anyone has ever\nasked before. And it will work.","time":1372121},{"__typename":"Cue","text":"That's pretty cool. And data is data. So\nit can do that from English to code.","time":1375163},{"__typename":"Cue","text":"It can do that from English to French.","time":1380038},{"__typename":"Cue","text":"Again, we never told it to\nlearn about translation.","time":1381829},{"__typename":"Cue","text":"We never told it about the concepts of\nEnglish and French, but it learned them,","time":1384204},{"__typename":"Cue","text":"even though we never said this is what\nEnglish is and this is what French is","time":1388996},{"__typename":"Cue","text":"and this is what it means to translate,\nit can still do it.","time":1391496},{"__typename":"Cue","text":"Wow, I mean, for creative people,","time":1395538},{"__typename":"Cue","text":"is there a world coming where the sort of","time":1398704},{"__typename":"Cue","text":"the palette of possibility that they\ncan be exposed to is just explodes?","time":1401371},{"__typename":"Cue","text":"I mean, if you're a musician,","time":1406204},{"__typename":"Cue","text":"is there a near future where you can say\nto your eye, OK, I'm going to bed now,","time":1407954},{"__typename":"Cue","text":"but in the morning I'd love\nyou to present me with","time":1412246},{"__typename":"Cue","text":"a thousand tuba jingles with words\nattached that you have of","time":1415288},{"__typename":"Cue","text":"a sort of mean factor to the and you\ncome down in the morning and","time":1420288},{"__typename":"Cue","text":"the computer shows you the stuff. And one\nof them, you go, wow, that is it.","time":1423163},{"__typename":"Cue","text":"That is a top 10 hit and you\nbuild a song from it.","time":1427079},{"__typename":"Cue","text":"Or is that going to be released?\nActually be the value add.","time":1430288},{"__typename":"Cue","text":"We released something last\nyear called Jukebox,","time":1433954},{"__typename":"Cue","text":"which is very near what you described,","time":1436038},{"__typename":"Cue","text":"where you can say I want music generated\nfor me in this style","time":1438038},{"__typename":"Cue","text":"or this kind of stuff, and it can\ncome up with the words as well.","time":1440996},{"__typename":"Cue","text":"And it's like pretty cool. And I really\nenjoy listening to music that it creates.","time":1443913},{"__typename":"Cue","text":"And I can sort of do four songs, two bars\nof a jingle, whatever you'd like.","time":1449996},{"__typename":"Cue","text":"And one of my very favorite\nartists reached out,","time":1454413},{"__typename":"Cue","text":"called to open it after we release this\nand said that he wanted to talk.","time":1457746},{"__typename":"Cue","text":"And I was like, well, I like total fanboy\nhere. I'd love to join that call.","time":1461663},{"__typename":"Cue","text":"And I was so nervous that he was\ngoing to say, this is terrible.","time":1465454},{"__typename":"Cue","text":"This is like a really sad thing\nfor human creativity.","time":1468246},{"__typename":"Cue","text":"Like, you know, why are you doing this?\nThis is like whatever.","time":1470579},{"__typename":"Cue","text":"And he was so excited. And he's like,\nthis has been so inspiring.","time":1474454},{"__typename":"Cue","text":"I want to do a new album with this.","time":1477579},{"__typename":"Cue","text":"You know, it's like, give\nme all these new ideas.","time":1479371},{"__typename":"Cue","text":"It's making me much better at my job.","time":1481038},{"__typename":"Cue","text":"I'm going to make better music\nbecause of this tool.","time":1482704},{"__typename":"Cue","text":"And that was awesome. And I hope that's\nhow it all continues to go.","time":1485371},{"__typename":"Cue","text":"And I think it is going to lead to this.","time":1488579},{"__typename":"Cue","text":"We see a similar thing now with Dolly,","time":1490371},{"__typename":"Cue","text":"where graphic designers sometimes tell us\nthat they just they see this new set","time":1492121},{"__typename":"Cue","text":"of possibilities because there's\nnew creative inspiration","time":1496121},{"__typename":"Cue","text":"and they're cycle time, like the amount\nof time it takes to just come up with","time":1498996},{"__typename":"Cue","text":"an idea and be able to look\nat it and then decide","time":1502871},{"__typename":"Cue","text":"whether to go down that path or head in a\ndifferent direction goes down so much.","time":1505704},{"__typename":"Cue","text":"And so I think it's going to just be this\nlike incredible creative explosion","time":1510079},{"__typename":"Cue","text":"for humans.","time":1513163},{"__typename":"Cue","text":"And how far away are we some before?","time":1514746},{"__typename":"Cue","text":"And I it comes up with a genuinely\npowerful new idea,","time":1518163},{"__typename":"Cue","text":"an idea that solves the problem that\nhumans have been wrestling with.","time":1524704},{"__typename":"Cue","text":"It doesn't have to be as quite\non the scale as of, OK,","time":1528746},{"__typename":"Cue","text":"we've got a virus coming. Please\ndescribe to us what a what","time":1532829},{"__typename":"Cue","text":"a national rational response\nshould look like,","time":1536246},{"__typename":"Cue","text":"but some kind of genuinely\ninnovative idea","time":1540746},{"__typename":"Cue","text":"or solution like one one internal question\nwe've asked ourselves is,","time":1544454},{"__typename":"Cue","text":"when will the first genuinely interesting,\npurely AI written TED talk show up?","time":1549538},{"__typename":"Cue","text":"I think that's a great milestone.","time":1555579},{"__typename":"Cue","text":"I will say it's always hard to guess\ntimeline's I'm sure I'll be wrong","time":1557038},{"__typename":"Cue","text":"on this, but I would guess the\nfirst genuinely interesting.","time":1560788},{"__typename":"Cue","text":"Ted talk, thought of written delivered\nby an AIDS within the kind of","time":1566329},{"__typename":"Cue","text":"the seven ish year time frame.\nMaybe a little bit less.","time":1571204},{"__typename":"Cue","text":"And it feels like I mean, just reading\nthat Guardian essay that was kind of it","time":1577204},{"__typename":"Cue","text":"was a composite of several different GPG\nthree responses to questions about,","time":1581913},{"__typename":"Cue","text":"you know, the threats of\nrobotics or whatever.","time":1588038},{"__typename":"Cue","text":"If you throw in a human\neditor into the mix,","time":1590704},{"__typename":"Cue","text":"you could probably imagine something much\nsooner. Indeed. Like tomorrow. Yeah.","time":1594329},{"__typename":"Cue","text":"So the hybrid the hybrid version where\nit's basically a tool assisted TED talk,","time":1598621},{"__typename":"Cue","text":"but that it is better than any TED talk","time":1604413},{"__typename":"Cue","text":"a human could generate in one\nhundred hours or whatever,","time":1606663},{"__typename":"Cue","text":"if you can sort of combine human\ndiscretion with A.I. horsepower.","time":1609454},{"__typename":"Cue","text":"I suspect that's like our next year","time":1614371},{"__typename":"Cue","text":"or two years from now kind of thing\nwhere it's just really quite good.","time":1616246},{"__typename":"Cue","text":"That's that's really interesting. How do\nyou view the impact of A.I. on jobs?","time":1621454},{"__typename":"Cue","text":"There's obviously been the familiar story\nis that every White-Collar job is now","time":1629371},{"__typename":"Cue","text":"up for destruction. What's\nwhat's your view there?","time":1634746},{"__typename":"Cue","text":"You know, it's I think it's always\nhard to make these predictions.","time":1640371},{"__typename":"Cue","text":"That is definitely the familiar story now.","time":1644746},{"__typename":"Cue","text":"Five years ago, it was every blue collar\njob is up for destruction,","time":1649246},{"__typename":"Cue","text":"maybe like last year it was.","time":1653788},{"__typename":"Cue","text":"Every creative job is up for destruction\nbecause of things like Jukebox I.","time":1655246},{"__typename":"Cue","text":"I think there will be an\nenormous impact on.","time":1661996},{"__typename":"Cue","text":"The job market, and I really hate it,","time":1666788},{"__typename":"Cue","text":"I think it's kind of gross when\npeople like working on","time":1669371},{"__typename":"Cue","text":"I pretend like there's not going\nto be or sort of say, oh,","time":1671788},{"__typename":"Cue","text":"don't worry about it. It'll\njust all obviously better.","time":1674538},{"__typename":"Cue","text":"It doesn't always obviously get better.\nI think what is true is.","time":1676579},{"__typename":"Cue","text":"Every technological revolution\nproduces a change in jobs,","time":1682496},{"__typename":"Cue","text":"we always find new ones, at least so far.","time":1686579},{"__typename":"Cue","text":"It's difficult to predict from\nwhere we're sitting now what","time":1689621},{"__typename":"Cue","text":"the new ones will be and this\ntechnological revolution is likely to be.","time":1692871},{"__typename":"Cue","text":"Again, it's always tempting to\nsay this time it's different.","time":1698663},{"__typename":"Cue","text":"Maybe I'll be totally wrong.","time":1700746},{"__typename":"Cue","text":"But from what I see now, this\ntechnological revolution is likely to be","time":1701788},{"__typename":"Cue","text":"more. Dramatic. More of a\nstaccato note than most,","time":1705996},{"__typename":"Cue","text":"and I think we as a society need to figure\nout how we're going to cushion","time":1713746},{"__typename":"Cue","text":"everybody through that. I've got my\nown ideas about how to do that.","time":1718454},{"__typename":"Cue","text":"I, I wouldn't say that I have any reason\nto believe they're the right ones,","time":1721996},{"__typename":"Cue","text":"but doing nothing and not\nreally engaging with","time":1726954},{"__typename":"Cue","text":"the magnitude of what's about to happen,","time":1731038},{"__typename":"Cue","text":"I think it's like not an\nacceptable answer.","time":1733163},{"__typename":"Cue","text":"So there's going to be huge impact.","time":1736246},{"__typename":"Cue","text":"It's difficult to predict where\nit shows up the most.","time":1738246},{"__typename":"Cue","text":"I think previous predictions\nhave mostly been wrong,","time":1740704},{"__typename":"Cue","text":"but I I'd like to see us all as a society,\ncertainly as a field,","time":1743746},{"__typename":"Cue","text":"engage in what what the shifts\nwe want to make to","time":1750163},{"__typename":"Cue","text":"the social contract are to kind\nof get through that in","time":1754121},{"__typename":"Cue","text":"a way that is maximally beneficial\nto everybody.","time":1756746},{"__typename":"Cue","text":"I mean, in every past revolution,","time":1760413},{"__typename":"Cue","text":"there's always been a space\nfor humans to move to.","time":1762663},{"__typename":"Cue","text":"That is, if you like, moving\nup the food chain,","time":1766038},{"__typename":"Cue","text":"it's sort of we've retreated to the things\nthat humans could uniquely do,","time":1768746},{"__typename":"Cue","text":"think better, be more creative\nand so forth.","time":1773329},{"__typename":"Cue","text":"I guess the worry about A.I. is that\nin principle, I believe this,","time":1775621},{"__typename":"Cue","text":"that there is no human cognitive feat\nthat won't ultimately be doable,","time":1779954},{"__typename":"Cue","text":"probably better by artificial\ngeneral touch,","time":1788454},{"__typename":"Cue","text":"simply because of the extra firepower\nthat ultimately they can have,","time":1790913},{"__typename":"Cue","text":"the vast knowledge they bring\nto the table and so forth.","time":1795413},{"__typename":"Cue","text":"Is that basically right, that there is\nultimately no safe sort of space where","time":1799954},{"__typename":"Cue","text":"we can say, oh, but that would\nnever be able to do that","time":1805454},{"__typename":"Cue","text":"on a very long time horizon? I agree with\nyou, but that's such a long time horizon.","time":1807788},{"__typename":"Cue","text":"I think that, you know, like maybe\nwe've merged by that point,","time":1813579},{"__typename":"Cue","text":"like maybe we're all plugged\nin and then, like,","time":1817913},{"__typename":"Cue","text":"we're this sort of symbiotic thing.","time":1821204},{"__typename":"Cue","text":"Like, I think there's an\ninteresting example,","time":1824329},{"__typename":"Cue","text":"as we were talking about\na few minutes ago,","time":1826371},{"__typename":"Cue","text":"where right now we have these systems that\nhave sort of enormous horsepower","time":1828496},{"__typename":"Cue","text":"but no steering wheel. It's like, you know\n, incredible capabilities,","time":1832788},{"__typename":"Cue","text":"but no judgment. And there's like these\nobvious ways in which today even","time":1836371},{"__typename":"Cue","text":"a human plus three is far better\nthan either on their own.","time":1840454},{"__typename":"Cue","text":"Many people speak about a world\nwhere it's sort of A.I.","time":1845913},{"__typename":"Cue","text":"as this external threat you speak about.","time":1849454},{"__typename":"Cue","text":"At some point, we actually merge\nwith eyes in some way.","time":1852829},{"__typename":"Cue","text":"What do you mean by that?","time":1856704},{"__typename":"Cue","text":"There's a lot of different versions of\nwhat I think is possible there, you know,","time":1859496},{"__typename":"Cue","text":"in some sense, I'd argue the merge\nhas already like begun","time":1863579},{"__typename":"Cue","text":"the human technology merge like we have\nthis thing in our hands that sort of","time":1866329},{"__typename":"Cue","text":"dictates a lot of what we think,","time":1869996},{"__typename":"Cue","text":"but it gives us real superpowers and\nthat can go much, much further.","time":1871246},{"__typename":"Cue","text":"Maybe it goes all the way to like the\nElon Musk vision of neuro link","time":1876079},{"__typename":"Cue","text":"and having our brains plugged\ninto computers","time":1879871},{"__typename":"Cue","text":"and sort of like literally we have a\ncomputer on the back of our head","time":1881788},{"__typename":"Cue","text":"or goes the other direction and\nwe get uploaded into one.","time":1885454},{"__typename":"Cue","text":"Or maybe it's just that we all have","time":1888204},{"__typename":"Cue","text":"a chat bot that kind of\nconstantly steers us","time":1889579},{"__typename":"Cue","text":"and helps us make better decisions\nthan we could.","time":1893038},{"__typename":"Cue","text":"But in any case, I think the fundamental\nthing is it's not like the humans versus","time":1895954},{"__typename":"Cue","text":"the eyes competing to be the. Smartest\nsentient thing on earth or beyond.","time":1899871},{"__typename":"Cue","text":"But it's that this idea of\nbeing on the same team.","time":1906871},{"__typename":"Cue","text":"Hmm. I certainly get very excited by the\nsort of the medium term potential","time":1910121},{"__typename":"Cue","text":"for creative people of all sorts if\nthey're willing to expand their palette","time":1916038},{"__typename":"Cue","text":"of possibilities. But with the\nuse of A.I. to be willing to.","time":1920621},{"__typename":"Cue","text":"I mean, the one thing that the history\nof technology has shown again","time":1924163},{"__typename":"Cue","text":"and again is that something this powerful\nand with this much benefit is unstoppable","time":1927038},{"__typename":"Cue","text":"and you will get rewarded for embracing\nit the most and the earliest.","time":1933163},{"__typename":"Cue","text":"So talk about what can go wrong with that,","time":1953663},{"__typename":"Cue","text":"so let's move away from just the sort\nof economic displacement factor.","time":1957829},{"__typename":"Cue","text":"You were a co-founder of Open Eye because\nyou saw existential risks to","time":1962996},{"__typename":"Cue","text":"humanity from high today. What\nwould you put as the sort of","time":1970288},{"__typename":"Cue","text":"the most worrying of those risks? And\nhow is open eye working to minimize?","time":1974746},{"__typename":"Cue","text":"I still think all of the really\nhorrifying risks exist.","time":1982329},{"__typename":"Cue","text":"I am more confident, much\nmore confident than","time":1985621},{"__typename":"Cue","text":"I was five years ago when we started that\nthere are technical things we can do","time":1989663},{"__typename":"Cue","text":"about. How we build these systems\nand the research and","time":1996663},{"__typename":"Cue","text":"the alignment that make us much\nmore likely to end up in","time":2000579},{"__typename":"Cue","text":"the kind of really wonderful camp, but,\nyou know,","time":2003913},{"__typename":"Cue","text":"like maybe open I fall behind","time":2007579},{"__typename":"Cue","text":"and maybe somebody else feels ajai that\nthinks about it in a very different way","time":2010246},{"__typename":"Cue","text":"or doesn't care as much as we'd like\nabout safety and the risks","time":2013871},{"__typename":"Cue","text":"or how to strike a different trade off\nof how fast we should go with this","time":2017579},{"__typename":"Cue","text":"and where we should sort of\njust say, like, you know,","time":2021204},{"__typename":"Cue","text":"like let's push on for the\neconomic benefits.","time":2023496},{"__typename":"Cue","text":"But I think all of this sort\nof like, you know,","time":2026329},{"__typename":"Cue","text":"traditionally what's been in the\nrealm of sci fi risks are real","time":2028746},{"__typename":"Cue","text":"and we should not ignore them. And\nI still lose sleep over them.","time":2033329},{"__typename":"Cue","text":"And just to update people is artificial\ngeneral intelligence.","time":2036913},{"__typename":"Cue","text":"Right now, we have incredible examples of\npowerful AI operating on specific areas.","time":2041746},{"__typename":"Cue","text":"Ajai is the ability of a computer\nmind to connect the dots","time":2046871},{"__typename":"Cue","text":"and to make decisions at the same level\nof breadth that that humans have had.","time":2052538},{"__typename":"Cue","text":"What's your sort of elevator pitch on\nAjai about how to identify and how","time":2058621},{"__typename":"Cue","text":"to think of it? Yeah, I mean,","time":2063163},{"__typename":"Cue","text":"the way that I would say it is that for","time":2064038},{"__typename":"Cue","text":"a while we were in this world of like\nvery narrow A.I. , you know,","time":2066788},{"__typename":"Cue","text":"that could like classify images of cats or\nwhatever, more advanced stuff in that.","time":2070413},{"__typename":"Cue","text":"But that kind of thing. We are now in\nthe era of general purpose, AI,","time":2073704},{"__typename":"Cue","text":"where you have these systems that are\nstill very much imperfect tools,","time":2080038},{"__typename":"Cue","text":"but that can generalize. And one thing\nlike GPP three can write essays","time":2086288},{"__typename":"Cue","text":"and translate between languages\nand write computer code","time":2093788},{"__typename":"Cue","text":"and do very complicated search.","time":2097538},{"__typename":"Cue","text":"It's like a single model that understands\nenough of what's really going","time":2100288},{"__typename":"Cue","text":"on to do a broad array of tasks\nand learn new things quickly,","time":2103329},{"__typename":"Cue","text":"sort of like people can. And then\neventually we'll get to this other realm.","time":2107079},{"__typename":"Cue","text":"Some people call it ajai, some\npeople call ostler things.","time":2111829},{"__typename":"Cue","text":"But I think it implies that the systems\nare like to some degree self directed,","time":2115079},{"__typename":"Cue","text":"have some intentionality of their own","time":2120496},{"__typename":"Cue","text":"is a simple summary to say that,","time":2123371},{"__typename":"Cue","text":"like the fundamental risk is that there's","time":2125454},{"__typename":"Cue","text":"the potential with general artificial\nintelligence of","time":2129163},{"__typename":"Cue","text":"a sort of runaway effect of\nself-improvement that can happen far","time":2132079},{"__typename":"Cue","text":"faster than any kind of humans\ncan even keep up with,","time":2136454},{"__typename":"Cue","text":"so that the day after you get to ajai,","time":2139704},{"__typename":"Cue","text":"suddenly computers are thousands\nof times more advanced than us","time":2142954},{"__typename":"Cue","text":"and we have no way of controlling\nwhat they do with that power.","time":2148079},{"__typename":"Cue","text":"Yeah, and that is certainly\nin the risk space,","time":2154788},{"__typename":"Cue","text":"which is that we build this thing and\nat some point somewhat suddenly,","time":2157288},{"__typename":"Cue","text":"it's much more powerful than we are, we\nhaven't really done the full merge yet.","time":2163371},{"__typename":"Cue","text":"There's an event horizon there and\nit's sort of hard to see to","time":2168163},{"__typename":"Cue","text":"the other side of it. Again, lots of\nreasons to think it will go OK.","time":2170996},{"__typename":"Cue","text":"Lots of reasons to think we won't\neven get to that scenario.","time":2174538},{"__typename":"Cue","text":"But that is something that.","time":2177704},{"__typename":"Cue","text":"I don't think people should brush under\nthe rug as much as they do,","time":2180079},{"__typename":"Cue","text":"it's in the possibility space for sure,","time":2182579},{"__typename":"Cue","text":"and in the possibility subspace\nof that is one where, like,","time":2185038},{"__typename":"Cue","text":"we didn't actually do as good of a job\non the alignment work as we thought.","time":2188538},{"__typename":"Cue","text":"And this sort of child of\nhumanity kind of acts in","time":2192163},{"__typename":"Cue","text":"a very different way than we think.","time":2196663},{"__typename":"Cue","text":"A framework that I find useful is to sort\nof think about like a two by two matrix,","time":2198371},{"__typename":"Cue","text":"which is short timelines to ajai\nand long timelines to ajai and","time":2202746},{"__typename":"Cue","text":"a slow take off and a fast take\noff on the other axis.","time":2207704},{"__typename":"Cue","text":"And in the short timelines,\nfast take off quadrant,","time":2211663},{"__typename":"Cue","text":"which is not where I think\nwe're going to be.","time":2217204},{"__typename":"Cue","text":"But if we get there, I think there's\na lot of scenarios in","time":2218746},{"__typename":"Cue","text":"the direction that you are describing\nthat are worrisome.","time":2221496},{"__typename":"Cue","text":"And we would want to spend a\nlot of effort planning for.","time":2225038},{"__typename":"Cue","text":"I mean, the fact that a computer could\nstart editing its own code","time":2229121},{"__typename":"Cue","text":"and improving itself while we're asleep\nand you wake up in the morning","time":2233288},{"__typename":"Cue","text":"and it's got smarter, that is the start\nof something super powerful","time":2237288},{"__typename":"Cue","text":"and potentially scary.","time":2241246},{"__typename":"Cue","text":"I have tremendous misgivings about letting\nmy system, not one we have today,","time":2244454},{"__typename":"Cue","text":"but one that we might not have","time":2249413},{"__typename":"Cue","text":"and too many more years start editing its\nown code while we're not paying attention.","time":2250496},{"__typename":"Cue","text":"I think that's the kind\nof thing that is worth","time":2255288},{"__typename":"Cue","text":"a great deal of societal discussion about,\nyou know, just because we can do that.","time":2257246},{"__typename":"Cue","text":"Should we?","time":2261079},{"__typename":"Cue","text":"Yes, because one of the things that's\nthat's been most shocking to you about","time":2262788},{"__typename":"Cue","text":"the last few years has been just the\npower of unintended consequences.","time":2267454},{"__typename":"Cue","text":"It's like you don't have to have","time":2271329},{"__typename":"Cue","text":"a belief that there's some\nsort of waking up of of","time":2273371},{"__typename":"Cue","text":"an alien intelligence that suddenly\ndecided it wants to wreak havoc","time":2277163},{"__typename":"Cue","text":"on humans. That may never happen.","time":2281038},{"__typename":"Cue","text":"What you can have is just incredible\npower that goes amok.","time":2285746},{"__typename":"Cue","text":"So a lot of people would argue that\nwhat's happened in technology in","time":2290621},{"__typename":"Cue","text":"the last few years is actually\nan example of that.","time":2294079},{"__typename":"Cue","text":"You know, social media companies created\nthese intelligences that were programmed","time":2296121},{"__typename":"Cue","text":"to maximally harvest attention,\nfor example, for sure.","time":2302038},{"__typename":"Cue","text":"And they understand this from that turned\nout to be in some ways horrifying","time":2306663},{"__typename":"Cue","text":"and extraordinarily","time":2311454},{"__typename":"Cue","text":"damaging. Is that a meaningful sort of\ncanary in the coal mine saying, look out,","time":2313746},{"__typename":"Cue","text":"humanity, this could be really dangerous?","time":2318663},{"__typename":"Cue","text":"And how how on earth do you\nprotect against those","time":2320746},{"__typename":"Cue","text":"kinds of unintended consequences?","time":2325579},{"__typename":"Cue","text":"I think you raise a great\npoint in general,","time":2327454},{"__typename":"Cue","text":"which is these systems don't have to wish\nill to humanity to cause ill just when","time":2329454},{"__typename":"Cue","text":"you have, like, very powerful systems. I\nmean, unintended consequences for sure.","time":2333996},{"__typename":"Cue","text":"But another version of that is\nand I think this applies at","time":2338746},{"__typename":"Cue","text":"the technical level, at the company level,\nat the societal level,","time":2342288},{"__typename":"Cue","text":"incentives are superpower's.","time":2345663},{"__typename":"Cue","text":"Charlie Munger had this thing on,","time":2347913},{"__typename":"Cue","text":"which is incentives are so powerful that\nif you can spend any time whatsoever","time":2349246},{"__typename":"Cue","text":"working on the incentive system,","time":2353204},{"__typename":"Cue","text":"that's what you should do before\nyou work on anything else.","time":2355079},{"__typename":"Cue","text":"And I really believe that.","time":2357579},{"__typename":"Cue","text":"And I think that applies to the\nindividual models we build","time":2359163},{"__typename":"Cue","text":"and what their reward functions look like.","time":2362538},{"__typename":"Cue","text":"I think it applies to society\nin a big way,","time":2364579},{"__typename":"Cue","text":"and I think it applies to our\ncorporate structure at open.","time":2367788},{"__typename":"Cue","text":"I you know, we sort of observe that if\nyou have very well-meaning people,","time":2370829},{"__typename":"Cue","text":"but they have this incentive to sort\nof maximize attention harvesting","time":2375121},{"__typename":"Cue","text":"and profit forever through\nno one's ill intentions,","time":2378496},{"__typename":"Cue","text":"that leads to a quite undesirable outcome.","time":2382454},{"__typename":"Cue","text":"And so we set up opening is this thing\ncalled a capped profit model specifically","time":2386163},{"__typename":"Cue","text":"so that we don't have the system incentive\nto just generate maximum value","time":2390496},{"__typename":"Cue","text":"forever with an AGI that seems\nlike obviously quite broken.","time":2394996},{"__typename":"Cue","text":"But even though we knew that was bad","time":2398413},{"__typename":"Cue","text":"and even though we all like to think\nof ourselves as good people,","time":2399788},{"__typename":"Cue","text":"it took us a long time to figure\nout the right structure,","time":2402454},{"__typename":"Cue","text":"to figure out a charter that's\ngoing to govern us and","time":2406204},{"__typename":"Cue","text":"a set of incentives that we believe\nwill let us do our work.","time":2409121},{"__typename":"Cue","text":"And kind of these we have these like\nthree elements that we talk about","time":2413288},{"__typename":"Cue","text":"a lot research sort of engineering,","time":2415871},{"__typename":"Cue","text":"development and deployment\npolicy and safety.","time":2418496},{"__typename":"Cue","text":"Put those all together under a system\nwhere you don't have to rely on.","time":2421746},{"__typename":"Cue","text":"Anything but the natural\nincentives to push in","time":2428954},{"__typename":"Cue","text":"a direction that we hope will minimize","time":2432079},{"__typename":"Cue","text":"the sort of negative unintended\nconsequences.","time":2434788},{"__typename":"Cue","text":"So help me understand this,","time":2439121},{"__typename":"Cue","text":"because this is I think this is confusing\nto some people. So you started opening.","time":2440621},{"__typename":"Cue","text":"I initially I think Elon Musk,\nthe co-founder,","time":2446788},{"__typename":"Cue","text":"and there was a group of you and","time":2449329},{"__typename":"Cue","text":"the argument was this technology\nis too powerful to be left,","time":2451329},{"__typename":"Cue","text":"developed in secret and to be left\ndeveloped purely by corporations who have","time":2456454},{"__typename":"Cue","text":"whatever incentive they may have.","time":2463788},{"__typename":"Cue","text":"We need a nonprofit that will develop\nand share knowledge openly.","time":2465454},{"__typename":"Cue","text":"First of all, just even\nat that early stage,","time":2472329},{"__typename":"Cue","text":"some people were confused about this.","time":2474788},{"__typename":"Cue","text":"It was saying if this thing\nis so dangerous,","time":2476246},{"__typename":"Cue","text":"why on earth would you want to","time":2479413},{"__typename":"Cue","text":"make it secrets even more available?","time":2484246},{"__typename":"Cue","text":"Well, maybe giving the tools to that sort\nof AI terrorist in his bedroom somewhere,","time":2486829},{"__typename":"Cue","text":"I think I think we got misunderstood in\nthe way we were talking about that.","time":2492913},{"__typename":"Cue","text":"We certainly don't think that the\nright thing to do is to, like,","time":2497163},{"__typename":"Cue","text":"build this a super weapon and hand it to\na terrorist. That's obviously awful.","time":2501538},{"__typename":"Cue","text":"One of the reasons that we like our\nAPI model is it lets us make","time":2506121},{"__typename":"Cue","text":"the most powerful AI technology anyone\nin the world has, as far as we know,","time":2510204},{"__typename":"Cue","text":"available to ever would like to use it,\nbut to put some controls on its usage.","time":2514996},{"__typename":"Cue","text":"And also, if we make a mistake,","time":2520454},{"__typename":"Cue","text":"to be able to pull it back or change it\nor tweak it or improve it or whatever.","time":2521829},{"__typename":"Cue","text":"But we do want to put and this is\ncontinued will continue to be true with","time":2525496},{"__typename":"Cue","text":"appropriate restrictions and guardrails,","time":2529788},{"__typename":"Cue","text":"very powerful technology in the hands\nof people. I think that is fair.","time":2532663},{"__typename":"Cue","text":"I think that will lead to the best results\nfor the society as a whole.","time":2536579},{"__typename":"Cue","text":"And I think it will sort\nof maximize benefit.","time":2540913},{"__typename":"Cue","text":"But that's very different than sort of\nshipping the whole model and saying, here,","time":2544163},{"__typename":"Cue","text":"do whatever you want with it. We're\nable to enforce rules on it.","time":2547579},{"__typename":"Cue","text":"We also think and this is part of\nthe mission that like something","time":2551829},{"__typename":"Cue","text":"the field was doing a lot of that we\ndidn't feel good about was sort of saying","time":2556204},{"__typename":"Cue","text":"like, oh, we're going to keep the pace\nof progress and capabilities secret.","time":2560329},{"__typename":"Cue","text":"That doesn't feel right, because\nI think we do need","time":2564329},{"__typename":"Cue","text":"a societal conversation about\nwhat's what's going on here,","time":2567038},{"__typename":"Cue","text":"what the impacts are going to be.","time":2570329},{"__typename":"Cue","text":"And so we although we don't always say,\nlike, you know, here's the super weapon,","time":2571746},{"__typename":"Cue","text":"hopefully we do try to say, like, this is\nreally serious. This is a big deal.","time":2576079},{"__typename":"Cue","text":"This is going to affect all of us.","time":2580746},{"__typename":"Cue","text":"We need to have a big conversation\nabout what to do with it.","time":2582246},{"__typename":"Cue","text":"Help me understand the structure\na bit better,","time":2586288},{"__typename":"Cue","text":"because you definitely surprised much\npeople when you announced that Microsoft","time":2588454},{"__typename":"Cue","text":"were putting a billion dollars into\nthe organization and in return,","time":2592871},{"__typename":"Cue","text":"I guess they get certain exclusive\nlicensing rights.","time":2600038},{"__typename":"Cue","text":"And so, for example, they are the\nexclusive licensee of CP3.","time":2603163},{"__typename":"Cue","text":"So talk about that structure\nof how you win.","time":2609288},{"__typename":"Cue","text":"Microsoft presumably have invested not\npurely for altruistic purposes.","time":2612163},{"__typename":"Cue","text":"They think that they will make money\non that billion dollars.","time":2616246},{"__typename":"Cue","text":"I sure hope they do. I love capitalism,","time":2620246},{"__typename":"Cue","text":"but I think that I really loved even\nmore about Microsoft as a partner.","time":2622663},{"__typename":"Cue","text":"And I'll talk about the structure\nand the exclusive license in","time":2626913},{"__typename":"Cue","text":"a minute is that we like went around\nto people that might find us.","time":2630371},{"__typename":"Cue","text":"And we said one of the things here is that\nwe're going to try to make you some","time":2633496},{"__typename":"Cue","text":"money. But like Adjei going\nwell is more important.","time":2636496},{"__typename":"Cue","text":"And we need you to sign this document\nthat says if things don't go","time":2639496},{"__typename":"Cue","text":"the way we think and we can't make you\nmoney like you just cheerfully walk away","time":2642079},{"__typename":"Cue","text":"from it and we do the right\nthing for humanity.","time":2646204},{"__typename":"Cue","text":"And they were like, yes, we are\nenthusiastic about that.","time":2648246},{"__typename":"Cue","text":"We get that the mission comes first here.","time":2650746},{"__typename":"Cue","text":"So again, I hope a phenomenal\ninvestment for them.","time":2652871},{"__typename":"Cue","text":"But they were like they really\npleasantly surprised us on","time":2656204},{"__typename":"Cue","text":"the upside of how aligned\nthey were with us,","time":2659079},{"__typename":"Cue","text":"about how strange the world may get here\nand the need for us to have flexibility","time":2660788},{"__typename":"Cue","text":"and put our mission first, even if that\nmeans they lose all their money,","time":2665413},{"__typename":"Cue","text":"which I hope they don't and\ndon't think they will.","time":2668496},{"__typename":"Cue","text":"So the way it's set up is that if at some\npoint in the coming year or two,","time":2671413},{"__typename":"Cue","text":"two years, Microsoft decide that there's\nsome incredible commercial opportunity","time":2674746},{"__typename":"Cue","text":"that they could realize out of the eye\nthat you've built and you feel actually,","time":2679871},{"__typename":"Cue","text":"no, that's that's damaging. You\ncan block it. You can veto it.","time":2683788},{"__typename":"Cue","text":"Correct. So the four most powerful\nversion of three","time":2686829},{"__typename":"Cue","text":"and its successors are available\nvia the API,","time":2690079},{"__typename":"Cue","text":"and we intend for that to continue.","time":2692579},{"__typename":"Cue","text":"What Microsoft has is the ability to sort\nof put that model directly into their","time":2695204},{"__typename":"Cue","text":"own technology. If they want to do that.","time":2699954},{"__typename":"Cue","text":"We don't plan to do that\nwith other people","time":2702288},{"__typename":"Cue","text":"because we can't have all these controls\nthat we talked about earlier.","time":2704246},{"__typename":"Cue","text":"But they're like a close trusted partner\nand they really care about safety, too.","time":2707746},{"__typename":"Cue","text":"But our goal is that anybody who\nwants to use the API can have","time":2711954},{"__typename":"Cue","text":"the most powerful versions\nof what we've trained.","time":2714788},{"__typename":"Cue","text":"And the structure of the API lets us\ncontinue to increase the safety","time":2717829},{"__typename":"Cue","text":"and fix problems when we find them.\nBut but the structure.","time":2722704},{"__typename":"Cue","text":"So we start out as a non-profit,\nas you said,","time":2726329},{"__typename":"Cue","text":"we realized pretty quickly that although\nwe went into this thinking that","time":2728871},{"__typename":"Cue","text":"the way to get to ajai would be about\nsmarter and smarter algorithms,","time":2733663},{"__typename":"Cue","text":"that we just needed bigger and\nbigger computers as well.","time":2736704},{"__typename":"Cue","text":"And that was going to require a scale\nof capital that no one will,","time":2739871},{"__typename":"Cue","text":"at least certainly not me, could figure\nout how to raise is a nonprofit.","time":2743163},{"__typename":"Cue","text":"We also needed to sort of be able to\ncompensate very highly compensated,","time":2746621},{"__typename":"Cue","text":"talented individuals that do this,","time":2750121},{"__typename":"Cue","text":"but are full for profit company had\nrunaway incentives problem,","time":2752746},{"__typename":"Cue","text":"among other things. Also just one about\nsort of fairness in society","time":2757454},{"__typename":"Cue","text":"and wealth concentration that didn't\nfeel right to us either.","time":2760746},{"__typename":"Cue","text":"And so we came up with this kind\nof hybrid where we have","time":2764454},{"__typename":"Cue","text":"a nonprofit that governs what we do,\nand it has a subsidiary, LLC,","time":2768204},{"__typename":"Cue","text":"that we structure in a way to make\na fixed amount of profit","time":2774454},{"__typename":"Cue","text":"so that all of our investors\nand employees,","time":2779163},{"__typename":"Cue","text":"hopefully if things go how we like,\nif not no one gets any money,","time":2780913},{"__typename":"Cue","text":"but hopefully they get to make this one\ntime great return on their investment","time":2784579},{"__typename":"Cue","text":"or the time that they spent\nit open their equity here.","time":2788913},{"__typename":"Cue","text":"And then beyond that, all the value\nflows back to the nonprofit","time":2792038},{"__typename":"Cue","text":"and we figure out how to share it as\nfairly as we can with the world.","time":2795788},{"__typename":"Cue","text":"And I think that this structure","time":2800121},{"__typename":"Cue","text":"and this nonprofit with this very\nstrong charter in place","time":2802538},{"__typename":"Cue","text":"and everybody who joins signing up\nfor the mission come in first and","time":2805038},{"__typename":"Cue","text":"the fact the world may get strange,\nI think that.","time":2808913},{"__typename":"Cue","text":"That was at least the best idea\nwe could come up with,","time":2813788},{"__typename":"Cue","text":"and I think it feels so far like the\nincentive system is working,","time":2816371},{"__typename":"Cue","text":"just as I sort of watch the way that\nwe and our partners make decisions.","time":2821038},{"__typename":"Cue","text":"But if I read it right, the cap on","time":2824621},{"__typename":"Cue","text":"the gain that investors can make is\n100 Axum. It's a massive call that","time":2827871},{"__typename":"Cue","text":"was for our very first round investors.\nIt's way, way lower.","time":2832163},{"__typename":"Cue","text":"Like as we now take a bit of capital,\nit's way, way lower.","time":2835704},{"__typename":"Cue","text":"So your deal with Microsoft\nisn't you can only make","time":2839413},{"__typename":"Cue","text":"the first hundred billion dollars.\nI don't know.","time":2842288},{"__typename":"Cue","text":"It's way lower than after that.\nWe're giving it to the world.","time":2843954},{"__typename":"Cue","text":"It's way lower than that.\nHave you disclosed","time":2846454},{"__typename":"Cue","text":"what I don't know if we have, so\nI won't accidentally do it now.","time":2848996},{"__typename":"Cue","text":"All right. OK, so explain a bit more about\nthe charter and how it is that you.","time":2853079},{"__typename":"Cue","text":"Hope to avoid or I guess help contribute\nto an eye that is safe for humanity.","time":2862704},{"__typename":"Cue","text":"What do you see as the keys to us\navoiding the worst mistakes","time":2870746},{"__typename":"Cue","text":"and really holding on to something that's\nthat's beneficial for humanity?","time":2876329},{"__typename":"Cue","text":"My answer there is actually more about,","time":2880996},{"__typename":"Cue","text":"like technical and societal\nissues than the charter.","time":2885829},{"__typename":"Cue","text":"So if it's OK for me to answer it\nfrom that perspective, sure.","time":2889246},{"__typename":"Cue","text":"OK, I'm happy to talk about\nthe charter to.","time":2892496},{"__typename":"Cue","text":"I think this question of alignment\nthat we talked about","time":2897246},{"__typename":"Cue","text":"a little earlier is paramount,","time":2901704},{"__typename":"Cue","text":"and then I think to understand that it's\nuseful to differentiate between","time":2904329},{"__typename":"Cue","text":"accidental misuse of a system and\nintentional misuse of a system.","time":2909746},{"__typename":"Cue","text":"So like intentional would be a bad actor\nsaying, I've got this powerful system,","time":2915871},{"__typename":"Cue","text":"I'm going to use it to like hack into\nall the computers in the world","time":2920496},{"__typename":"Cue","text":"and wreak havoc on the power grids.","time":2923079},{"__typename":"Cue","text":"And accidental would be kind of the Nick\nBostrom make a lot of paper clips","time":2925496},{"__typename":"Cue","text":"and view humans as collateral\ndamage in both cases.","time":2930663},{"__typename":"Cue","text":"But to varying degrees, if\nwe can really, truly,","time":2935038},{"__typename":"Cue","text":"technically solve the alignment\nproblem and","time":2939413},{"__typename":"Cue","text":"the societal problem of deciding to which\nset of human values do we align,","time":2942663},{"__typename":"Cue","text":"then the systems understand\nright and wrong,","time":2948163},{"__typename":"Cue","text":"and they understand probably\nbetter than we ever can,","time":2951038},{"__typename":"Cue","text":"unintended consequences from complex\nactions and very complex systems.","time":2954121},{"__typename":"Cue","text":"And, you know, if we can train\na system which is like.","time":2959079},{"__typename":"Cue","text":"Don't harm humanity and the system can\nreally understand what we mean when we","time":2963871},{"__typename":"Cue","text":"say that, again, who is we and what does\nthat have some asterisks on them?","time":2969121},{"__typename":"Cue","text":"Sorry, go ahead.","time":2973704},{"__typename":"Cue","text":"Well, that's if they could understand\nwhat it means to not harm humanity,","time":2974788},{"__typename":"Cue","text":"that there's a lot wrapped\nup in that sentence.","time":2981579},{"__typename":"Cue","text":"Because what's been so striking\nto me about efforts","time":2984538},{"__typename":"Cue","text":"so far is that they seem\nto have been based on","time":2987371},{"__typename":"Cue","text":"a very naive view of human nature.","time":2990538},{"__typename":"Cue","text":"Go back to the sort of Facebook\nand Twitter examples of, well,","time":2992663},{"__typename":"Cue","text":"the engineers building some of","time":2996246},{"__typename":"Cue","text":"the systems would say we've just designed\nthem around what humans want to do.","time":2997413},{"__typename":"Cue","text":"You said, well, if someone wants\nto click on something,","time":3002121},{"__typename":"Cue","text":"we will give them more of that thing. And\nwhat could possibly be wrong with that?","time":3004829},{"__typename":"Cue","text":"We're just supporting human choice,","time":3009329},{"__typename":"Cue","text":"ignoring the fact that humans are\ncomplicated, farshid animals for sure,","time":3011288},{"__typename":"Cue","text":"who are constantly making choices,","time":3016413},{"__typename":"Cue","text":"that a more effective version of\nthemselves would agree is not in their","time":3018371},{"__typename":"Cue","text":"long term interests. So\nthat's one part of it.","time":3023079},{"__typename":"Cue","text":"And then you've got layered\non top of that or","time":3025454},{"__typename":"Cue","text":"the complications of systemic\ncomplexity where, you know,","time":3027788},{"__typename":"Cue","text":"multiple choices by thousands\nof people end up creating","time":3034788},{"__typename":"Cue","text":"a reality that possibly have designed\nfor how how to cut through that.","time":3039038},{"__typename":"Cue","text":"Like an AI has to make a decision based\non a moment, on a specific data set.","time":3045996},{"__typename":"Cue","text":"As those decisions get more powerful,","time":3052329},{"__typename":"Cue","text":"how can we be confident that they don't\nlead to this sort of system crashing","time":3054704},{"__typename":"Cue","text":"basically in some way?","time":3060204},{"__typename":"Cue","text":"I think that I've heard a lot\nof behavioral psychologists","time":3063163},{"__typename":"Cue","text":"and other people that have studied\nthis say in different ways,","time":3066996},{"__typename":"Cue","text":"are that I hate to keep\npicking on Facebook,","time":3070496},{"__typename":"Cue","text":"but we can do it one more time\nsince we're on the topic.","time":3072496},{"__typename":"Cue","text":"Maybe you can't in any given moment in\nnight where you're tired and you have","time":3075371},{"__typename":"Cue","text":"a stressful day, stop yourself from\nthe dopamine hit of scrolling","time":3079538},{"__typename":"Cue","text":"and Instagram, even though you\nknow that's bad for you","time":3084038},{"__typename":"Cue","text":"and it's not leading to your best life.","time":3087204},{"__typename":"Cue","text":"But if you were asked in a reflective\nmoment where you were sort of fully alert","time":3089288},{"__typename":"Cue","text":"and thoughtful, do you want\nto spend as much time","time":3093204},{"__typename":"Cue","text":"as you do scrolling through Instagram?\nDoes it make you happier or not?","time":3095913},{"__typename":"Cue","text":"You would actually be able to give\nlike the right long term answer?","time":3099704},{"__typename":"Cue","text":"It's sort of the spirit is willing, but\nthe flesh is weak kind of moment.","time":3103663},{"__typename":"Cue","text":"And one thing that I am hopeful is that\nhumans do know what we want and what.","time":3107079},{"__typename":"Cue","text":"On the whole, and presented\nwith research or sort of","time":3114954},{"__typename":"Cue","text":"an objective view about what makes us\nhappy and doesn't we're pretty,","time":3117788},{"__typename":"Cue","text":"what's so great about it,\nthey're pretty good.","time":3121413},{"__typename":"Cue","text":"But in any particular moment,","time":3123538},{"__typename":"Cue","text":"we are subjected to our animal\ninstincts and it is easy for","time":3126288},{"__typename":"Cue","text":"the lower brain to take over the eye. Well\n, I think be an even higher brain.","time":3131788},{"__typename":"Cue","text":"And as we can teach it, you know,\nhere is what we really do value.","time":3137996},{"__typename":"Cue","text":"Here's what we really do want.","time":3142288},{"__typename":"Cue","text":"It will help us make better decisions\nthan we are capable of,","time":3143454},{"__typename":"Cue","text":"even in our best moments.","time":3146746},{"__typename":"Cue","text":"So is that being proposed and talked\nabout as an actual rule?","time":3149704},{"__typename":"Cue","text":"Because it strikes me that there is\nsomething potentially super profound here","time":3153246},{"__typename":"Cue","text":"to introduce some kind of rule","time":3157663},{"__typename":"Cue","text":"for development of AIDS that\nthey have to tap into not.","time":3161621},{"__typename":"Cue","text":"What humans one, which is\nan ill defined question,","time":3166871},{"__typename":"Cue","text":"but as to what humans in reflective mode\nwant. Yeah, we talk about this a lot.","time":3170163},{"__typename":"Cue","text":"I mean, do you see a real chance where\nsomething like that could be incorporated","time":3176496},{"__typename":"Cue","text":"as a sort of an absolute golden\nrule and and if you like,","time":3181746},{"__typename":"Cue","text":"spread around the community so that it\nseeps into corporations and elsewhere?","time":3184996},{"__typename":"Cue","text":"Because that I've seen no\nevidence that, well,","time":3191413},{"__typename":"Cue","text":"a little corporation that was\npotentially a game changer.","time":3194496},{"__typename":"Cue","text":"Corporations have this weird\nincentive problem. Right.","time":3197204},{"__typename":"Cue","text":"What I was trying to speak\nabout was something that","time":3201121},{"__typename":"Cue","text":"I think should be technologically possible\n,","time":3202996},{"__typename":"Cue","text":"and that's something that we\nas a society should demand.","time":3205163},{"__typename":"Cue","text":"And I think it is technically possible for\nthis to be sort of like a layer above","time":3208996},{"__typename":"Cue","text":"the neocortex that makes even better\ndecisions for us and our welfare","time":3213413},{"__typename":"Cue","text":"and our long term happiness and\nfulfillment than we could make","time":3216788},{"__typename":"Cue","text":"on our own. And I think it is possible\nfor us as a society to demand that.","time":3219371},{"__typename":"Cue","text":"And if we can do like a pincer move\nbetween what the technology is capable of","time":3223788},{"__typename":"Cue","text":"and what we what we as society demand,","time":3228038},{"__typename":"Cue","text":"maybe we can make everybody\nin the middle that way.","time":3230163},{"__typename":"Cue","text":"I mean, there are instances of even though\ncompanies have their incentives to","time":3234496},{"__typename":"Cue","text":"make money and so forth, they\nalso in the knowledge age.","time":3238663},{"__typename":"Cue","text":"Can't make money if they have pissed\noff too many of their employees","time":3244288},{"__typename":"Cue","text":"and customers and investors by analogy\nof the climate space right now,","time":3249121},{"__typename":"Cue","text":"you can see more and more companies,","time":3253996},{"__typename":"Cue","text":"even those that are emitting huge amounts\nof carbon dioxide, saying, wait a sec,","time":3255371},{"__typename":"Cue","text":"we're struggling to recruit talented\npeople because they don't want to work","time":3259454},{"__typename":"Cue","text":"for someone who's evil. And their\ncustomers are saying,","time":3262704},{"__typename":"Cue","text":"we don't want to buy something\nthat is evil.","time":3264996},{"__typename":"Cue","text":"And so, you know, ultimately you can\npicture processes where they do better.","time":3267454},{"__typename":"Cue","text":"And I I believe that most engineers, for\nexample, work in Silicon Valley.","time":3273329},{"__typename":"Cue","text":"Companies are actually good people\nwho want to design great products","time":3279413},{"__typename":"Cue","text":"for humanity. I think that the people\nwho run these companies want to be","time":3283954},{"__typename":"Cue","text":"a net contribution to humanity.","time":3287246},{"__typename":"Cue","text":"It's we've we've rushed really quickly","time":3289038},{"__typename":"Cue","text":"and design stuff without thinking\nit through properly.","time":3291788},{"__typename":"Cue","text":"And it's led to a mess up.","time":3293996},{"__typename":"Cue","text":"So it's like, OK, don't move fast,\nbreak things,","time":3295954},{"__typename":"Cue","text":"slow down and build beautiful\nthings that are built on","time":3298913},{"__typename":"Cue","text":"a real version of human nature and on\na real version of system complexity","time":3302371},{"__typename":"Cue","text":"and the risks associated with\nsystemic complexity.","time":3308913},{"__typename":"Cue","text":"Is that the agenda that fundamentally\nyou think that you can push somehow?","time":3312538},{"__typename":"Cue","text":"Yes, but I think the way we can push it is\nby getting the incentive system right.","time":3316538},{"__typename":"Cue","text":"I think most people are fundamentally\nextremely good.","time":3321538},{"__typename":"Cue","text":"Very few people wake up in the morning\nthinking about how can I make the world","time":3325204},{"__typename":"Cue","text":"a worse place? But the incentive systems\nthat we're in are so powerful.","time":3328079},{"__typename":"Cue","text":"And even those engineers who join with","time":3333163},{"__typename":"Cue","text":"the absolute best of intentions get sucked\ninto this world where they're like","time":3335163},{"__typename":"Cue","text":"trying to go up from it all for and five\nor whatever Facebook calls those things","time":3339413},{"__typename":"Cue","text":"and you like, it's pretty exciting.","time":3344163},{"__typename":"Cue","text":"You get caught up playing the game,","time":3346288},{"__typename":"Cue","text":"you're rewarded for kind of doing things\nthat move the company's key metrics.","time":3347704},{"__typename":"Cue","text":"It's like fun to get promoted.","time":3351538},{"__typename":"Cue","text":"It feels good to make more money and the\nincentive systems of the company.","time":3352996},{"__typename":"Cue","text":"And that's what it rewards.","time":3358704},{"__typename":"Cue","text":"An individual performance are maybe\nlike not what we all want.","time":3359788},{"__typename":"Cue","text":"And here I don't want to pick\non Facebook at all because","time":3363871},{"__typename":"Cue","text":"I think there's versions of this at play\nit like every big tech company,","time":3365829},{"__typename":"Cue","text":"including in some ways I'm\nsure it open I but to","time":3368829},{"__typename":"Cue","text":"the degree that we can better align\nthe incentives of companies with","time":3372329},{"__typename":"Cue","text":"the welfare of society and\nthen the incentives of","time":3377204},{"__typename":"Cue","text":"an individual at those companies within\nthe now realign incentives","time":3379579},{"__typename":"Cue","text":"for those companies, the more likely we\nare to be able to have things like ajai","time":3383246},{"__typename":"Cue","text":"that. Follow an incentive system of.","time":3389121},{"__typename":"Cue","text":"What we want in our most\nreflective best moments","time":3394954},{"__typename":"Cue","text":"and are even better than what\nwe could think of ourselves","time":3397663},{"__typename":"Cue","text":"is is it still the vision for open\neye that you will get to?","time":3400954},{"__typename":"Cue","text":"Artificial general intelligence ahead of.","time":3407413},{"__typename":"Cue","text":"The corporations, so that you can somehow\nput a stake in the ground","time":3411496},{"__typename":"Cue","text":"and build it the right way.","time":3416204},{"__typename":"Cue","text":"Is that really a realistic\nthing to to dream for?","time":3419746},{"__typename":"Cue","text":"And if not, how do you live\nup to the mission","time":3422579},{"__typename":"Cue","text":"and help ensure that this thing\ndoesn't go off the rails?","time":3425121},{"__typename":"Cue","text":"I think it is. Look, I certainly don't\nthink we will be the only group to build","time":3427996},{"__typename":"Cue","text":"an AGI, but I think we could be the first.","time":3432496},{"__typename":"Cue","text":"And I think if you are the first, you\nhave a lot of norms that empower.","time":3435371},{"__typename":"Cue","text":"And I think you've already seen that.","time":3439246},{"__typename":"Cue","text":"You know, we have released some of the\nmost powerful systems to date.","time":3440538},{"__typename":"Cue","text":"And I think the way that we have done that\nkind of in controlled release where","time":3444496},{"__typename":"Cue","text":"we've released a bigger model than\na bigger one than a bigger one,","time":3448704},{"__typename":"Cue","text":"and we sort of try and talk about\nthe potential misuse cases","time":3450913},{"__typename":"Cue","text":"and we try to like talk about the\nimportance of releasing this behind","time":3454121},{"__typename":"Cue","text":"an API so that you can make changes.","time":3456871},{"__typename":"Cue","text":"Other groups have followed suit\nin some of those directions,","time":3459496},{"__typename":"Cue","text":"and I think that's good. So, yes,","time":3462704},{"__typename":"Cue","text":"I don't think we can be the only\nI do think we can be ahead.","time":3464579},{"__typename":"Cue","text":"And if we are ahead, I think we can use\nthat leverage to hopefully push people","time":3467413},{"__typename":"Cue","text":"in a better direction or maybe we're\nwrong and somebody else has","time":3472163},{"__typename":"Cue","text":"a better direction. We're\ndoing something about","time":3475038},{"__typename":"Cue","text":"do you have a structural advantage in\nthat your mission is to do this","time":3477871},{"__typename":"Cue","text":"for everyone as opposed to for\nsome corporate objective.","time":3481954},{"__typename":"Cue","text":"And that that that allows you that.","time":3486621},{"__typename":"Cue","text":"Why is it that we came out of open\neye and not someone else?","time":3489454},{"__typename":"Cue","text":"It's like it's surprising in some ways\nwhen you're up against so much money","time":3492704},{"__typename":"Cue","text":"and so much talent in these other\ncompanies that you came up with this","time":3496663},{"__typename":"Cue","text":"platform ahead of. You know, in","time":3500746},{"__typename":"Cue","text":"some sense it's surprising\nand in some sense,","time":3501788},{"__typename":"Cue","text":"like the startup wins most of the time,","time":3504038},{"__typename":"Cue","text":"like I'm a huge believer in\nstartups as the best force","time":3505913},{"__typename":"Cue","text":"for innovation we have in the world today.","time":3508913},{"__typename":"Cue","text":"I talked a little bit about how\nwe combine these three.","time":3511621},{"__typename":"Cue","text":"Different clans of research,","time":3516579},{"__typename":"Cue","text":"engineering and sort of safety","time":3518579},{"__typename":"Cue","text":"and policy that don't normally combine\nwell and I think we have","time":3519913},{"__typename":"Cue","text":"an unusual strength, there were\nclearly like well funded.","time":3524704},{"__typename":"Cue","text":"We have super talented people.","time":3527996},{"__typename":"Cue","text":"But what we really have\nis like intense focus","time":3529663},{"__typename":"Cue","text":"and self belief that what we're\ndoing is possible and good.","time":3532538},{"__typename":"Cue","text":"And I appreciate the implied compliment.","time":3535246},{"__typename":"Cue","text":"But, you know, we, like, work really hard.","time":3538704},{"__typename":"Cue","text":"And if we stop doing that, I'm sure\nsomeone would run by us fast.","time":3540996},{"__typename":"Cue","text":"Tell us a bit more about some of\nyour prior life sentences.","time":3545371},{"__typename":"Cue","text":"Yeah, for several years, you\nwere running Y Combinator,","time":3548371},{"__typename":"Cue","text":"which had incredible impact\non some 70 companies.","time":3551704},{"__typename":"Cue","text":"There are so many startup stories\nthat began at Y Combinator.","time":3555746},{"__typename":"Cue","text":"What were key drivers in your own life\nthat took you on the path you're on?","time":3559538},{"__typename":"Cue","text":"And how did that path end\nup at Y Combinator?","time":3563121},{"__typename":"Cue","text":"No exaggeration. I think I have back to\nback had the two jobs that are at least","time":3567538},{"__typename":"Cue","text":"the most interesting to me\nin all of Silicon Valley.","time":3572371},{"__typename":"Cue","text":"I, I was I went to college to\nstudy computer science.","time":3574663},{"__typename":"Cue","text":"I was a major computer nerd growing up.","time":3578996},{"__typename":"Cue","text":"I knew like a little bit about startups,\nbut not very much.","time":3581246},{"__typename":"Cue","text":"I started working on this project the\nsame year I started working on that.","time":3584121},{"__typename":"Cue","text":"This thing called Y Combinator started\nand funded me and my co-founders.","time":3587454},{"__typename":"Cue","text":"And we dropped out of school\nand did this company,","time":3592163},{"__typename":"Cue","text":"which I ran for like seven years. And\nthen after that I got acquired.","time":3595746},{"__typename":"Cue","text":"I had stayed close to my\ncomment the whole time.","time":3599954},{"__typename":"Cue","text":"I thought it was just this incredible\ngroup of people and spirit","time":3601829},{"__typename":"Cue","text":"and set of incentives and just badly\nmisunderstood by most of the world,","time":3606121},{"__typename":"Cue","text":"but obvious to everyone within it that it\nwas going to create huge amounts of","time":3611496},{"__typename":"Cue","text":"value and do a lot of new things.","time":3614996},{"__typename":"Cue","text":"My company had acquired PJI,\nwho is the founder of ICI,","time":3619163},{"__typename":"Cue","text":"and like truly one of the most incredible\nhumans and business people.","time":3622413},{"__typename":"Cue","text":"And Paul Burrell, Paul Graham asked\nme if I wanted to run it.","time":3626163},{"__typename":"Cue","text":"And kind of like the central\nlearning of my career,","time":3629913},{"__typename":"Cue","text":"why I individual startups has been\nthat if you really scale them up,","time":3632246},{"__typename":"Cue","text":"remarkable things can happen. And I.","time":3636746},{"__typename":"Cue","text":"I did it and I was like, one of the things\nthat would make this exciting","time":3643288},{"__typename":"Cue","text":"for me personally motivating would\nbe if I could sort of push it in","time":3647288},{"__typename":"Cue","text":"the direction of doing these hard tech\ncompanies, one of which became open.","time":3650871},{"__typename":"Cue","text":"I describe actually","time":3655871},{"__typename":"Cue","text":"what Y Combinator is, you know,","time":3657579},{"__typename":"Cue","text":"how many people come through it to give\nus a couple of stories of its impact.","time":3659704},{"__typename":"Cue","text":"Yeah. So you basically apply as a\nhandful of people and an idea,","time":3663413},{"__typename":"Cue","text":"maybe a prototype and say,","time":3667079},{"__typename":"Cue","text":"I would like to start a company\nand will you please fund me?","time":3668996},{"__typename":"Cue","text":"And we review those applications and\nwe I shouldn't say we anymore.","time":3671829},{"__typename":"Cue","text":"I guess they fund four hundred\ncompanies a year.","time":3674913},{"__typename":"Cue","text":"You get about one hundred and fifty\nthousand dollars while she takes about","time":3677829},{"__typename":"Cue","text":"seven percent ownership and then gives you\nlots of advice and then networking","time":3680288},{"__typename":"Cue","text":"and sort of this like fast track\nprogram for starting a startup.","time":3683663},{"__typename":"Cue","text":"I haven't looked at this in a while,","time":3687704},{"__typename":"Cue","text":"but at one point a significant\nfraction of","time":3689079},{"__typename":"Cue","text":"the billion dollar plus companies\nin the US that got started.","time":3691871},{"__typename":"Cue","text":"It all came through the Wiki program,","time":3694621},{"__typename":"Cue","text":"some recently in the news ones have been\nlike Airbnb, Jordache, Coinbase,","time":3698371},{"__typename":"Cue","text":"insta card stripe. And I think it's just\nit has become an incredible way to help.","time":3702663},{"__typename":"Cue","text":"People who understand technology get\na three month course in business,","time":3710413},{"__typename":"Cue","text":"but instead of like herding\nyou with an MBA,","time":3715538},{"__typename":"Cue","text":"we actually teach you the things that\nmatter and kind of go on to do incredible,","time":3717788},{"__typename":"Cue","text":"incredible work.","time":3722996},{"__typename":"Cue","text":"What is it about entrepreneurs?\nWhy do they matter?","time":3724746},{"__typename":"Cue","text":"Some people just find them\nkind of annoying.","time":3727871},{"__typename":"Cue","text":"But I think you would argue I think\nI would argue that they have done","time":3731454},{"__typename":"Cue","text":"as much as anyone to shape the future. Why\n? What is it about them?","time":3734913},{"__typename":"Cue","text":"I think it is the ability to take.","time":3740496},{"__typename":"Cue","text":"And idea and by force of will to make\nit happen in the world and in","time":3743996},{"__typename":"Cue","text":"an incentive system that rewards you\nfor making the most impact on","time":3750829},{"__typename":"Cue","text":"the most people like in our system.","time":3756788},{"__typename":"Cue","text":"That's how we get most of the\nthings that that we use.","time":3760163},{"__typename":"Cue","text":"That's how we got the computer\nthat I'm using,","time":3764038},{"__typename":"Cue","text":"the software I'm using\nto talk to you on it.","time":3766496},{"__typename":"Cue","text":"Like all of this, you know, everyone in\nlife, everything has a balance sheet.","time":3769413},{"__typename":"Cue","text":"There's plenty of very annoying\nthings about them.","time":3773079},{"__typename":"Cue","text":"And there's plenty of very\nannoying things about","time":3775121},{"__typename":"Cue","text":"the system that sort of idolizes them.","time":3777121},{"__typename":"Cue","text":"But we do get something really\nimportant in return.","time":3780746},{"__typename":"Cue","text":"And I think that as a force","time":3784829},{"__typename":"Cue","text":"for making things that make all of our\nlives better happen, it's very cool.","time":3786454},{"__typename":"Cue","text":"Otherwise, you know, like if you have,\nlike, a great idea,","time":3793038},{"__typename":"Cue","text":"but you don't actually do anything useful\nwith it for people, that's still cool.","time":3795621},{"__typename":"Cue","text":"It's still intellectually interesting.","time":3798871},{"__typename":"Cue","text":"But like, there's got to\nbe something about","time":3800413},{"__typename":"Cue","text":"the reward function in\nsociety that is like,","time":3803371},{"__typename":"Cue","text":"did you actually do something useful?\nDid you create value?","time":3805329},{"__typename":"Cue","text":"And I think entrepreneurship\nand startups are","time":3808871},{"__typename":"Cue","text":"a wonderful way to do that. You know, we\nget all these great software companies.","time":3811246},{"__typename":"Cue","text":"But I also think it's like how\nwe're going to get ajai,","time":3815079},{"__typename":"Cue","text":"how we're going to get nuclear fusion, how\nwe're going to get life extension.","time":3817663},{"__typename":"Cue","text":"And like on any of those topics are\na long list of other things","time":3820788},{"__typename":"Cue","text":"I could point to. There's like\na number of startups that","time":3823788},{"__typename":"Cue","text":"I think are doing incredible work, some\nof which will actually deliver.","time":3826413},{"__typename":"Cue","text":"It is a truly amazing thing when you put\nthe camera back and to believe that","time":3830996},{"__typename":"Cue","text":"a human being could be\nlying awake at night","time":3836246},{"__typename":"Cue","text":"and something pops inside their\nmind as a patterning of","time":3838413},{"__typename":"Cue","text":"the neurons in their brain that is\neffectively them saying, aha,","time":3842663},{"__typename":"Cue","text":"I can see a way where the future\ncould be better and","time":3847663},{"__typename":"Cue","text":"and they can actually picture it.","time":3850954},{"__typename":"Cue","text":"And then they wake up and then they talk\nto other people and they persuade them","time":3852538},{"__typename":"Cue","text":"and they persuade investors and so forth.","time":3855246},{"__typename":"Cue","text":"And the fact that this this\nsystem can happen","time":3857246},{"__typename":"Cue","text":"and that they can then actually change\nthe history changes in some sense.","time":3860413},{"__typename":"Cue","text":"It is mind boggling that that happens that\nway and it happens again and again.","time":3865954},{"__typename":"Cue","text":"So you've seen so many of these stories\nhappen. What would you say?","time":3871496},{"__typename":"Cue","text":"Is the is there a key thing that\ndifferentiates good entrepreneurs from","time":3875704},{"__typename":"Cue","text":"others? If you could double down\non one trait, what would it be?","time":3884121},{"__typename":"Cue","text":"If I could pick only one, I\nwould pick determination.","time":3888413},{"__typename":"Cue","text":"I think that is the biggest\npredictor of success,","time":3892621},{"__typename":"Cue","text":"the biggest differentiator predictor.","time":3894621},{"__typename":"Cue","text":"And if you would allow a second,","time":3896996},{"__typename":"Cue","text":"I would pick like communication\nskills or evangelism","time":3898871},{"__typename":"Cue","text":"or something in that direction as well.","time":3902621},{"__typename":"Cue","text":"There are all of the obvious ones\nthat matter, like intelligence,","time":3904746},{"__typename":"Cue","text":"but there's like a lot of smart\npeople in the world.","time":3907621},{"__typename":"Cue","text":"And when I look back at kind of","time":3909829},{"__typename":"Cue","text":"the thousands of entrepreneurs\nI've worked with,","time":3911204},{"__typename":"Cue","text":"all of many of whom were\nlike quite capable,","time":3913496},{"__typename":"Cue","text":"I would say that's like one and two of","time":3916079},{"__typename":"Cue","text":"the surprisingly differentiated\ncharacteristics.","time":3917704},{"__typename":"Cue","text":"What it's it's what I look at,","time":3920288},{"__typename":"Cue","text":"the different things that you've\nbuilt and you're working on.","time":3922496},{"__typename":"Cue","text":"I mean, it could not be more foundational\nfor the future. I mean, entrepreneurship.","time":3924663},{"__typename":"Cue","text":"I know this is I agree that this is\nreally what has driven the future.","time":3929579},{"__typename":"Cue","text":"Do you see some people get really now they\nlook at Silicon Valley and they look","time":3935413},{"__typename":"Cue","text":"at this story and they worry\nabout the culture. Right.","time":3939079},{"__typename":"Cue","text":"That it's this is a bro culture.","time":3941329},{"__typename":"Cue","text":"Do you see prospects of that changing\nanytime soon? And would you welcome it?","time":3944079},{"__typename":"Cue","text":"Can we get better companies by\nreally working to expand","time":3948663},{"__typename":"Cue","text":"a group of people who\ncan be entrepreneurs","time":3952663},{"__typename":"Cue","text":"and who can contribute\nto aid, for example?","time":3955288},{"__typename":"Cue","text":"For sure. And in fact,\nI think I'm hopeful,","time":3957496},{"__typename":"Cue","text":"since these are the two things\nI've thought the most about.","time":3960538},{"__typename":"Cue","text":"I'm excited for the day when\nsomeone combines them","time":3962746},{"__typename":"Cue","text":"and uses A.I. to better select\nwho did more fairly,","time":3965038},{"__typename":"Cue","text":"maybe even select who to fund\nand how to advise them","time":3969329},{"__typename":"Cue","text":"and really kind of make entrepreneurship\nsuper widely available that will lead","time":3972454},{"__typename":"Cue","text":"to like better outcomes and sort of more\nsocietal wealth for all of us. So are.","time":3976829},{"__typename":"Cue","text":"So, yeah, I think.","time":3982913},{"__typename":"Cue","text":"Broadening the set of people able to start\ncompanies and that sort of get","time":3987079},{"__typename":"Cue","text":"the resources that you need,","time":3992246},{"__typename":"Cue","text":"that is like an unequivocally good\nthing and it's something that","time":3994163},{"__typename":"Cue","text":"I think Silicon Valley is making some\nprogress in. But I hope we see a lot more.","time":3997454},{"__typename":"Cue","text":"And I do really, truly think that","time":4001496},{"__typename":"Cue","text":"the technology industry entrepreneurship\nis one of the greatest forces for","time":4004246},{"__typename":"Cue","text":"self betterment. If we can\njust figure out how to be","time":4010204},{"__typename":"Cue","text":"a little bit more inclusive\nin how we do things.","time":4014579},{"__typename":"Cue","text":"My last question today is about\nideas were spreading.","time":4017371},{"__typename":"Cue","text":"If you could inject one idea into\nthe mind of everyone listening,","time":4020663},{"__typename":"Cue","text":"what what would the idea be?","time":4025621},{"__typename":"Cue","text":"We've touched on it a bunch,","time":4029996},{"__typename":"Cue","text":"but the one idea would be the ajai\nreally is going to happen.","time":4031454},{"__typename":"Cue","text":"You have to engage with it seriously,","time":4035413},{"__typename":"Cue","text":"and you shouldn't just listen to this and\nthen brush aside and go about life","time":4037579},{"__typename":"Cue","text":"as if it's not going to happen because\nit is going to affect everything.","time":4040954},{"__typename":"Cue","text":"And we will all we all, I think,\nhave an obligation,","time":4044371},{"__typename":"Cue","text":"but also an opportunity to figure out what\nnot means and how we want the world","time":4048038},{"__typename":"Cue","text":"and this sort of one time shift to go","time":4053871},{"__typename":"Cue","text":"on.","time":4058954},{"__typename":"Cue","text":"I'm kind of awed by the breadth\nof things are engaged with.","time":4059413},{"__typename":"Cue","text":"Thank you so much for spending so\nmuch time sharing your vision.","time":4063038},{"__typename":"Cue","text":"Thanks so much for having me.","time":4066454},{"__typename":"Cue","text":"OK, that's it for today. You can\nread more about open eyes,","time":4078579},{"__typename":"Cue","text":"vision and progress at open eye dotcom.","time":4082496},{"__typename":"Cue","text":"If you want to try playing with yourself,\nit's a little tricky.","time":4086913},{"__typename":"Cue","text":"You have to find a website that\nhas licensed the API.","time":4091329},{"__typename":"Cue","text":"The one I went to was philosopher\nehi dot com,","time":4094871},{"__typename":"Cue","text":"where you just you pay a few dollars to\nget access to a very strange mind.","time":4099746},{"__typename":"Cue","text":"That's actually quite a lot of fun.","time":4104538},{"__typename":"Cue","text":"The interview is part of the\nTED Audio Collective,","time":4106579},{"__typename":"Cue","text":"a collection of podcasts dedicated\nto sparking curiosity","time":4109871},{"__typename":"Cue","text":"and sharing ideas that matter.","time":4113704},{"__typename":"Cue","text":"This show is produced by Kim Net2Phone\nPittas and edited by Grace Rubenstein","time":4116038},{"__typename":"Cue","text":"and Sheila Boffano, Sambor Islamic Sir.","time":4121829},{"__typename":"Cue","text":"Fact Check is by Paul Durbin and special\nthanks to Michele Quent,","time":4125454},{"__typename":"Cue","text":"Colin Helmes and Anna Felin. If you like\nthe show, please write and review it.","time":4129204},{"__typename":"Cue","text":"It helps other people find us.","time":4135663},{"__typename":"Cue","text":"We read every review, so thanks so much\nfor listening. See you next time.","time":4137496}]}],"id":"192053","language":{"__typename":"Language","id":"35","endonym":"English","englishName":"English","internalLanguageCode":"en","rtl":false},"reviewer":null,"translator":null},"video":{"__typename":"Video","id":"75257","talkExtras":{"__typename":"TalkExtras","footnotes":[]}}},"commentsEnabled":false,"commentsLoggedInOnly":false},"__N_SSP":true},"page":"/talks/[...slug]","query":{"language":"en","slug":["the_ted_interview_the_race_to_build_ai_that_benefits_humanity_with_sam_altman_from_april_2021","transcript"]},"buildId":"Wbr9uI2wHDOXAcMZVzpse","isFallback":false,"isExperimentalCompile":false,"dynamicIds":[72727,78082,34187,32580,21153],"gssp":true,"scriptLoader":[]}</script></body></html>